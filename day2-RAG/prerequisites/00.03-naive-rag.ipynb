{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ” 00.03: ê¸°ë³¸ RAG ë§›ë³´ê¸°\n",
    "\n",
    "## ğŸ’¡ ê²€ìƒ‰ + ìƒì„±ì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì²´í—˜!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5ef3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (RAG ì²´í—˜ìš©)\n",
    "!pip install -q langchain-community langchain-core sentence-transformers faiss-cpu transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ìƒ˜í”Œ ë¬¸ì„œ + í•œêµ­ì–´ ì„ë² ë”© ë²¡í„°ìŠ¤í† ì–´ ì¤€ë¹„\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# í•œêµ­ì–´ê°€ ì„ì¸ ê°„ë‹¨í•œ ë¬¸ì„œë“¤ (ë„ë©”ì¸ ë‹¤ì–‘í™”)\n",
    "raw_docs = [\n",
    "    {\n",
    "        \"title\": \"íŒŒì´ì¬ ì—­ì‚¬\",\n",
    "        \"content\": \"íŒŒì´ì¬ì€ 1991ë…„ ê·€ë„ ë°˜ ë¡œì„¬ì´ ë§Œë“  ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì´ë©°, í˜„ì¬ë„ í™œë°œí•˜ê²Œ ì‚¬ìš©ëœë‹¤.\",\n",
    "        \"category\": \"programming\",\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"ë¨¸ì‹ ëŸ¬ë‹ ê°œìš”\",\n",
    "        \"content\": \"ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¡œ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ê¸°ìˆ ì´ë©°, ìµœê·¼ì—ëŠ” ë”¥ëŸ¬ë‹ì´ í° ì£¼ëª©ì„ ë°›ê³  ìˆë‹¤.\",\n",
    "        \"category\": \"ai\",\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"RAG ì†Œê°œ\",\n",
    "        \"content\": \"RAGëŠ” ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•œ ì‹œìŠ¤í…œìœ¼ë¡œ, ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•„ ë‹µë³€ì˜ ì‹ ë¢°ë„ë¥¼ ë†’ì¸ë‹¤.\",\n",
    "        \"category\": \"ai\",\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"ì„œìš¸ ì—¬í–‰\",\n",
    "        \"content\": \"ì„œìš¸ì€ ë‹¤ì–‘í•œ ë¬¸í™”ìœ ì‚°ê³¼ ìµœì‹  íŠ¸ë Œë“œê°€ ê³µì¡´í•˜ëŠ” ë„ì‹œë¡œ, ì£¼ë§ ì—¬í–‰ì§€ë¡œ ì¸ê¸°ê°€ ë†’ë‹¤.\",\n",
    "        \"category\": \"travel\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# LangChain Document ê°ì²´ë¡œ ë³€í™˜\n",
    "documents = [Document(page_content=doc[\"content\"], metadata={\"title\": doc[\"title\"], \"category\": doc[\"category\"]}) for doc in raw_docs]\n",
    "\n",
    "# í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ë¡œ ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• (bge-m3ëŠ” ë‹¤êµ­ì–´ ì§€ì›)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "print(\"âœ… ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ì§ˆë¬¸ìœ¼ë¡œ ìƒìœ„ kê°œ ë¬¸ì„œ ê²€ìƒ‰ + ë¦¬ë”ë³´ë“œ í™•ì¸\n",
    "import pandas as pd\n",
    "\n",
    "question = \"RAGëŠ” ì–´ë–»ê²Œ ë™ì‘í•´?\"\n",
    "results = vectorstore.similarity_search(question, k=4)\n",
    "\n",
    "leaderboard = pd.DataFrame([\n",
    "    {\n",
    "        \"rank\": idx + 1,\n",
    "        \"title\": res.metadata.get(\"title\", \"\"),\n",
    "        \"category\": res.metadata.get(\"category\", \"\"),\n",
    "        \"snippet\": res.page_content[:40] + \"...\",\n",
    "    }\n",
    "    for idx, res in enumerate(results)\n",
    "])\n",
    "\n",
    "print(f\"ğŸ” ì§ˆë¬¸: {question}\")\n",
    "display(leaderboard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f957369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ê°€ì¥ ê´€ë ¨ ìˆëŠ” ë¬¸ì„œ í•˜ì´ë¼ì´íŠ¸ë¡œ í™•ì¸\n",
    "import re\n",
    "\n",
    "selected_doc = results[0].page_content\n",
    "keywords = question.replace('?', '').split()\n",
    "\n",
    "highlighted = selected_doc\n",
    "for kw in keywords:\n",
    "    if kw:\n",
    "        highlighted = re.sub(kw, f\"**{kw}**\", highlighted, flags=re.IGNORECASE)\n",
    "\n",
    "print(\"ğŸ“„ ìƒìœ„ ë¬¸ì„œ ë‚´ìš© (ì§ˆë¬¸ í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸):\")\n",
    "print(highlighted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Day1ì—ì„œ ì‚¬ìš©í•œ EXAONE instruct ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_exaone():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\", trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "def generate_answer(question: str, docs) -> str:\n",
    "    context = \" \".join([doc.page_content for doc in docs[:2]])\n",
    "    prompt = f\"\"\"[SYSTEM]\n",
    "ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "\n",
    "[CONTEXT]\n",
    "{context}\n",
    "\n",
    "[QUESTION]\n",
    "{question}\n",
    "\n",
    "[ANSWER]\n",
    "\"\"\"\n",
    "    inputs = exaone_tokenizer(prompt, return_tensors=\"pt\").to(exaone_model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = exaone_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=120,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            eos_token_id=exaone_tokenizer.eos_token_id\n",
    "        )\n",
    "    return exaone_tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"[ANSWER]\")[-1].strip()\n",
    "\n",
    "exaone_tokenizer, exaone_model = load_exaone()\n",
    "answer = generate_answer(question, results)\n",
    "print(\"ğŸ¤– EXAONE ê¸°ë°˜ ë‹µë³€:\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f969fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ê°„ë‹¨ ì„±ëŠ¥ ì§€í‘œ ë³´ë“œ (í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€ & similarity score)\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€(ì§ˆë¬¸ê³¼ ë¬¸ì„œê°€ ì–¼ë§ˆë‚˜ ê²¹ì¹˜ëŠ”ì§€) ê³„ì‚°\n",
    "# - ì§ˆë¬¸ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ ë¯¸ë¦¬ ì •ì˜í•´ ë‘ê³ \n",
    "# - ë¬¸ì„œì— ëª‡ ê°œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ë¹„ìœ¨ë¡œ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "def keyword_hit_ratio(text, keywords):\n",
    "    return sum(1 for kw in keywords if kw.lower() in text.lower()) / len(keywords)\n",
    "\n",
    "# Naive RAGì˜ í•œê³„ë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•´, ê°„ë‹¨í•œ ë„¤ ê°€ì§€ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "keywords = [\"RAG\", \"ê²€ìƒ‰\", \"ìƒì„±\", \"ì‹œìŠ¤í…œ\"]\n",
    "metrics = []\n",
    "\n",
    "for idx, res in enumerate(results):\n",
    "    # SequenceMatcherëŠ” íŒŒì´ì¬ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ë‘ ë¬¸ìì—´ì˜ ê³µí†µ subsequence ë¹„ìœ¨ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "# - í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê±°ë‚˜ ë²¡í„°í™”í•˜ì§€ ì•Šê³  ë¬¸ì ê¸°ë°˜ìœ¼ë¡œ ë¹„êµí•©ë‹ˆë‹¤.\n",
    "# - ì¶œë ¥ì€ 0(ì™„ì „íˆ ë‹¤ë¦„)~1(ì™„ì „íˆ ê°™ìŒ) ì‚¬ì´ì˜ ë¶€ë™ì†Œìˆ˜ì ì…ë‹ˆë‹¤.\n",
    "# - ì¦‰, ìŠ¤íŒŒìŠ¤/ë´ìŠ¤ ì„ë² ë”©ì´ ì•„ë‹ˆë¼ ê°„ë‹¨í•œ ë¬¸ìì—´ ë§¤ì¹­ì„ ë³´ëŠ” ì§€í‘œì…ë‹ˆë‹¤.\n",
    "# - â€œë‘ ë¬¸ì¥ì— ì—°ì†ëœ ê¸€ìê°€ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•˜ê²Œ ë‚˜ì—´ë˜ì–´ ìˆëƒâ€\n",
    "\n",
    "\n",
    "    similarity = SequenceMatcher(None, question.lower(), res.page_content.lower()).ratio()\n",
    "    metrics.append({\n",
    "        \"Rank\": idx + 1,\n",
    "        \"ë¬¸ì„œ ì œëª©\": res.metadata.get(\"title\", \"\"),\n",
    "        \"ì¹´í…Œê³ ë¦¬\": res.metadata.get(\"category\", \"\"),\n",
    "        \"í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€\": round(keyword_hit_ratio(res.page_content, keywords), 2),\n",
    "        \"ì§ˆë¬¸-ë¬¸ì„œ ìœ ì‚¬ë„\": round(similarity, 2)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(\"ğŸ“Š ìƒìœ„ ë¬¸ì„œ ë¹„êµ (í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€ / ì§ˆë¬¸-ë¬¸ì„œ ìœ ì‚¬ë„):\")\n",
    "display(metrics_df)\n",
    "\n",
    "print(\"í•´ì„ ê°€ì´ë“œ:\")\n",
    "print(\"- Rank: ê²€ìƒ‰ ê²°ê³¼ ìˆœìœ„ (1ìœ„ê°€ ê°€ì¥ ê´€ë ¨ë„ê°€ ë†’ì€ ë¬¸ì„œ)\")\n",
    "print(\"- í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€: ì§ˆë¬¸ í•µì‹¬ í‚¤ì›Œë“œê°€ ë¬¸ì„œì— ì–¼ë§ˆë‚˜ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ (1.0ì´ë©´ ëª¨ë‘ í¬í•¨)\")\n",
    "print(\"- ì§ˆë¬¸-ë¬¸ì„œ ìœ ì‚¬ë„: ì „ì²´ í…ìŠ¤íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì§ˆë¬¸ê³¼ ë¬¸ì„œê°€ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ (0~1 ì‚¬ì´)\")\n",
    "print(\"â¡ï¸ ì˜ˆì‹œ) 1ìœ„ ë¬¸ì„œì˜ ì»¤ë²„ë¦¬ì§€/ìœ ì‚¬ë„ê°€ ë‚®ë‹¤ë©´ ë‚˜ì´ë¸Œ RAGê°€ í…ìŠ¤íŠ¸ í‘œë©´ë§Œ ë³´ê³  ì˜ëª»ëœ ë¬¸ì„œë¥¼ ë½‘ì•˜ì„ ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤.\")\n",
    "print(\"â¡ï¸ ì´ ì§€í‘œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì‹¤ìŠµì—ì„œ ì¿¼ë¦¬ ê°œì„ , ë©”íƒ€ë°ì´í„° í•„í„°ë§ ë“±ì´ ì™œ í•„ìš”í•œì§€ ì—°ê²°í•©ë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. RAG ê³¼ì • ì •ë¦¬\n",
    "print(\"ğŸ‰ ê¸°ë³¸ RAG ì²´í—˜ ì™„ë£Œ!\")\n",
    "print(\"âœ… 1ë‹¨ê³„: ì§ˆë¬¸ ì…ë ¥\")\n",
    "print(\"âœ… 2ë‹¨ê³„: ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰\")\n",
    "print(\"âœ… 3ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ë¡œ ë‹µë³€ ìƒì„±\")\n",
    "print(\"ğŸ’¡ ë‹¤ìŒ: 00.04ì—ì„œ ê³ ê¸‰ RAG ê¸°ëŠ¥ë“¤ ë§›ë³´ê¸°!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc7389",
   "metadata": {},
   "source": [
    "## âœ… ê¸°ë³¸ RAG ì²´í—˜ ì •ë¦¬\n",
    "- ê²€ìƒ‰ ìƒìœ„ ê²°ê³¼(Top-k)ê°€ ì–´ë–»ê²Œ ìˆœìœ„ì™€ ìŠ¤ì½”ì–´ê°€ ë‹¤ë¥¸ì§€ ì§ì ‘ í™•ì¸\n",
    "- EXAONE ëª¨ë¸ì„ ì‚¬ìš©í•´ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ìƒì„±\n",
    "- í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€ / ìœ ì‚¬ë„ ë¹„êµë¥¼ í†µí•´ ë‚˜ì´ë¸Œ RAGì˜ í•œê³„(ë¬¸ë§¥ ëˆ„ë½, ë‹¤ì˜ì–´ ë¬¸ì œ ë“±) ì²´ê°\n",
    "- ë‹¤ìŒ ì‹¤ìŠµ(02)ì—ì„œ ë‹¤ë£¨ê²Œ ë  ì‹¤íŒ¨ íŒ¨í„´ì„ ë¯¸ë¦¬ íŒíŠ¸ë¡œ ì†Œê°œí•´ ì£¼ì„¸ìš”.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}