{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d51eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (langchain-community ë“±)\n",
    "!pip install -q langchain-community langchain-core sentence-transformers faiss-cpu transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620fef36",
   "metadata": {},
   "source": [
    "# ğŸ“Š 00.05: RAG í‰ê°€ ë§›ë³´ê¸°\n",
    "Naive RAGê°€ ì–´ë–¤ ê¸°ì¤€ì—ì„œ ë¶€ì¡±í•œì§€ ì§ì ‘ ì²´í—˜í•´ ë´…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731e3b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. í‰ê°€ í™˜ê²½ ì¤€ë¹„\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "RAW_DOCS = [\n",
    "    {\"title\": \"íŒŒì´ì¬ ì—­ì‚¬\", \"content\": \"íŒŒì´ì¬ì€ 1991ë…„ ê·€ë„ ë°˜ ë¡œì„¬ì´ ë§Œë“  ì–¸ì–´ì…ë‹ˆë‹¤.\", \"category\": \"programming\"},\n",
    "    {\"title\": \"ë¨¸ì‹ ëŸ¬ë‹ ê°œìš”\", \"content\": \"ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¥¼ í•™ìŠµí•´ íŒ¨í„´ì„ ì°¾ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\", \"category\": \"ai\"},\n",
    "    {\"title\": \"RAG ì†Œê°œ\", \"content\": \"RAG ì‹œìŠ¤í…œì€ ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•´ ê´€ë ¨ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.\", \"category\": \"ai\"},\n",
    "    {\"title\": \"ì„œìš¸ ì—¬í–‰\", \"content\": \"ì„œìš¸ì€ ë¬¸í™”ìœ ì‚°ê³¼ ìµœì‹  íŠ¸ë Œë“œê°€ ê³µì¡´í•˜ëŠ” ë„ì‹œì…ë‹ˆë‹¤.\", \"category\": \"travel\"},\n",
    "]\n",
    "\n",
    "DOCUMENTS = [Document(page_content=d[\"content\"], metadata=d) for d in RAW_DOCS]\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "vectorstore = FAISS.from_documents(DOCUMENTS, embedding_model)\n",
    "print(\"âœ… í‰ê°€ìš© ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5871490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. í‰ê°€ìš© ì§ˆë¬¸ ì„¸íŠ¸ ì •ì˜\n",
    "EVAL_QUESTIONS = [\n",
    "    {\"question\": \"íŒŒì´ì¬ì„ ë§Œë“  ì‚¬ëŒì€ ëˆ„êµ¬ì•¼?\", \"expected_kw\": \"ê·€ë„\", \"type\": \"factual\"},\n",
    "    {\"question\": \"RAGëŠ” ë¬´ì—‡ì„ ê²°í•©í•´?\", \"expected_kw\": \"ê²€ìƒ‰\", \"type\": \"definition\"},\n",
    "    {\"question\": \"ë¨¸ì‹ ëŸ¬ë‹ì€ ë¬´ìŠ¨ ê¸°ìˆ ì´ì•¼?\", \"expected_kw\": \"íŒ¨í„´\", \"type\": \"definition\"},\n",
    "    {\"question\": \"ì„œìš¸ ì—¬í–‰ì€ ì–´ë–¨ê¹Œ?\", \"expected_kw\": \"ë¬¸í™”\", \"type\": \"experience\"},\n",
    "]\n",
    "print(\"í‰ê°€ ì§ˆë¬¸ ê°œìˆ˜:\", len(EVAL_QUESTIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdc6408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Top-k êµ¬ì„± ê²°ê³¼ ë° ì§€í‘œ ê³„ì‚°\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "results_summary = []\n",
    "\n",
    "for item in EVAL_QUESTIONS:\n",
    "    question = item['question']\n",
    "    expected = item['expected_kw']\n",
    "    retrieved = vectorstore.similarity_search(question, k=3)\n",
    "    coverage = [expected in doc.page_content for doc in retrieved]\n",
    "    similarity = [SequenceMatcher(None, question.lower(), doc.page_content.lower()).ratio() for doc in retrieved]\n",
    "    results_summary.append({\n",
    "        'question': question,\n",
    "        'type': item['type'],\n",
    "        'top1_hit': coverage[0],\n",
    "        'recall@3': float(np.mean(coverage)),\n",
    "        'avg_sim': round(float(np.mean(similarity)), 2)\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(results_summary)\n",
    "print('ğŸ” ìƒìœ„ ë¬¸ì„œ ì„±ëŠ¥ ìš”ì•½')\n",
    "display(summary_df)\n",
    "\n",
    "print('í•´ì„ ì˜ˆì‹œ:')\n",
    "for row in summary_df.to_dict('records'):\n",
    "    msg = \" - '{q}' ({t}) -> Top1 Hit: {hit}, Recall@3: {rc:.0f}%, í‰ê·  ë¬¸ììœ ì‚¬ë„: {sim:.2f}\".format(\n",
    "        q=row['question'], t=row['type'], hit=row['top1_hit'], rc=row['recall@3']*100, sim=row['avg_sim'])\n",
    "    print(msg)\n",
    "print('í•´ì„ ë°©ë²•:')\n",
    "print('â€¢ Top1 Hit=True ì´ë”ë¼ë„ Recall@3ê°€ 33%ë¼ë©´, ìƒìœ„ 3ê°œ ì¤‘ ê´€ë ¨ ë¬¸ì„œëŠ” 1ê°œë¿ì„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.')\n",
    "print('â€¢ í‰ê·  ë¬¸ììœ ì‚¬ë„ê°€ 0.1~0.3ì´ë©´ ì§ˆë¬¸-ë¬¸ì„œê°€ í‘œë©´ì ìœ¼ë¡œë§Œ ì¡°ê¸ˆ ë¹„ìŠ·í•˜ë‹¤ëŠ” ëœ» â†’ ì˜ë¯¸ ì—°êµ¬ í•„ìš”.')\n",
    "print('â€¢ ë”°ë¼ì„œ ë‚˜ì´ë¸Œ RAGëŠ” ì²« ë¬¸ì„œë§Œ ìš´ ì¢‹ê²Œ ë§ì„ ë¿, í›„ë³´êµ°ì´ ì·¨ì•½í•˜ë‹¤ëŠ” ì‚¬ì‹¤ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8960b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. í‰ê°€ ì§€í‘œ ìš”ì•½\n",
    "accuracy = summary_df['top1_hit'].mean() * 100\n",
    "recall3 = summary_df['recall@3'].mean() * 100\n",
    "avg_similarity = summary_df['avg_sim'].mean()\n",
    "\n",
    "print('ğŸ“ˆ ìš”ì•½ ì§€í‘œ')\n",
    "print('- Top1 ì •í™•ë„: {:.1f}% (1ìœ„ ë¬¸ì„œê°€ ì •ë‹µì„ í¬í•¨í–ˆëŠ”ì§€)'.format(accuracy))\n",
    "print('- Recall@3 í‰ê· : {:.1f}% (ìƒìœ„ 3ê°œ ì¤‘ ê´€ë ¨ ë¬¸ì„œ ë¹„ìœ¨)'.format(recall3))\n",
    "print('- í‰ê·  ë¬¸ììœ ì‚¬ë„: {:.2f} (0~1, ë¬¸ìì—´ ìˆ˜ì¤€ ìœ ì‚¬ë„)'.format(avg_similarity))\n",
    "\n",
    "if accuracy < 80:\n",
    "    print('âš ï¸ í‚¤ì›Œë“œê°€ ë¹ ì§„ ê²°ê³¼ê°€ ìˆìœ¼ë¯€ë¡œ ê²€ìƒ‰ ê°•í™” í•„ìš” (ë‹¤ìŒ ê³ ê¸‰ ì‹¤ìŠµì—ì„œ í•´ê²°)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b511fab5",
   "metadata": {},
   "outputs": [],
   "source": "# 5. LLM ê¸°ë°˜ RAG í‰ê°€ ì‹œìŠ¤í…œ (RAGAs ìŠ¤íƒ€ì¼)\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport re\n\nexaone_tokenizer = AutoTokenizer.from_pretrained('LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct', trust_remote_code=True)\nexaone_model = AutoModelForCausalLM.from_pretrained('LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct', trust_remote_code=True, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32, device_map='auto')\n\n# ë””ë²„ê¹… ëª¨ë“œ (Trueë¡œ ì„¤ì •í•˜ë©´ ëª¨ë¸ ì‘ë‹µ ì¶œë ¥)\nDEBUG_MODE = True\n\ndef evaluate_with_llm(question, context, answer, metric_type):\n    \"\"\"LLMì„ ì‚¬ìš©í•˜ì—¬ RAG ë‹µë³€ì„ í‰ê°€ (ê°œì„ ëœ ë²„ì „)\"\"\"\n    \n    if metric_type == \"faithfulness\":\n        prompt = f\"\"\"ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì—ë§Œ ê·¼ê±°í•˜ëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.\n\nì»¨í…ìŠ¤íŠ¸: {context}\në‹µë³€: {answer}\n\ní‰ê°€ ì˜ˆì‹œ:\n- ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ë‚´ìš©ë§Œ ì‚¬ìš© â†’ 5ì \n- ì»¨í…ìŠ¤íŠ¸ ë‚´ìš© + ì•½ê°„ì˜ ì™¸ë¶€ ì§€ì‹ â†’ 3ì \n- í™˜ê°ì´ ë§ìŒ â†’ 0ì \n\në‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í–ˆë‚˜ìš”?\në°˜ë“œì‹œ \"ì ìˆ˜: X\" í˜•ì‹ìœ¼ë¡œ ë‹µí•˜ì„¸ìš” (XëŠ” 0, 1, 2, 3, 4, 5 ì¤‘ í•˜ë‚˜):\"\"\"\n\n    elif metric_type == \"relevancy\":\n        prompt = f\"\"\"ë‹µë³€ì´ ì§ˆë¬¸ì— ì§ì ‘ ë‹µí•˜ëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.\n\nì§ˆë¬¸: {question}\në‹µë³€: {answer}\n\ní‰ê°€ ì˜ˆì‹œ:\n- ì§ˆë¬¸ì— ì •í™•íˆ ë‹µí•¨ â†’ 5ì \n- ì§ˆë¬¸ê³¼ ì•½ê°„ ê´€ë ¨ â†’ 3ì \n- ì§ˆë¬¸ê³¼ ë¬´ê´€ â†’ 0ì \n\në‹µë³€ì´ ì§ˆë¬¸ì— ë‹µí•˜ë‚˜ìš”?\në°˜ë“œì‹œ \"ì ìˆ˜: X\" í˜•ì‹ìœ¼ë¡œ ë‹µí•˜ì„¸ìš” (XëŠ” 0, 1, 2, 3, 4, 5 ì¤‘ í•˜ë‚˜):\"\"\"\n\n    elif metric_type == \"correctness\":\n        prompt = f\"\"\"ë‹µë³€ì´ ì •í™•í•œì§€ í‰ê°€í•˜ì„¸ìš”.\n\nì§ˆë¬¸: {question}\nì •ë‹µ ì»¨í…ìŠ¤íŠ¸: {context}\në‹µë³€: {answer}\n\ní‰ê°€ ì˜ˆì‹œ:\n- ì™„ë²½íˆ ì •í™• â†’ 5ì \n- ëŒ€ì²´ë¡œ ì •í™• â†’ 3ì \n- í‹€ë¦¼ â†’ 0ì \n\në‹µë³€ì´ ì •í™•í•œê°€ìš”?\në°˜ë“œì‹œ \"ì ìˆ˜: X\" í˜•ì‹ìœ¼ë¡œ ë‹µí•˜ì„¸ìš” (XëŠ” 0, 1, 2, 3, 4, 5 ì¤‘ í•˜ë‚˜):\"\"\"\n    \n    else:\n        return 0.0\n    \n    inputs = exaone_tokenizer(prompt, return_tensors='pt').to(exaone_model.device)\n    input_length = inputs.input_ids.shape[1]\n    \n    with torch.no_grad():\n        outputs = exaone_model.generate(\n            **inputs, \n            max_new_tokens=30,\n            temperature=0.1,  # ë” ì¼ê´€ëœ ì¶œë ¥\n            do_sample=True,\n            pad_token_id=exaone_tokenizer.eos_token_id\n        )\n    \n    generated_tokens = outputs[0][input_length:]\n    response = exaone_tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n    \n    # ë””ë²„ê¹…: ëª¨ë¸ ì‘ë‹µ ì¶œë ¥\n    if DEBUG_MODE:\n        print(f\"  [{metric_type}] ëª¨ë¸ ì‘ë‹µ: {response[:50]}\")\n    \n    # ê°œì„ ëœ ìˆ«ì ì¶”ì¶œ: \"ì ìˆ˜: 5\", \"5ì \", \"5\" ë“± ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›\n    score_patterns = [\n        r'ì ìˆ˜[:\\s]*([0-5])',  # \"ì ìˆ˜: 5\" or \"ì ìˆ˜ 5\"\n        r'([0-5])\\s*ì ',       # \"5ì \"\n        r'^([0-5])$',          # \"5\"\n        r'([0-5])',            # ì•„ë¬´ ìˆ«ìë‚˜\n    ]\n    \n    score = None\n    for pattern in score_patterns:\n        match = re.search(pattern, response)\n        if match:\n            score = int(match.group(1)) / 5.0  # 0~1 ìŠ¤ì¼€ì¼ë¡œ ì •ê·œí™”\n            break\n    \n    if score is None:\n        if DEBUG_MODE:\n            print(f\"  âš ï¸ íŒŒì‹± ì‹¤íŒ¨! ê¸°ë³¸ê°’ 0.5 ì‚¬ìš©\")\n        score = 0.5\n    \n    return score\n\n# ì „ì²´ í‰ê°€ ì‹¤í–‰\nprint('ğŸ” LLM ê¸°ë°˜ RAG í‰ê°€ ì‹œì‘ (RAGAs ìŠ¤íƒ€ì¼)')\nprint('-' * 80)\n\neval_results = []\n\nfor item in EVAL_QUESTIONS:\n    question = item['question']\n    \n    # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰\n    retrieved_docs = vectorstore.similarity_search(question, k=1)\n    context = retrieved_docs[0].page_content\n    \n    # ë‹µë³€ ìƒì„±\n    answer_prompt = f\"ì§ˆë¬¸: {question}\\nì»¨í…ìŠ¤íŠ¸: {context}\\në‹µë³€:\"\n    inputs = exaone_tokenizer(answer_prompt, return_tensors='pt').to(exaone_model.device)\n    input_length = inputs.input_ids.shape[1]\n    \n    with torch.no_grad():\n        output = exaone_model.generate(**inputs, max_new_tokens=80)\n    \n    generated_tokens = output[0][input_length:]\n    answer = exaone_tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n    \n    if DEBUG_MODE:\n        print(f\"\\nì§ˆë¬¸: {question}\")\n        print(f\"ë‹µë³€: {answer[:60]}\")\n    \n    # 3ê°€ì§€ ë©”íŠ¸ë¦­ í‰ê°€\n    faithfulness = evaluate_with_llm(question, context, answer, \"faithfulness\")\n    relevancy = evaluate_with_llm(question, context, answer, \"relevancy\")\n    correctness = evaluate_with_llm(question, context, answer, \"correctness\")\n    \n    avg_score = (faithfulness + relevancy + correctness) / 3\n    \n    eval_results.append({\n        'question': question[:30] + '...' if len(question) > 30 else question,\n        'context': context[:50] + '...' if len(context) > 50 else context,\n        'answer': answer[:40] + '...' if len(answer) > 40 else answer,\n        'faithfulness': round(faithfulness, 2),\n        'relevancy': round(relevancy, 2),\n        'correctness': round(correctness, 2),\n        'avg_score': round(avg_score, 2)\n    })\n    \n    print(f\"âœ“ í‰ê· : {avg_score:.2f}\")\n\n# ê²°ê³¼ DataFrame ì¶œë ¥\neval_df = pd.DataFrame(eval_results)\nprint('\\nğŸ“Š LLM ê¸°ë°˜ RAG í‰ê°€ ê²°ê³¼ (0~1 ìŠ¤ì¼€ì¼)')\ndisplay(eval_df)\n\nprint('\\ní•´ì„ ê°€ì´ë“œ:')\nprint('- faithfulness: ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í–ˆëŠ”ì§€ (í™˜ê° ì²´í¬)')\nprint('- relevancy: ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ìˆëŠ”ì§€')\nprint('- correctness: ë‹µë³€ì´ ì •í™•í•˜ê³  ì™„ì „í•œì§€')\nprint('- avg_score: 3ê°€ì§€ ë©”íŠ¸ë¦­ í‰ê· ')\nprint('- 0.7 ì´ìƒ: ìš°ìˆ˜, 0.5~0.7: ë³´í†µ, 0.5 ë¯¸ë§Œ: ê°œì„  í•„ìš”')\nprint(f'\\nğŸ’¡ ë””ë²„ê·¸ ëª¨ë“œ: {DEBUG_MODE} (ì…€ ìƒë‹¨ì—ì„œ Falseë¡œ ë³€ê²½ ê°€ëŠ¥)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cff737",
   "metadata": {},
   "outputs": [],
   "source": "# 6. Q&A ìš”ì•½\nprint('ğŸ‰ 00.05 RAG í‰ê°€ ë§›ë³´ê¸° ì™„ë£Œ!')\nprint('- ìƒìœ„ ë¬¸ì„œ ìˆœìœ„ì™€ í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€ë¥¼ í†µí•´ ë‚˜ì´ë¸Œ RAG í•œê³„ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤.')\nprint('- Recall@3, ë¬¸ì ìœ ì‚¬ë„ ë“± ê°„ë‹¨ ì§€í‘œë¡œ ì„±ëŠ¥ì„ ìˆ˜ì¹˜í™”í–ˆìŠµë‹ˆë‹¤.')\nprint('- LLM ê¸°ë°˜ ìì²´ í‰ê°€ë¡œ Faithfulness, Relevancy, Correctnessë¥¼ ì¸¡ì •í–ˆìŠµë‹ˆë‹¤.')\nprint('â¡ï¸ ë‹¤ìŒ ì‹¤ìŠµì—ì„œ ì¿¼ë¦¬ ê°œì„ , ë©”íƒ€ë°ì´í„° í•„í„°ë§, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ì ìš©í•´ ë³´ì„¸ìš”.')"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}