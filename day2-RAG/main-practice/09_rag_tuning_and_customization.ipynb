{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ Day 2 ì‹¤ìŠµ 8: RAG íŠœë‹ & ì»¤ìŠ¤í„°ë§ˆì´ì§• - ë‚˜ë§Œì˜ RAG ë§Œë“¤ê¸°\n",
    "\n",
    "## ğŸ¯ í•™ìŠµ ëª©í‘œ\n",
    "- **íŒŒë¼ë¯¸í„° íŠœë‹**: Chunk Size, Top-K, Threshold ì§ì ‘ ì¡°ì •í•´ë³´ê¸°\n",
    "- **ìµœì  ì„¤ì • ì°¾ê¸°**: ì§ˆë¬¸ íƒ€ì…ë³„ ìµœì  íŒŒë¼ë¯¸í„° ì¡°í•© ë°œê²¬\n",
    "- **ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹**: ë‚˜ë§Œì˜ í‰ê°€ ë°ì´í„°ì…‹ ë§Œë“¤ê¸°\n",
    "- **ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…**: Gradioë¡œ 3ì¤„ UI ë§Œë“¤ê¸°\n",
    "\n",
    "## ğŸ’¡ í•µì‹¬ ì•„ì´ë””ì–´\n",
    "\n",
    "```\n",
    "ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ê¸°ë²•ë“¤:\n",
    "  01ë²ˆ: Naive RAG\n",
    "  03ë²ˆ: Multi-Query\n",
    "  04ë²ˆ: Metadata Filtering\n",
    "  05ë²ˆ: Hybrid + Reranking\n",
    "  06ë²ˆ: Corrective RAG\n",
    "  07ë²ˆ: RAG Evaluation\n",
    "\n",
    "í•˜ì§€ë§Œ... ì–¸ì œ ì–´ë–¤ ì„¤ì •ì„ ì¨ì•¼ í• ê¹Œ?\n",
    "\n",
    "ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ”:\n",
    "  âœ… ì§ì ‘ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•´ë³´ê³ \n",
    "  âœ… ê²°ê³¼ë¥¼ ê´€ì°°í•˜ê³ \n",
    "  âœ… ë‚˜ë§Œì˜ ìµœì  ì„¤ì •ì„ ì°¾ì•„ë´…ë‹ˆë‹¤!\n",
    "```\n",
    "\n",
    "## ğŸ“‹ ì‹¤ìŠµ êµ¬ì„±\n",
    "1. **Part 1**: íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í—˜ì‹¤ (Chunk Size, Top-K, Threshold)\n",
    "2. **Part 2**: ë‚˜ë§Œì˜ í‰ê°€ ë°ì´í„°ì…‹ ë§Œë“¤ê¸°\n",
    "3. **Part 3**: Gradio UIë¡œ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…\n",
    "4. **Part 4**: ìµœì¢… ì •ë¦¬ & Day 3 Preview\n",
    "\n",
    "---\n",
    "\n",
    "> ğŸ’¡ **ì´ë²ˆ ì‹¤ìŠµì˜ íŠ¹ë³„í•¨**\n",
    "> - ê¸°ì¡´: ì½”ë“œ ì‹¤í–‰ë§Œ â†’ ì´ë²ˆ: **ì§ì ‘ ìˆ˜ì •í•˜ê³  ì‹¤í—˜**\n",
    "> - ê¸°ì¡´: ì •í•´ì§„ ë‹µ â†’ ì´ë²ˆ: **ë‚˜ë§Œì˜ ìµœì  ì„¤ì • ë°œê²¬**\n",
    "> - ê¸°ì¡´: ë³´ê¸°ë§Œ â†’ ì´ë²ˆ: **ë§Œë“¤ì–´ë³´ê¸°**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° import\n",
    "!pip install -q langchain-community faiss-cpu sentence-transformers pandas numpy scikit-learn\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë° í‰ê°€ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "# 13ê°œ ë ˆì‹œí”¼ ë¬¸ì„œ (í•µì‹¬ 5 + Confuser 5 + ë…¸ì´ì¦ˆ 3)\n",
    "recipe_docs_raw = [\n",
    "    # í•µì‹¬ 5ê°œ (ì •í†µ ë ˆì‹œí”¼)\n",
    "    {\"recipe_name\": \"ê¹€ì¹˜ì°Œê°œ\", \"content\": \"ê¹€ì¹˜ì°Œê°œ ë§Œë“œëŠ” ë²•: ë¼ì§€ê³ ê¸°ë¥¼ ì°¸ê¸°ë¦„ì— ë³¶ë‹¤ê°€ ê¹€ì¹˜ë¥¼ ë„£ê³  ë³¶ëŠ”ë‹¤. ë¬¼ì„ ë¶“ê³  ë‘ë¶€, ëŒ€íŒŒ, ê³ ì¶§ê°€ë£¨ë¥¼ ë„£ì–´ ë“ì¸ë‹¤. ê°„ì€ êµ­ê°„ì¥ê³¼ ìƒˆìš°ì “ìœ¼ë¡œ ë§ì¶˜ë‹¤. ë§¤ì½¤í•˜ê³  ì¹¼ì¹¼í•œ ë§›ì´ ì¼í’ˆì´ë‹¤.\"},\n",
    "    {\"recipe_name\": \"ëœì¥ì°Œê°œ\", \"content\": \"ëœì¥ì°Œê°œ ë§Œë“œëŠ” ë²•: ë©¸ì¹˜ì™€ ë‹¤ì‹œë§ˆë¡œ ìœ¡ìˆ˜ë¥¼ ë‚¸ë‹¤. ëœì¥ì„ í’€ê³  ê°ì, í˜¸ë°•, ë‘ë¶€ë¥¼ ë„£ëŠ”ë‹¤. ì²­ì–‘ê³ ì¶”ì™€ ëŒ€íŒŒë¥¼ ë„£ì–´ ë§ˆë¬´ë¦¬í•œë‹¤. êµ¬ìˆ˜í•˜ê³  ì–¼í°í•œ ë§›ì´ íŠ¹ì§•ì´ë‹¤.\"},\n",
    "    {\"recipe_name\": \"ì œìœ¡ë³¶ìŒ\", \"content\": \"ì œìœ¡ë³¶ìŒ ë§Œë“œëŠ” ë²•: ë¼ì§€ê³ ê¸°ë¥¼ ê³ ì¶§ê°€ë£¨, ê°„ì¥, ì„¤íƒ•, ë‹¤ì§„ ë§ˆëŠ˜ë¡œ ì–‘ë…í•œë‹¤. ì–‘íŒŒì™€ í•¨ê»˜ ì„¼ ë¶ˆì—ì„œ ë³¶ëŠ”ë‹¤. ëŒ€íŒŒë¥¼ ë„£ê³  ë§ˆë¬´ë¦¬í•œë‹¤. ë§¤ì½¤ë‹¬ì½¤í•œ ë§›ì´ ë°¥ë„ë‘‘ì´ë‹¤.\"},\n",
    "    {\"recipe_name\": \"ë¶ˆê³ ê¸°\", \"content\": \"ë¶ˆê³ ê¸° ë§Œë“œëŠ” ë²•: ì†Œê³ ê¸°ë¥¼ ê°„ì¥, ì„¤íƒ•, ë°°ì¦™, ì°¸ê¸°ë¦„, ë‹¤ì§„ ë§ˆëŠ˜ë¡œ ì¬ìš´ë‹¤. ì–‘íŒŒ, ë²„ì„¯ê³¼ í•¨ê»˜ ë³¶ëŠ”ë‹¤. ë‹¹ë©´ì„ ì¶”ê°€í•˜ë©´ ë” ë§›ìˆë‹¤. ë‹¬ì½¤í•˜ê³  ë¶€ë“œëŸ¬ìš´ ë§›ì´ íŠ¹ì§•ì´ë‹¤.\"},\n",
    "    {\"recipe_name\": \"ë¹„ë¹”ë°¥\", \"content\": \"ë¹„ë¹”ë°¥ ë§Œë“œëŠ” ë²•: ë°¥ ìœ„ì— ë‚˜ë¬¼(ì‹œê¸ˆì¹˜, ê³ ì‚¬ë¦¬, ë„ë¼ì§€), ê³ ê¸°, ê³„ë€ í”„ë¼ì´ë¥¼ ì˜¬ë¦°ë‹¤. ê³ ì¶”ì¥ì„ ë„£ê³  ì°¸ê¸°ë¦„ì„ ë‘ë¥¸ë‹¤. ì˜ ë¹„ë²¼ ë¨¹ëŠ”ë‹¤. ì˜ì–‘ ë§Œì  í•œ ê·¸ë¦‡ ì‹ì‚¬ë‹¤.\"},\n",
    "    \n",
    "    # Confuser 5ê°œ (ë¹„ìŠ·í•˜ì§€ë§Œ ë‹¤ë¥¸ ë ˆì‹œí”¼ - TV í”„ë¡œê·¸ë¨ ë³€í˜•)\n",
    "    {\"recipe_name\": \"ë°±ì¢…ì› ì°¸ì¹˜ê¹€ì¹˜ì°Œê°œ\", \"content\": \"ë°±ì¢…ì› ì°¸ì¹˜ê¹€ì¹˜ì°Œê°œ: ì°¸ì¹˜ìº”ì„ ê¸°ë¦„ì§¸ ë„£ëŠ” ê²Œ í¬ì¸íŠ¸ë‹¤. ê¹€ì¹˜ì™€ ì°¸ì¹˜ë¥¼ ë³¶ë‹¤ê°€ ë¬¼ì„ ë¶“ëŠ”ë‹¤. ë‘ë¶€ì™€ ëŒ€íŒŒë¥¼ ë„£ê³  ë“ì¸ë‹¤. ì°¸ì¹˜ì˜ ê°ì¹ ë§›ì´ ì¼í’ˆì´ë‹¤. ë°±ì¢…ì›ì˜ ê³¨ëª©ì‹ë‹¹ì—ì„œ ì†Œê°œëœ ë ˆì‹œí”¼ë‹¤.\"},\n",
    "    {\"recipe_name\": \"í‘ë°±ìš”ë¦¬ì‚¬ ì´ëª¨ì¹´ì„¸ ë‘ë¶€ì°Œê°œ\", \"content\": \"í‘ë°±ìš”ë¦¬ì‚¬ ì´ëª¨ì¹´ì„¸ ë‘ë¶€ì°Œê°œ: ëœì¥ê³¼ ê³ ì¶”ì¥ì„ ì„ì–´ ì“´ë‹¤. ë‘ë¶€ë¥¼ í¼ì§í•˜ê²Œ ì°ì–´ ë„£ëŠ”ë‹¤. ì²­ì–‘ê³ ì¶”ë¥¼ ë§ì´ ë„£ì–´ ë§¤ìš´ë§›ì„ ë‚¸ë‹¤. ì´ëª¨ì¹´ì„¸ ì…°í”„ì˜ ë¹„ë²•ì´ ë‹´ê¸´ ë ˆì‹œí”¼ë‹¤.\"},\n",
    "    {\"recipe_name\": \"ë°±ì¢…ì› ì œìœ¡ë³¶ìŒ\", \"content\": \"ë°±ì¢…ì› ì œìœ¡ë³¶ìŒ: ê³ ì¶”ì¥ ëŒ€ì‹  ê³ ì¶§ê°€ë£¨ë¥¼ ë§ì´ ì“´ë‹¤. ì–‘íŒŒë¥¼ ë§ì´ ë„£ì–´ ë‹¨ë§›ì„ ë‚¸ë‹¤. ë¬¼ì„ ì•½ê°„ ë„£ì–´ ìì‘í•˜ê²Œ ë“ì¸ë‹¤. ë°±ì¢…ì›ì˜ ìš”ë¦¬ë¹„ì±…ì—ì„œ ì†Œê°œëœ í™©ê¸ˆ ë ˆì‹œí”¼ë‹¤.\"},\n",
    "    {\"recipe_name\": \"ê¹€ì¹˜ë³¶ìŒë°¥\", \"content\": \"ê¹€ì¹˜ë³¶ìŒë°¥ ë§Œë“œëŠ” ë²•: ê¹€ì¹˜ë¥¼ ì˜ê²Œ ì°ì–´ ì°¸ê¸°ë¦„ì— ë³¶ëŠ”ë‹¤. ë°¥ì„ ë„£ê³  ë³¶ë‹¤ê°€ ê¹€ì¹˜êµ­ë¬¼ì„ ì¶”ê°€í•œë‹¤. ê³„ë€ í”„ë¼ì´ë¥¼ ì˜¬ë ¤ ë§ˆë¬´ë¦¬í•œë‹¤. ê°„ë‹¨í•˜ì§€ë§Œ ë§›ìˆëŠ” í•œ ë¼ ì‹ì‚¬ë‹¤.\"},\n",
    "    {\"recipe_name\": \"ìˆœë‘ë¶€ì°Œê°œ\", \"content\": \"ìˆœë‘ë¶€ì°Œê°œ ë§Œë“œëŠ” ë²•: ë©¸ì¹˜ ìœ¡ìˆ˜ì— ê³ ì¶§ê°€ë£¨ë¥¼ í’€ì–´ êµ­ë¬¼ì„ ë‚¸ë‹¤. ìˆœë‘ë¶€ë¥¼ ë„£ê³  ì¡°ê°œ, ìƒˆìš°ë¥¼ ì¶”ê°€í•œë‹¤. ê³„ë€ì„ í’€ì–´ ë„£ëŠ”ë‹¤. ì–¼í°í•˜ê³  ë¶€ë“œëŸ¬ìš´ ë§›ì´ ì¼í’ˆì´ë‹¤.\"},\n",
    "    \n",
    "    # ë…¸ì´ì¦ˆ 3ê°œ (ê´€ë ¨ ì—†ëŠ” ì •ë³´)\n",
    "    {\"recipe_name\": \"í•œì‹ ì¡°ë¦¬ ìš©ì–´\", \"content\": \"í•œì‹ ì¡°ë¦¬ ìš©ì–´ ì •ë¦¬: ë³¶ìŒ(ç‚’)ì€ ê¸°ë¦„ì— ì¬ë£Œë¥¼ ìµíˆëŠ” ê²ƒ. ì¡°ë¦¼(ç…®)ì€ ì–‘ë…ì¥ì— ì¬ë£Œë¥¼ ë„£ê³  ì¡¸ì´ëŠ” ê²ƒ. ì°œ(è’¸)ì€ ìˆ˜ì¦ê¸°ë¡œ ìµíˆëŠ” ê²ƒ. ë¬´ì¹¨ì€ ì–‘ë…ì— ë²„ë¬´ë¦¬ëŠ” ê²ƒ.\"},\n",
    "    {\"recipe_name\": \"ìŒì‹ ë³´ê´€ë²•\", \"content\": \"ìŒì‹ ë³´ê´€ë²•: ê¹€ì¹˜ëŠ” ëƒ‰ì¥ ë³´ê´€í•œë‹¤. ë‘ë¶€ëŠ” ë¬¼ì— ë‹´ê°€ ëƒ‰ì¥ ë³´ê´€í•œë‹¤. ê³ ê¸°ëŠ” ëƒ‰ë™ ë³´ê´€ì´ ì¢‹ë‹¤. ë‚˜ë¬¼ì€ ë°€í ìš©ê¸°ì— ë‹´ì•„ ëƒ‰ì¥ ë³´ê´€í•œë‹¤.\"},\n",
    "    {\"recipe_name\": \"ì¹¼ ì‚¬ìš©ë²•\", \"content\": \"ì¹¼ ì‚¬ìš©ë²•: ì±„ì¹¼ì€ ì±„ë¥¼ ì° ë•Œ, ì‹ë„ëŠ” ìƒì„ ì„ ì†ì§ˆí•  ë•Œ, ì¤‘ì‹ë„ëŠ” ê³ ê¸°ë¥¼ ìë¥¼ ë•Œ ì‚¬ìš©í•œë‹¤. ë„ë§ˆëŠ” ìš©ë„ë³„ë¡œ êµ¬ë¶„í•´ì„œ ì“´ë‹¤.\"},\n",
    "]\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "# Context Precision (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜)\n",
    "def calculate_context_precision(query, retrieved_docs, embeddings_model, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Precision = (ê²€ìƒ‰ëœ ë¬¸ì„œ ì¤‘ ê´€ë ¨ ìˆëŠ” ê²ƒ) / (ê²€ìƒ‰ëœ ì „ì²´)\n",
    "    \n",
    "    ê´€ë ¨ì„± íŒë‹¨: queryì™€ docì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ >= threshold\n",
    "    \"\"\"\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    query_embedding = embeddings_model.embed_query(query)\n",
    "    relevant_count = 0\n",
    "    \n",
    "    for doc in retrieved_docs:\n",
    "        doc_embedding = embeddings_model.embed_query(doc.page_content)\n",
    "        similarity = cosine_similarity([query_embedding], [doc_embedding])[0][0]\n",
    "        \n",
    "        if similarity >= threshold:\n",
    "            relevant_count += 1\n",
    "    \n",
    "    precision = relevant_count / len(retrieved_docs) if retrieved_docs else 0.0\n",
    "    return precision, relevant_count\n",
    "\n",
    "# Context Recall (í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜)\n",
    "def calculate_context_recall(required_recipes, retrieved_docs):\n",
    "    \"\"\"\n",
    "    Recall = (ì°¾ì€ ì •ë‹µ) / (ì‹¤ì œ ì •ë‹µ ì „ì²´)\n",
    "    \n",
    "    ì •ë‹µ íŒë‹¨: metadataì˜ recipe_nameì´ required_recipesì— ìˆëŠ”ì§€\n",
    "    \"\"\"\n",
    "    retrieved_recipes = [doc.metadata.get('recipe_name', '') for doc in retrieved_docs]\n",
    "    found_count = 0\n",
    "    for required in required_recipes:\n",
    "        if required in retrieved_recipes:\n",
    "            found_count += 1\n",
    "    recall = found_count / len(required_recipes) if required_recipes else 0.0\n",
    "    return recall, found_count\n",
    "\n",
    "print(\"âœ… ë°ì´í„° ë° í‰ê°€ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(f\"   ë ˆì‹œí”¼ ìˆ˜: {len(recipe_docs_raw)}ê°œ\")\n",
    "print(f\"   ì„ë² ë”© ëª¨ë¸: BAAI/bge-m3\")\n",
    "print()\n",
    "print(\"ğŸ’¡ í‰ê°€ ë°©ì‹:\")\n",
    "print(\"   - Precision: queryì™€ docì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ >= threshold\")\n",
    "print(\"   - Recall: required_recipesê°€ ê²€ìƒ‰ ê²°ê³¼ì— ìˆëŠ”ì§€ (metadata ê¸°ë°˜)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ì‹¤ìŠµ ë°ì´í„°: ë ˆì‹œí”¼ ì½”í¼ìŠ¤\n",
    "\n",
    "### ë°ì´í„° êµ¬ì„±\n",
    "\n",
    "**ì™œ ì´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë‚˜ìš”?**\n",
    "- íŒŒë¼ë¯¸í„° íŠœë‹ íš¨ê³¼ë¥¼ **ì‹¤ì œë¡œ ì²´í—˜**í•˜ë ¤ë©´ ì ì ˆí•œ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤\n",
    "- ë„ˆë¬´ ì§§ìœ¼ë©´ â†’ chunk_size ë³€ê²½í•´ë„ ì°¨ì´ ì—†ìŒ\n",
    "- ë„ˆë¬´ ê°„ë‹¨í•˜ë©´ â†’ í•­ìƒ ì„±ê³µí•´ì„œ íŠœë‹ ì˜ë¯¸ ì—†ìŒ\n",
    "\n",
    "**ìš°ë¦¬ê°€ ì¤€ë¹„í•œ ë°ì´í„°:**\n",
    "\n",
    "| íƒ€ì… | ê°œìˆ˜ | í‰ê·  ê¸¸ì´ | ëª©ì  |\n",
    "|------|------|-----------|------|\n",
    "| **í•µì‹¬ ë ˆì‹œí”¼** | 5ê°œ | ~900ì | ê¸´ ì¡°ë¦¬ë²• í¬í•¨ (íŒŒë¼ë¯¸í„° íŠœë‹ íš¨ê³¼ ê·¹ëŒ€í™”) |\n",
    "| **Confuser ë ˆì‹œí”¼** | 5ê°œ | ~300ì | ë¹„ìŠ·í•œ ìš”ë¦¬ (ê¹€ì¹˜ì°Œê°œ vs ì°¸ì¹˜ê¹€ì¹˜ì°Œê°œ) |\n",
    "| **ë…¸ì´ì¦ˆ ë¬¸ì„œ** | 3ê°œ | ~200ì | ì¼ë°˜ ìš”ë¦¬ ì •ë³´ |\n",
    "| **í•©ê³„** | **13ê°œ** | **~700ì** | ì¶©ë¶„í•œ ì‹¤í—˜ í™˜ê²½ |\n",
    "\n",
    "### í•µì‹¬ ë ˆì‹œí”¼ ì˜ˆì‹œ (ê¹€ì¹˜ì°Œê°œ)\n",
    "\n",
    "```\n",
    "ã€ì¬ë£Œã€‘ë¼ì§€ê³ ê¸° 200g, ë¬µì€ê¹€ì¹˜ 300g, ë‘ë¶€ 1/2ëª¨, ëŒ€íŒŒ 1ëŒ€...\n",
    "\n",
    "ã€ì¡°ë¦¬ë²•ã€‘\n",
    "1ë‹¨ê³„: ë¼ì§€ê³ ê¸°ë¥¼ ë¨¹ê¸° ì¢‹ì€ í¬ê¸°ë¡œ ìë¥¸ë‹¤\n",
    "2ë‹¨ê³„: ê¹€ì¹˜ëŠ” í•œì… í¬ê¸°ë¡œ ìë¥¸ë‹¤\n",
    "...\n",
    "```\n",
    "\n",
    "**ğŸ’¡ ì™œ ì´ë ‡ê²Œ êµ¬ì„±í–ˆë‚˜ìš”?**\n",
    "1. **ê¸¸ì´ ë‹¤ì–‘ì„±** - íŒŒë¼ë¯¸í„° íŠœë‹ íš¨ê³¼ë¥¼ í™•ì¸í•˜ë ¤ë©´ ì¶©ë¶„í•œ ê¸¸ì´ í•„ìš”\n",
    "2. **í˜¼ë€ ìš”ì†Œ** - ë¹„ìŠ·í•œ ë ˆì‹œí”¼ë¡œ Precision/Recall ì°¨ì´ í™•ì¸\n",
    "3. **ì‹¤ì „ì„±** - ì‹¤ì œ RAG ì‹œìŠ¤í…œë„ ì´ëŸ° í™˜ê²½ì—ì„œ ì‘ë™\n",
    "\n",
    "---\n",
    "\n",
    "### í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤\n",
    "\n",
    "ì‹¤ìŠµì—ì„œ ì‚¬ìš©í•  ì§ˆë¬¸ 3ê°€ì§€:\n",
    "\n",
    "1. **ê°„ë‹¨í•œ ì§ˆë¬¸** (1ê°œ ë ˆì‹œí”¼) - \"ê¹€ì¹˜ì°Œê°œ ë§Œë“œëŠ” ë²• ì•Œë ¤ì¤˜\"\n",
    "2. **ë³µì¡í•œ ì§ˆë¬¸** (2ê°œ ë ˆì‹œí”¼) - \"ì°Œê°œ ì¢…ë¥˜ ì¤‘ì— ë‘ë¶€ ë“¤ì–´ê°€ê³  ë§¤ìš´ ê±° ë­ ìˆì–´?\"\n",
    "3. **ì• ë§¤í•œ ì§ˆë¬¸** (1ê°œ ë ˆì‹œí”¼) - \"ê³ ê¸° ë“¤ì–´ê°€ëŠ” ë§¤ìš´ ìš”ë¦¬ ì¶”ì²œí•´ì¤˜\"\n",
    "\n",
    "â†’ chunk_size, top_kë¥¼ ì¡°ì ˆí•˜ë©´ì„œ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ê´€ì°°í•˜ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š ê²°ê³¼ í•´ì„ ê°€ì´ë“œ\n",
    "\n",
    "**ì‹¤í—˜ ì „ì— ê¼­ ì½ì–´ë³´ì„¸ìš”!**\n",
    "\n",
    "#### âš ï¸ Precisionê³¼ Recallì˜ ê³„ì‚° ë°©ì‹\n",
    "\n",
    "**ì´ ë…¸íŠ¸ë¶ì˜ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹:**\n",
    "\n",
    "| ì§€í‘œ | ê³„ì‚° ë°©ë²• | ì˜ˆì‹œ |\n",
    "|------|-----------|------|\n",
    "| **Precision** | (ê²€ìƒ‰ëœ ë¬¸ì„œ ì¤‘ ê´€ë ¨ ìˆëŠ” ê²ƒ) / (ê²€ìƒ‰ëœ ì „ì²´) | 3ê°œ ê²€ìƒ‰, 2ê°œ ê´€ë ¨ â†’ 2/3 = 0.67 |\n",
    "| **Recall** | (ì°¾ì€ ì •ë‹µ) / (ì‹¤ì œ ì •ë‹µ ì „ì²´) | 2ê°œ í•„ìš”, 1ê°œ ì°¾ìŒ â†’ 1/2 = 0.50 |\n",
    "\n",
    "**í˜„ì¬ êµ¬í˜„:**\n",
    "\n",
    "```python\n",
    "# Precision: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ (ìœ ì—°í•œ ê´€ë ¨ì„± íŒë‹¨)\n",
    "similarity = cosine_similarity(query_embedding, doc_embedding)\n",
    "if similarity >= 0.3:  # threshold\n",
    "    relevant_count += 1\n",
    "\n",
    "# Recall: í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ (ëª…í™•í•œ ì •ë‹µ íŒë‹¨)\n",
    "if retrieved_recipe in required_recipes:\n",
    "    found_count += 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ” ì™œ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ì¸ê°€?\n",
    "\n",
    "| ì§€í‘œ | ë°©ì‹ | ì´ìœ  |\n",
    "|------|------|------|\n",
    "| **Precision** | ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (threshold=0.3) | âœ… ì˜ë¯¸ì  ìœ ì‚¬ë„ ë°˜ì˜<br>âœ… ì‹¤ì „ RAGì™€ ì¼ì¹˜<br>âœ… ìœ ì—°í•œ ê´€ë ¨ì„± íŒë‹¨ |\n",
    "| **Recall** | í‚¤ì›Œë“œ ë§¤ì¹­ (metadata) | âœ… ì •ë‹µ/ì˜¤ë‹µ ëª…í™•<br>âœ… \"í•„ìš”í•œ ë¬¸ì„œë¥¼ ì°¾ì•˜ëŠ”ì§€\" ëª…í™•í•˜ê²Œ íŒë‹¨ |\n",
    "\n",
    "**ğŸ’¡ ì‹¤ì „ RAGì—ì„œ ê°€ì¥ ë§ì´ ì“°ëŠ” ë°©ì‹ì…ë‹ˆë‹¤!**\n",
    "- Precision: \"ê²€ìƒ‰ í’ˆì§ˆ\" â†’ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ í‰ê°€\n",
    "- Recall: \"ì •ë‹µ ì»¤ë²„ë¦¬ì§€\" â†’ ì •ë‹µ ë¬¸ì„œë¥¼ ì°¾ì•˜ëŠ”ì§€ í‰ê°€\n",
    "\n",
    "---\n",
    "\n",
    "#### â“ ì™œ Recallì€ exact matchì¸ê°€?\n",
    "\n",
    "**Q: Recallë„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ í•˜ë©´ ì•ˆ ë˜ë‚˜ìš”?**\n",
    "\n",
    "**A: ì•ˆ ë©ë‹ˆë‹¤! ì´ìœ :**\n",
    "\n",
    "**1. ì •ë‹µì„ ëª» ì°¾ì•„ë„ Recallì´ ë†’ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ**\n",
    "\n",
    "```\n",
    "ì§ˆë¬¸: \"ì°Œê°œ ì¢…ë¥˜ ì¤‘ì— ë‘ë¶€ ë“¤ì–´ê°€ê³  ë§¤ìš´ ê±° ë­ ìˆì–´?\"\n",
    "ì •ë‹µ (í•„ìš”í•œ ë ˆì‹œí”¼): [\"ê¹€ì¹˜ì°Œê°œ\", \"ëœì¥ì°Œê°œ\"]\n",
    "\n",
    "ê²€ìƒ‰ ê²°ê³¼:\n",
    "1. ìˆœë‘ë¶€ì°Œê°œ (ìœ ì‚¬ë„ 0.8) âœ… ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë†’ìŒ\n",
    "2. í‘ë°±ìš”ë¦¬ì‚¬ ë‘ë¶€ì°Œê°œ (ìœ ì‚¬ë„ 0.75) âœ… ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë†’ìŒ\n",
    "3. ë°±ì¢…ì› ì°¸ì¹˜ê¹€ì¹˜ì°Œê°œ (ìœ ì‚¬ë„ 0.7) âœ… ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë†’ìŒ\n",
    "\n",
    "âŒ ë§Œì•½ Recallë„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì“°ë©´?\n",
    "â†’ Recall = 3/2 = ... ê³„ì‚°ì´ ì• ë§¤í•¨\n",
    "â†’ ë˜ëŠ” threshold ë„˜ëŠ” ê²ƒ ì¹´ìš´íŠ¸ â†’ 3ê°œ ëª¨ë‘ ë†’ìŒ â†’ Recall ë†’ê²Œ ë‚˜ì˜´\n",
    "\n",
    "âœ… í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ”?\n",
    "â†’ ê¹€ì¹˜ì°Œê°œ, ëœì¥ì°Œê°œë¥¼ í•˜ë‚˜ë„ ëª» ì°¾ì•˜ìŒ!\n",
    "â†’ Recallì€ 0/2 = 0.0ì´ì–´ì•¼ ì •í™•í•¨\n",
    "```\n",
    "\n",
    "**2. Recallì˜ ëª©ì : \"ë‚´ê°€ ì›í•˜ëŠ” ê²ƒì„ ì°¾ì•˜ë‚˜?\"**\n",
    "\n",
    "- Precision: \"ì´ê²ƒë“¤ì´ ê´€ë ¨ ìˆë‚˜?\" (ìœ ì—°í•˜ê²Œ íŒë‹¨ OK)\n",
    "- **Recall: \"ë‚´ê°€ ì›í•˜ëŠ” ê²Œ ì—¬ê¸° ìˆë‚˜?\" (ì—„ê²©í•˜ê²Œ íŒë‹¨ í•„ìš”)**\n",
    "\n",
    "**3. ì‹¤ì „ ì˜ˆì‹œ**\n",
    "\n",
    "```\n",
    "ì‹¤ë¬´ ìƒí™©: ë²•ë¥  RAG ì‹œìŠ¤í…œ\n",
    "ì§ˆë¬¸: \"2023ë…„ ê°œì •ëœ ë¯¼ë²• ì œ750ì¡° ë‚´ìš©\"\n",
    "í•„ìš”í•œ ë¬¸ì„œ: [\"ë¯¼ë²• ì œ750ì¡° (2023 ê°œì •íŒ)\"]\n",
    "\n",
    "ê²€ìƒ‰ ê²°ê³¼:\n",
    "1. ë¯¼ë²• ì œ749ì¡° (ìœ ì‚¬ë„ 0.9) â† ê´€ë ¨ì€ ìˆì§€ë§Œ ì •ë‹µ ì•„ë‹˜\n",
    "2. ë¯¼ë²• ì œ750ì¡° (2020 ê°œì •íŒ) (ìœ ì‚¬ë„ 0.85) â† ê´€ë ¨ì€ ìˆì§€ë§Œ ì •ë‹µ ì•„ë‹˜\n",
    "3. ë¯¼ë²• ì œ751ì¡° (ìœ ì‚¬ë„ 0.8) â† ê´€ë ¨ì€ ìˆì§€ë§Œ ì •ë‹µ ì•„ë‹˜\n",
    "\n",
    "ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ Recall ê³„ì‚°í•˜ë©´?\n",
    "â†’ ë‹¤ ë†’ê²Œ ë‚˜ì˜´ â†’ Recall ë†’ìŒ\n",
    "â†’ âŒ í•˜ì§€ë§Œ ì •í™•í•œ 2023ë…„ ê°œì •íŒì„ ëª» ì°¾ìŒ!\n",
    "\n",
    "exact matchë¡œ Recall ê³„ì‚°í•˜ë©´?\n",
    "â†’ Recall = 0/1 = 0.0\n",
    "â†’ âœ… \"ì›í•˜ëŠ” ë¬¸ì„œë¥¼ ëª» ì°¾ì•˜ë‹¤\"ë¥¼ ì •í™•íˆ ë°˜ì˜\n",
    "```\n",
    "\n",
    "**ê²°ë¡ :**\n",
    "- Precisionì€ ìœ ì—°í•˜ê²Œ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„) â†’ ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€\n",
    "- Recallì€ ì—„ê²©í•˜ê²Œ (exact match) â†’ ì •ë‹µ ì»¤ë²„ë¦¬ì§€ í‰ê°€\n",
    "- ì´ ì¡°í•©ì´ ê°€ì¥ ì •í™•í•œ RAG í‰ê°€!\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ’¡ ì‹¤ì œ ì˜ˆì‹œ\n",
    "\n",
    "**ì§ˆë¬¸:** \"ì°Œê°œ ì¢…ë¥˜ ì¤‘ì— ë‘ë¶€ ë“¤ì–´ê°€ê³  ë§¤ìš´ ê±° ë­ ìˆì–´?\"\n",
    "\n",
    "**ì •ë‹µ (required_recipes):** [\"ê¹€ì¹˜ì°Œê°œ\", \"ëœì¥ì°Œê°œ\"] (2ê°œ)\n",
    "\n",
    "**ê²€ìƒ‰ ê²°ê³¼ (k=3):**\n",
    "1. ìˆœë‘ë¶€ì°Œê°œ (ìœ ì‚¬ë„: 0.42) âœ… ê´€ë ¨ ìˆìŒ (threshold=0.3 ì´ˆê³¼)\n",
    "2. í‘ë°±ìš”ë¦¬ì‚¬ ë‘ë¶€ì°Œê°œ (ìœ ì‚¬ë„: 0.38) âœ… ê´€ë ¨ ìˆìŒ\n",
    "3. ê¹€ì¹˜ì°Œê°œ (ìœ ì‚¬ë„: 0.51) âœ… ê´€ë ¨ ìˆìŒ, âœ… ì •ë‹µ!\n",
    "\n",
    "**ê³„ì‚°:**\n",
    "\n",
    "```\n",
    "Precision = (ê´€ë ¨ ìˆëŠ” ë¬¸ì„œ) / (ê²€ìƒ‰ëœ ì „ì²´)\n",
    "         = 3 / 3 = 1.00\n",
    "         (ìˆœë‘ë¶€ì°Œê°œ 0.42, ë‘ë¶€ì°Œê°œ 0.38, ê¹€ì¹˜ì°Œê°œ 0.51 ëª¨ë‘ >= 0.3)\n",
    "\n",
    "Recall = (ì°¾ì€ ì •ë‹µ) / (ì‹¤ì œ ì •ë‹µ)\n",
    "       = 1 / 2 = 0.50\n",
    "       (ê¹€ì¹˜ì°Œê°œ O, ëœì¥ì°Œê°œ X)\n",
    "```\n",
    "\n",
    "**í•´ì„:**\n",
    "- Precision 1.00 = ê²€ìƒ‰ëœ 3ê°œ ë¬¸ì„œ ëª¨ë‘ queryì™€ ê´€ë ¨ ìˆìŒ (ê²€ìƒ‰ í’ˆì§ˆ ì¢‹ìŒ)\n",
    "- Recall 0.50 = í•„ìš”í•œ ë ˆì‹œí”¼ 2ê°œ ì¤‘ 1ê°œë§Œ ì°¾ìŒ (ëœì¥ì°Œê°œ ëª» ì°¾ìŒ)\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ¤” ì™œ ì´ë ‡ê²Œ ë˜ëŠ”ê°€?\n",
    "\n",
    "**Precision ë†’ìŒ:**\n",
    "- ìˆœë‘ë¶€ì°Œê°œ, í‘ë°±ìš”ë¦¬ì‚¬ ë‘ë¶€ì°Œê°œë„ \"ì°Œê°œ + ë‘ë¶€\"ë¼ëŠ” ì¸¡ë©´ì—ì„œ ê´€ë ¨ ìˆìŒ\n",
    "- â†’ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ì˜ë¯¸ì  ê´€ë ¨ì„±ì„ í¬ì°©!\n",
    "\n",
    "**Recall ë‚®ìŒ:**\n",
    "- ëœì¥ì°Œê°œë¥¼ ëª» ì°¾ìŒ\n",
    "- â†’ chunk_sizeê°€ ë„ˆë¬´ ì‘ê±°ë‚˜, top_kê°€ ë¶€ì¡±í•¨\n",
    "\n",
    "---\n",
    "\n",
    "#### âœ… íŒŒë¼ë¯¸í„° íŠœë‹ ê°€ì´ë“œ\n",
    "\n",
    "| ìƒí™© | Precision | Recall | í•´ê²°ì±… |\n",
    "|------|-----------|--------|--------|\n",
    "| chunk ë„ˆë¬´ ì‘ìŒ | ë‚®ì„ ìˆ˜ ìˆìŒ | âš ï¸ ë‚®ìŒ | chunk â†‘ (300~500) |\n",
    "| k ë„ˆë¬´ ì‘ìŒ | ë†’ì„ ìˆ˜ ìˆìŒ | âš ï¸ ë‚®ìŒ | k â†‘ (3~5) |\n",
    "| k ë„ˆë¬´ í¼ | âš ï¸ ë‚®ìŒ | ë†’ì„ ìˆ˜ ìˆìŒ | k â†“ |\n",
    "| threshold ë„ˆë¬´ ë†’ìŒ (0.7) | âš ï¸ ë‚®ìŒ | ë³€í™” ì—†ìŒ | threshold â†“ (0.3) |\n",
    "| threshold ë„ˆë¬´ ë‚®ìŒ (0.1) | ë†’ìŒ (ë…¸ì´ì¦ˆ í¬í•¨) | ë³€í™” ì—†ìŒ | threshold â†‘ (0.3) |\n",
    "| ê· í˜• ì¡í˜ | âœ… 0.6+ | âœ… 0.8+ | ìœ ì§€! |\n",
    "\n",
    "#### ğŸ’¡ ì‹¤í—˜ íŒ\n",
    "\n",
    "1. **í•œ ë²ˆì— í•˜ë‚˜ì”©** - chunkë§Œ ë°”ê¾¸ê³  ê²°ê³¼ í™•ì¸ â†’ k ë°”ê¾¸ê¸° â†’ threshold ë°”ê¾¸ê¸°\n",
    "2. **ì™„ë²½í•œ 1.0ì´ ëª©í‘œ ì•„ë‹˜** - Precision â‰¥ 0.6, Recall â‰¥ 0.8 ì´ë©´ ì„±ê³µ!\n",
    "3. **Precision vs Recall íŠ¸ë ˆì´ë“œì˜¤í”„** - kë¥¼ ëŠ˜ë¦¬ë©´ Recallì€ ë†’ì•„ì§€ì§€ë§Œ Precisionì€ ë‚®ì•„ì§ˆ ìˆ˜ ìˆìŒ\n",
    "4. **threshold ì¡°ì ˆ** - Precisionì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ thresholdë¥¼ ë†’ì—¬ë³´ì„¸ìš” (0.3 â†’ 0.5)\n",
    "\n",
    "---\n",
    "\n",
    "### ë‹¤ìŒ: ì‹¤ì œë¡œ ì‹¤í—˜í•´ë³´ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§ª ì‹¤ìŠµ 1: Chunk Size ì‹¤í—˜\n",
    "\n",
    "**ê³¼ì œ: `chunk_size`ë¥¼ ë°”ê¿”ë³´ì„¸ìš”!**\n",
    "\n",
    "**í˜„ì¬ ì„¤ì •ì€ ì˜ë„ì ìœ¼ë¡œ ë‚˜ì©ë‹ˆë‹¤!**\n",
    "- ì´ˆê¸°ê°’: chunk_size=**50** (ê·¹ë‹¨ì ìœ¼ë¡œ ì‘ìŒ ğŸ˜±)\n",
    "- ë¬¸ì œ: ë¬¸ë§¥ì´ ê·¹ë„ë¡œ ë¶€ì¡± â†’ ì¼ë¶€ ì§ˆë¬¸ì—ì„œ ì‹¤íŒ¨\n",
    "\n",
    "**ê°œì„  ë°©ë²•:**\n",
    "- 50 (í˜„ì¬) â†’ ìµœì•…! ë¬¸ë§¥ ê±°ì˜ ì—†ìŒ âŒ\n",
    "- 100 (ë§¤ìš° ì‘ìŒ) â†’ ë¬¸ë§¥ ë¶€ì¡±\n",
    "- 300 (ì ë‹¹) â†’ ì¢‹ìŒ! âœ…\n",
    "- 500 (ë³´í†µ) â†’ ê· í˜• âœ…\n",
    "- 1000 (í¼) â†’ ê´œì°®ìŒ\n",
    "- 2000 (ë§¤ìš° í¼) â†’ ë…¸ì´ì¦ˆ ë§ìŒ\n",
    "\n",
    "**ì¤‘ìš”í•œ ì´í•´:**\n",
    "- **ê°„ë‹¨í•œ ì§ˆë¬¸**ì—ì„œëŠ” chunk=50ë„ ì„±ê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "- í•˜ì§€ë§Œ **ë³µì¡í•œ ì§ˆë¬¸**ì´ë‚˜ **ì• ë§¤í•œ ì§ˆë¬¸**ì—ì„œëŠ” ì‹¤íŒ¨í•©ë‹ˆë‹¤\n",
    "- ëª©í‘œ: **ì•ˆì •ì ìœ¼ë¡œ** ì‘ë™í•˜ëŠ” ì„¤ì • ì°¾ê¸°!\n",
    "\n",
    "**ê´€ì°° í¬ì¸íŠ¸:**\n",
    "- chunk_sizeë¥¼ ëŠ˜ë¦¬ë©´ ì–´ë–»ê²Œ ë³€í•˜ë‚˜ìš”?\n",
    "- ì–´ë–¤ í¬ê¸°ê°€ ê°€ì¥ ê· í˜•ì¡í˜”ë‚˜ìš”?\n",
    "- chunk ê°œìˆ˜ê°€ ì–´ë–»ê²Œ ë³€í•˜ë‚˜ìš”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”! ğŸ‘‡\n",
    "# ============================================\n",
    "chunk_size = 50   # â† ì´ ìˆ«ìë¥¼ ë°”ê¿”ë³´ì„¸ìš”! (50, 100, 300, 500, 1000)\n",
    "                  # âš ï¸ í˜„ì¬ 50ì€ ì˜ë„ì ìœ¼ë¡œ ë‚˜ìœ ì„¤ì •ì…ë‹ˆë‹¤!\n",
    "                  # ğŸ’¡ 300~500ìœ¼ë¡œ ë°”ê¿”ë³´ì„¸ìš”!\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# Text Splitter ìƒì„±\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# ë¬¸ì„œ ë¶„í• \n",
    "documents = []\n",
    "for doc_data in recipe_docs_raw:\n",
    "    chunks = text_splitter.split_text(doc_data[\"content\"])\n",
    "    for chunk in chunks:\n",
    "        documents.append(Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                \"recipe_name\": doc_data[\"recipe_name\"]\n",
    "            }\n",
    "        ))\n",
    "\n",
    "# ë²¡í„°ìŠ¤í† ì–´ ìƒì„±\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "print(f\"âœ… Chunk Size = {chunk_size}ìë¡œ ì„¤ì •!\")\n",
    "print(f\"   ì „ì²´ ë ˆì‹œí”¼: {len(recipe_docs_raw)}ê°œ\")\n",
    "print(f\"   ë¶„í•  í›„ Chunk: {len(documents)}ê°œ\")\n",
    "print(f\"   í‰ê·  Chunk í¬ê¸°: {np.mean([len(d.page_content) for d in documents]):.0f}ì\")\n",
    "print()\n",
    "\n",
    "if chunk_size >= 1500:\n",
    "    print(\"âš ï¸ ê²½ê³ : chunk_sizeê°€ ë„ˆë¬´ í½ë‹ˆë‹¤!\")\n",
    "    print(\"   â†’ ê¸´ ë ˆì‹œí”¼ê°€ 1ê°œ chunkë¡œ â†’ ë…¸ì´ì¦ˆ ë§ìŒ\")\n",
    "    print(\"   ğŸ’¡ 300~500ìœ¼ë¡œ ì¤„ì—¬ë³´ì„¸ìš”!\")\n",
    "elif chunk_size < 100:\n",
    "    print(\"âš ï¸ ê²½ê³ : chunk_sizeê°€ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤!\")\n",
    "    print(\"   â†’ ë¬¸ë§¥ì´ ê·¹ë„ë¡œ ë¶€ì¡± â†’ ê²€ìƒ‰ ì •í™•ë„ ë§¤ìš° ë‚®ìŒ\")\n",
    "    print(\"   ğŸ’¡ 300~500ìœ¼ë¡œ ëŠ˜ë ¤ë³´ì„¸ìš”!\")\n",
    "elif chunk_size < 200:\n",
    "    print(\"âš ï¸ ê²½ê³ : chunk_sizeê°€ ì‘ìŠµë‹ˆë‹¤!\")\n",
    "    print(\"   â†’ ë¬¸ë§¥ ë¶€ì¡± â†’ ê²€ìƒ‰ ì •í™•ë„ ë‚®ìŒ\")\n",
    "    print(\"   ğŸ’¡ 300~500ìœ¼ë¡œ ëŠ˜ë ¤ë³´ì„¸ìš”!\")\n",
    "else:\n",
    "    print(\"âœ… ì ì ˆí•œ chunk_sizeì…ë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ (ë³µì¡í•œ ì§ˆë¬¸ - ì—¬ëŸ¬ ë ˆì‹œí”¼ ê´€ë ¨!)\n",
    "test_query = \"ì°Œê°œ ì¢…ë¥˜ ì¤‘ì— ë‘ë¶€ ë“¤ì–´ê°€ê³  ë§¤ìš´ ê±° ë­ ìˆì–´?\"\n",
    "required_recipes = [\"ê¹€ì¹˜ì°Œê°œ\", \"ëœì¥ì°Œê°œ\"]\n",
    "\n",
    "# ê²€ìƒ‰\n",
    "results = vectorstore.similarity_search(test_query, k=3)\n",
    "\n",
    "# í‰ê°€\n",
    "precision, rel_count = calculate_context_precision(test_query, results, embeddings, threshold=0.3)\n",
    "recall, found_count = calculate_context_recall(required_recipes, results)\n",
    "\n",
    "# ì¶œë ¥\n",
    "print(f\"ğŸ” ì§ˆë¬¸: '{test_query}'\")\n",
    "print(f\"   ì •ë‹µ: {required_recipes}\")\n",
    "print()\n",
    "print(f\"ğŸ“Š ê²°ê³¼ (Chunk={chunk_size}, k=3):\")\n",
    "print(f\"   Precision: {precision:.2f} ({rel_count}/3 ë¬¸ì„œê°€ ê´€ë ¨ ìˆìŒ)\")\n",
    "print(f\"   Recall: {recall:.2f} ({found_count}/{len(required_recipes)} í•„ìš” ë ˆì‹œí”¼ ì°¾ìŒ)\")\n",
    "print()\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë ˆì‹œí”¼ ì´ë¦„ë§Œ ì¶œë ¥\n",
    "print(\"ğŸ“ ê²€ìƒ‰ëœ ë ˆì‹œí”¼:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    recipe_name = doc.metadata.get('recipe_name', 'Unknown')\n",
    "    print(f\"   {i}. {recipe_name}\")\n",
    "print()\n",
    "\n",
    "# ê°„ë‹¨ í•´ì„\n",
    "if recall < 0.5:\n",
    "    print(\"âš ï¸ Recall ë‚®ìŒ: chunk_sizeë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš”!\")\n",
    "elif recall >= 0.8 and precision >= 0.6:\n",
    "    print(\"âœ… ì¢‹ì€ ê²°ê³¼!\")\n",
    "else:\n",
    "    print(\"ğŸ’¡ íŒŒë¼ë¯¸í„° ì¡°ì •ìœ¼ë¡œ ê°œì„  ê°€ëŠ¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ (ì™„ì „ ê°„ì ‘ì  - í‚¤ì›Œë“œ ì™„ì „ ì œê±°!)\n",
    "test_query = \"ê³ ê¸° ë“¤ì–´ê°€ëŠ” ë§¤ìš´ ìš”ë¦¬ ì¶”ì²œí•´ì¤˜\"\n",
    "required_recipes = [\"ê¹€ì¹˜ì°Œê°œ\"]\n",
    "\n",
    "# ê²€ìƒ‰\n",
    "results = vectorstore.similarity_search(test_query, k=3)\n",
    "\n",
    "# í‰ê°€\n",
    "precision, rel_count = calculate_context_precision(test_query, results, embeddings, threshold=0.3)\n",
    "recall, found_count = calculate_context_recall(required_recipes, results)\n",
    "\n",
    "# ì¶œë ¥\n",
    "print(f\"ğŸ” ì§ˆë¬¸: '{test_query}'\")\n",
    "print(f\"   ì •ë‹µ: {required_recipes}\")\n",
    "print()\n",
    "print(f\"ğŸ“Š ê²°ê³¼ (Chunk={chunk_size}, k=3):\")\n",
    "print(f\"   Precision: {precision:.2f} ({rel_count}/3 ë¬¸ì„œê°€ ê´€ë ¨ ìˆìŒ)\")\n",
    "print(f\"   Recall: {recall:.2f} ({found_count}/{len(required_recipes)} í•„ìš” ë ˆì‹œí”¼ ì°¾ìŒ)\")\n",
    "print()\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë ˆì‹œí”¼ ì´ë¦„ë§Œ ì¶œë ¥\n",
    "print(\"ğŸ“ ê²€ìƒ‰ëœ ë ˆì‹œí”¼:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    recipe_name = doc.metadata.get('recipe_name', 'Unknown')\n",
    "    print(f\"   {i}. {recipe_name}\")\n",
    "print()\n",
    "\n",
    "if recall == 0:\n",
    "    print(\"âš ï¸ ì •ë‹µì„ ëª» ì°¾ìŒ: ê°„ì ‘ì  ì§ˆë¬¸ì€ chunk_sizeâ†‘ ë˜ëŠ” kâ†‘ í•„ìš”\")\n",
    "elif recall >= 1.0 and precision >= 0.6:\n",
    "    print(\"âœ… ê°„ì ‘ ì§ˆë¬¸ë„ ì„±ê³µ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”! ğŸ‘‡\n",
    "# ============================================\n",
    "top_k = 1  # â† ì´ ìˆ«ìë¥¼ ë°”ê¿”ë³´ì„¸ìš”! (1, 3, 5, 10)\n",
    "           # âš ï¸ í˜„ì¬ 1ì€ ì˜ë„ì ìœ¼ë¡œ ë‚˜ìœ ì„¤ì •ì…ë‹ˆë‹¤!\n",
    "           # ğŸ’¡ 3~5ë¡œ ë°”ê¿”ë³´ì„¸ìš”!\n",
    "# ============================================\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸\n",
    "test_query = \"ê³ ê¸° ë“¤ì–´ê°€ëŠ” ë§¤ì½¤í•œ ë³¶ìŒìš”ë¦¬ ì¶”ì²œí•´ì¤˜\"\n",
    "required_recipes = [\"ì œìœ¡ë³¶ìŒ\", \"ë°±ì¢…ì› ì œìœ¡ë³¶ìŒ\"]\n",
    "\n",
    "# ê²€ìƒ‰\n",
    "results = vectorstore.similarity_search(test_query, k=top_k)\n",
    "\n",
    "# í‰ê°€\n",
    "precision, rel_count = calculate_context_precision(test_query, results, embeddings, threshold=0.3)\n",
    "recall, found_count = calculate_context_recall(required_recipes, results)\n",
    "\n",
    "# ì¶œë ¥\n",
    "print(f\"ğŸ” ì§ˆë¬¸: '{test_query}'\")\n",
    "print(f\"   ì •ë‹µ: {required_recipes}\")\n",
    "print()\n",
    "print(f\"ğŸ“Š ê²°ê³¼ (k={top_k}):\")\n",
    "print(f\"   Precision: {precision:.2f} ({rel_count}/{top_k} ë¬¸ì„œê°€ ê´€ë ¨ ìˆìŒ)\")\n",
    "print(f\"   Recall: {recall:.2f} ({found_count}/{len(required_recipes)} í•„ìš” ë ˆì‹œí”¼ ì°¾ìŒ)\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ“ ê²€ìƒ‰ëœ ë ˆì‹œí”¼:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    recipe_name = doc.metadata.get('recipe_name', 'Unknown')\n",
    "    print(f\"   {i}. {recipe_name}\")\n",
    "print()\n",
    "\n",
    "if top_k < 3:\n",
    "    print(\"âš ï¸ kê°€ ë„ˆë¬´ ì‘ìŒ: Recallì´ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n",
    "elif recall >= 0.8:\n",
    "    print(\"âœ… ì¢‹ì€ Recall!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ (ê°„ë‹¨í•œ ì§ˆë¬¸)\n",
    "test_query = \"ê¹€ì¹˜ì°Œê°œ ë§Œë“œëŠ” ë²• ì•Œë ¤ì¤˜\"\n",
    "required_recipes = [\"ê¹€ì¹˜ì°Œê°œ\"]\n",
    "\n",
    "# ê²€ìƒ‰\n",
    "results = vectorstore.similarity_search(test_query, k=top_k)\n",
    "\n",
    "# í‰ê°€\n",
    "precision, rel_count = calculate_context_precision(test_query, results, embeddings, threshold=0.3)\n",
    "recall, found_count = calculate_context_recall(required_recipes, results)\n",
    "\n",
    "# ì¶œë ¥\n",
    "print(f\"ğŸ” ì§ˆë¬¸: '{test_query}'\")\n",
    "print(f\"   ì •ë‹µ: {required_recipes}\")\n",
    "print()\n",
    "print(f\"ğŸ“Š ê²°ê³¼ (k={top_k}):\")\n",
    "print(f\"   Precision: {precision:.2f} ({rel_count}/{top_k} ë¬¸ì„œê°€ ê´€ë ¨ ìˆìŒ)\")\n",
    "print(f\"   Recall: {recall:.2f} ({found_count}/{len(required_recipes)} í•„ìš” ë ˆì‹œí”¼ ì°¾ìŒ)\")\n",
    "print()\n",
    "\n",
    "if recall >= 1.0:\n",
    "    print(\"âœ… ê°„ë‹¨í•œ ì§ˆë¬¸ì€ ì„±ê³µ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”! ğŸ‘‡\n",
    "# ============================================\n",
    "threshold = 0.7  # â† ì´ ìˆ«ìë¥¼ ë°”ê¿”ë³´ì„¸ìš”! (0.1, 0.3, 0.5, 0.7)\n",
    "                 # âš ï¸ í˜„ì¬ 0.7ì€ ì˜ë„ì ìœ¼ë¡œ ë†’ì€ ì„¤ì •ì…ë‹ˆë‹¤!\n",
    "                 # ğŸ’¡ 0.3~0.5ë¡œ ë°”ê¿”ë³´ì„¸ìš”!\n",
    "# ============================================\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸\n",
    "test_query = \"ê°„ë‹¨í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ë°¥ ìš”ë¦¬\"\n",
    "required_recipes = [\"ê¹€ì¹˜ë³¶ìŒë°¥\"]\n",
    "\n",
    "# ê²€ìƒ‰\n",
    "results = vectorstore.similarity_search(test_query, k=3)\n",
    "\n",
    "# í‰ê°€\n",
    "precision, rel_count = calculate_context_precision(test_query, results, embeddings, threshold=threshold)\n",
    "recall, found_count = calculate_context_recall(required_recipes, results)\n",
    "\n",
    "# ì¶œë ¥\n",
    "print(f\"ğŸ” ì§ˆë¬¸: '{test_query}'\")\n",
    "print(f\"   ì •ë‹µ: {required_recipes}\")\n",
    "print()\n",
    "print(f\"ğŸ“Š ê²°ê³¼ (threshold={threshold}):\")\n",
    "print(f\"   Precision: {precision:.2f} ({rel_count}/3 ë¬¸ì„œê°€ ê´€ë ¨ ìˆìŒ)\")\n",
    "print(f\"   Recall: {recall:.2f} ({found_count}/{len(required_recipes)} í•„ìš” ë ˆì‹œí”¼ ì°¾ìŒ)\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ“ ê²€ìƒ‰ëœ ë ˆì‹œí”¼:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    recipe_name = doc.metadata.get('recipe_name', 'Unknown')\n",
    "    print(f\"   {i}. {recipe_name}\")\n",
    "print()\n",
    "\n",
    "if threshold > 0.5:\n",
    "    print(\"âš ï¸ threshold ë†’ìŒ: Precisionì´ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n",
    "elif precision >= 0.6 and recall >= 0.8:\n",
    "    print(\"âœ… ê· í˜•ì¡íŒ ì„¤ì •!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ (ì• ë§¤í•œ ì§ˆë¬¸)\n",
    "test_query = \"ì•„ì¹¨ ë©”ë‰´ë¡œ ì¢‹ì€ ê±°\"\n",
    "required_recipes = [\"ê¹€ì¹˜ë³¶ìŒë°¥\"]\n",
    "\n",
    "# ê²€ìƒ‰\n",
    "results = vectorstore.similarity_search(test_query, k=3)\n",
    "\n",
    "# í‰ê°€\n",
    "precision, rel_count = calculate_context_precision(test_query, results, embeddings, threshold=threshold)\n",
    "recall, found_count = calculate_context_recall(required_recipes, results)\n",
    "\n",
    "# ì¶œë ¥\n",
    "print(f\"ğŸ” ì§ˆë¬¸: '{test_query}'\")\n",
    "print(f\"   ì •ë‹µ: {required_recipes}\")\n",
    "print()\n",
    "print(f\"ğŸ“Š ê²°ê³¼ (threshold={threshold}):\")\n",
    "print(f\"   Precision: {precision:.2f} ({rel_count}/3 ë¬¸ì„œê°€ ê´€ë ¨ ìˆìŒ)\")\n",
    "print(f\"   Recall: {recall:.2f} ({found_count}/{len(required_recipes)} í•„ìš” ë ˆì‹œí”¼ ì°¾ìŒ)\")\n",
    "print()\n",
    "\n",
    "if recall == 0:\n",
    "    print(\"âš ï¸ ì• ë§¤í•œ ì§ˆë¬¸: thresholdâ†“ ë˜ëŠ” kâ†‘ í•„ìš”\")\n",
    "elif recall >= 1.0:\n",
    "    print(\"âœ… ì• ë§¤í•œ ì§ˆë¬¸ë„ ì„±ê³µ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”! ğŸ‘‡\n",
    "# ============================================\n",
    "my_chunk_size = 100  # â† 300~500 ì¶”ì²œ!\n",
    "my_top_k = 1         # â† 3~5 ì¶”ì²œ!\n",
    "my_threshold = 0.7   # â† 0.3~0.5 ì¶”ì²œ!\n",
    "\n",
    "# âš ï¸ í˜„ì¬ ì„¤ì •ì€ ì˜ë„ì ìœ¼ë¡œ ë‚˜ì©ë‹ˆë‹¤!\n",
    "# ìœ„ 3ê°œ íŒŒë¼ë¯¸í„°ë¥¼ ëª¨ë‘ ë°”ê¿”ì„œ ìµœê³  ì„±ëŠ¥ì„ ë§Œë“¤ì–´ë³´ì„¸ìš”!\n",
    "# ============================================\n",
    "\n",
    "# ë²¡í„°ìŠ¤í† ì–´ ì¬ìƒì„±\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=my_chunk_size,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "documents = []\n",
    "for doc_data in recipe_docs_raw:\n",
    "    chunks = text_splitter.split_text(doc_data[\"content\"])\n",
    "    for chunk in chunks:\n",
    "        documents.append(Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\"recipe_name\": doc_data[\"recipe_name\"]}\n",
    "        ))\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "print(\"ğŸ¯ ë‚˜ë§Œì˜ ìµœì  ì„¤ì • í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"chunk_size={my_chunk_size}, top_k={my_top_k}, threshold={my_threshold}\")\n",
    "print()\n",
    "\n",
    "# 5ê°œ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸\n",
    "test_cases = [\n",
    "    {\"query\": \"ì°Œê°œ ì¢…ë¥˜ ì¤‘ì— ë‘ë¶€ ë“¤ì–´ê°€ê³  ë§¤ìš´ ê±° ë­ ìˆì–´?\", \"required\": [\"ê¹€ì¹˜ì°Œê°œ\", \"ëœì¥ì°Œê°œ\"]},\n",
    "    {\"query\": \"ê³ ê¸° ë“¤ì–´ê°€ëŠ” ë§¤ìš´ ìš”ë¦¬ ì¶”ì²œí•´ì¤˜\", \"required\": [\"ê¹€ì¹˜ì°Œê°œ\"]},\n",
    "    {\"query\": \"ê³ ê¸° ë“¤ì–´ê°€ëŠ” ë§¤ì½¤í•œ ë³¶ìŒìš”ë¦¬\", \"required\": [\"ì œìœ¡ë³¶ìŒ\", \"ë°±ì¢…ì› ì œìœ¡ë³¶ìŒ\"]},\n",
    "    {\"query\": \"ê°„ë‹¨í•œ ë°¥ ìš”ë¦¬\", \"required\": [\"ê¹€ì¹˜ë³¶ìŒë°¥\"]},\n",
    "    {\"query\": \"í•œ ê·¸ë¦‡ ì‹ì‚¬\", \"required\": [\"ë¹„ë¹”ë°¥\"]},\n",
    "]\n",
    "\n",
    "total_precision = 0\n",
    "total_recall = 0\n",
    "\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    results = vectorstore.similarity_search(test[\"query\"], k=my_top_k)\n",
    "    precision, rel_count = calculate_context_precision(test[\"query\"], results, embeddings, threshold=my_threshold)\n",
    "    recall, found_count = calculate_context_recall(test[\"required\"], results)\n",
    "    \n",
    "    total_precision += precision\n",
    "    total_recall += recall\n",
    "    \n",
    "    print(f\"[{i}] {test['query'][:30]}...\")\n",
    "    print(f\"    Precision: {precision:.2f}, Recall: {recall:.2f}\")\n",
    "\n",
    "avg_precision = total_precision / len(test_cases)\n",
    "avg_recall = total_recall / len(test_cases)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ“Š í‰ê·  Precision: {avg_precision:.2f}\")\n",
    "print(f\"ğŸ“Š í‰ê·  Recall: {avg_recall:.2f}\")\n",
    "print()\n",
    "\n",
    "if avg_precision >= 0.6 and avg_recall >= 0.8:\n",
    "    print(\"ğŸ‰ ì„±ê³µ! ìµœì  ì„¤ì •ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!\")\n",
    "elif avg_recall < 0.5:\n",
    "    print(\"âš ï¸ Recall ë‚®ìŒ: chunk_sizeâ†‘ ë˜ëŠ” top_kâ†‘ ì¶”ì²œ\")\n",
    "elif avg_precision < 0.4:\n",
    "    print(\"âš ï¸ Precision ë‚®ìŒ: thresholdâ†‘ ë˜ëŠ” top_kâ†“ ì¶”ì²œ\")\n",
    "else:\n",
    "    print(\"ğŸ’¡ ì¡°ê¸ˆë§Œ ë” ì¡°ì •í•˜ë©´ ì™„ë²½!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ì—¬ëŸ¬ë¶„ì˜ ìµœì  ì„¤ì •ì„ ì°¾ì•„ë³´ì„¸ìš”! ğŸ‘‡\n",
    "# ============================================\n",
    "\n",
    "# âš ï¸ ê²½ê³ : ì•„ë˜ ì„¤ì •ì€ ì˜ë„ì ìœ¼ë¡œ ê·¹ë‹¨ì ì…ë‹ˆë‹¤!\n",
    "# ë‹¨ìˆœí•œ ì§ˆë¬¸ì—ì„œëŠ” ì„±ê³µí•  ìˆ˜ ìˆì§€ë§Œ, ë³µì¡í•œ ì§ˆë¬¸ì—ì„œ ì‹¤íŒ¨í•©ë‹ˆë‹¤!\n",
    "\n",
    "# ë‹¨ìˆœ ì§ˆë¬¸ìš© ì„¤ì •\n",
    "config_simple = {\n",
    "    \"chunk_size\": 50,\n",
    "    \"top_k\": 1,\n",
    "    \"threshold\": 0.8\n",
    "}\n",
    "\n",
    "# ì¤‘ê°„ ì§ˆë¬¸ìš© ì„¤ì •\n",
    "config_medium = {\n",
    "    \"chunk_size\": 50,\n",
    "    \"top_k\": 2,\n",
    "    \"threshold\": 0.75\n",
    "}\n",
    "\n",
    "# ë³µí•© ì§ˆë¬¸ìš© ì„¤ì •\n",
    "config_complex = {\n",
    "    \"chunk_size\": 50,\n",
    "    \"top_k\": 1,\n",
    "    \"threshold\": 0.9\n",
    "}\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ (ì™„ì „ ê°„ì ‘ì  ì§ˆë¬¸!)\n",
    "test_cases = [\n",
    "    {\n",
    "        \"type\": \"ë‹¨ìˆœ\",\n",
    "        \"query\": \"ë§¤ìš´ êµ­ë¬¼ ìš”ë¦¬ ì¶”ì²œí•´ì¤˜\",\n",
    "        \"required\": [\"ê¹€ì¹˜ì°Œê°œ\"],\n",
    "        \"config\": config_simple\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"ì¤‘ê°„\",\n",
    "        \"query\": \"ë°±ì¢…ì› ë ˆì‹œí”¼ ë­ìˆì–´?\",\n",
    "        \"required\": [\"ë°±ì¢…ì› ì°¸ì¹˜ê¹€ì¹˜ì°Œê°œ\", \"ë°±ì¢…ì› ì œìœ¡ë³¶ìŒ\"],\n",
    "        \"config\": config_medium\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"ë³µí•©\",\n",
    "        \"query\": \"ìš”ë¦¬ ì¶”ì²œí•´ì¤˜\",\n",
    "        \"required\": [\"ê¹€ì¹˜ì°Œê°œ\", \"ì œìœ¡ë³¶ìŒ\", \"ë¶ˆê³ ê¸°\"],\n",
    "        \"config\": config_complex\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"ğŸ¯ ê°„ì ‘ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import pandas as pd\n",
    "\n",
    "results_summary = []\n",
    "\n",
    "for test in test_cases:\n",
    "    config = test[\"config\"]\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=config[\"chunk_size\"],\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "    \n",
    "    temp_docs = []\n",
    "    for doc_data in recipe_docs_raw:\n",
    "        chunks = text_splitter.split_text(doc_data[\"content\"])\n",
    "        for chunk in chunks:\n",
    "            temp_docs.append(Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\"recipe_name\": doc_data[\"recipe_name\"]}\n",
    "            ))\n",
    "    \n",
    "    temp_vectorstore = FAISS.from_documents(temp_docs, embeddings)\n",
    "    search_results = temp_vectorstore.similarity_search(test[\"query\"], k=config[\"top_k\"])\n",
    "    \n",
    "    precision, _ = calculate_context_precision(test[\"query\"], search_results, embeddings, threshold=config[\"threshold\"])\n",
    "    recall, _ = calculate_context_recall(test[\"required\"], search_results)\n",
    "    \n",
    "    success = \"âœ…\" if (precision >= 0.6 and recall >= 0.8) else \"âš ï¸\"\n",
    "    \n",
    "    print(f\"\\n[{test['type']}] \\\"{test['query']}\\\"\")\n",
    "    print(f\"   Precision={precision:.2f}, Recall={recall:.2f} {success}\")\n",
    "    \n",
    "    results_summary.append({\n",
    "        \"ì§ˆë¬¸ íƒ€ì…\": test[\"type\"],\n",
    "        \"Precision\": f\"{precision:.2f}\",\n",
    "        \"Recall\": f\"{recall:.2f}\",\n",
    "        \"í†µê³¼\": success\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "df = pd.DataFrame(results_summary)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "success_count = sum(1 for r in results_summary if r[\"í†µê³¼\"] == \"âœ…\")\n",
    "total_count = len(results_summary)\n",
    "print(f\"\\nğŸ¯ ì„±ê³µë¥ : {success_count}/{total_count} ({success_count/total_count*100:.0f}%)\")\n",
    "\n",
    "if success_count == 0:\n",
    "    print(\"\\nâš ï¸ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!\")\n",
    "    print(\"   â†’ ìœ„ì—ì„œ config ê°’ë“¤ì„ ìˆ˜ì •í•˜ê³  ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”!\")\n",
    "    print()\n",
    "    print(\"ğŸ’¡ ê¶Œì¥ ê°œì„  ë°©í–¥:\")\n",
    "    print(\"   1. chunk_size: 50 â†’ 300~500\")\n",
    "    print(\"   2. top_k: 1~2 â†’ 3~5\")\n",
    "    print(\"   3. threshold: 0.75~0.9 â†’ 0.3~0.5\")\n",
    "elif success_count < total_count:\n",
    "    print(\"\\nâš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!\")\n",
    "    print(\"   â†’ ì‹¤íŒ¨í•œ ì¼€ì´ìŠ¤ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•´ë³´ì„¸ìš”!\")\n",
    "else:\n",
    "    print(\"\\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì„±ê³µ! ì¶•í•˜í•©ë‹ˆë‹¤!\")\n",
    "    print(\"   â†’ ì—¬ëŸ¬ë¶„ë§Œì˜ ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Part 2: ë‚˜ë§Œì˜ í‰ê°€ ë°ì´í„°ì…‹ ë§Œë“¤ê¸°\n",
    "\n",
    "### í•µì‹¬ ì•„ì´ë””ì–´\n",
    "\n",
    "**\"ë‚´ê°€ ê´€ì‹¬ìˆëŠ” ë„ë©”ì¸ìœ¼ë¡œ RAGë¥¼ í…ŒìŠ¤íŠ¸í•´ë³´ì!\"**\n",
    "\n",
    "### ë°ì´í„°ì…‹ ë§Œë“¤ê¸° 3ë‹¨ê³„\n",
    "\n",
    "1. **ë„ë©”ì¸ ì„ íƒ**: ë ˆì‹œí”¼, ì˜í™”, IT, ê²Œì„ ê³µëµ ë“±\n",
    "2. **í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‘ì„±**: ì§ˆë¬¸ 10ê°œ + ì •ë‹µ ì‘ì„±\n",
    "3. **ìë™ í‰ê°€**: 07ë²ˆ RAGASë¡œ í‰ê°€\n",
    "\n",
    "### í…œí”Œë¦¿\n",
    "\n",
    "ì•„ë˜ í…œí”Œë¦¿ì„ ì°¸ê³ í•´ì„œ ì§ì ‘ ë§Œë“¤ì–´ë³´ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ì—¬ëŸ¬ë¶„ë§Œì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ë§Œë“¤ì–´ë³´ì„¸ìš”! ğŸ‘‡\n",
    "# ============================================\n",
    "\n",
    "my_custom_dataset = [\n",
    "    # ì˜ˆì‹œ 1: ê°„ë‹¨í•œ ì§ˆë¬¸\n",
    "    {\n",
    "        \"query\": \"ë§¤ìš´ êµ­ë¬¼ ìš”ë¦¬\",\n",
    "        \"required_recipes\": [\"ê¹€ì¹˜ì°Œê°œ\"]\n",
    "    },\n",
    "    \n",
    "    # ì˜ˆì‹œ 2: êµ¬ì²´ì  ì§ˆë¬¸\n",
    "    {\n",
    "        \"query\": \"ë¼ì§€ê³ ê¸° ë³¶ìŒ ìš”ë¦¬\",\n",
    "        \"required_recipes\": [\"ì œìœ¡ë³¶ìŒ\"]\n",
    "    },\n",
    "    \n",
    "    # ì˜ˆì‹œ 3: ë³µí•© ì§ˆë¬¸\n",
    "    {\n",
    "        \"query\": \"ì°Œê°œ ì¢…ë¥˜ ì¶”ì²œí•´ì¤˜\",\n",
    "        \"required_recipes\": [\"ê¹€ì¹˜ì°Œê°œ\", \"ëœì¥ì°Œê°œ\"]\n",
    "    },\n",
    "    \n",
    "    # ============================================\n",
    "    # ì•„ë˜ì— ì—¬ëŸ¬ë¶„ë§Œì˜ ì¼€ì´ìŠ¤ë¥¼ ì¶”ê°€í•˜ì„¸ìš”!\n",
    "    # ============================================\n",
    "    \n",
    "    # ì˜ˆì‹œ 4: ì—¬ëŸ¬ë¶„ì˜ ì¼€ì´ìŠ¤\n",
    "    # {\n",
    "    #     \"query\": \"ì—¬ëŸ¬ë¶„ì˜ ì§ˆë¬¸\",\n",
    "    #     \"required_recipes\": [\"ì •ë‹µ ë ˆì‹œí”¼1\", \"ì •ë‹µ ë ˆì‹œí”¼2\"]\n",
    "    # },\n",
    "]\n",
    "\n",
    "print(f\"âœ… ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(f\"   ì´ {len(my_custom_dataset)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤\")\n",
    "print()\n",
    "print(\"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤:\")\n",
    "for i, case in enumerate(my_custom_dataset, 1):\n",
    "    print(f\"   {i}. {case['query']}\")\n",
    "    print(f\"      â†’ ì •ë‹µ: {case['required_recipes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í‰ê°€\n",
    "print(\"ğŸ”¬ ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í‰ê°€ ì‹œì‘\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ë²¡í„°ìŠ¤í† ì–´ ì¬ìƒì„±\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "eval_docs = []\n",
    "\n",
    "for doc_data in recipe_docs_raw:\n",
    "    chunks = text_splitter.split_text(doc_data[\"content\"])\n",
    "    for chunk in chunks:\n",
    "        eval_docs.append(Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\"recipe_name\": doc_data[\"recipe_name\"]}\n",
    "        ))\n",
    "\n",
    "eval_vectorstore = FAISS.from_documents(eval_docs, embeddings)\n",
    "\n",
    "# ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹\n",
    "my_custom_dataset = [\n",
    "    {\"query\": \"ê¹€ì¹˜ ë“¤ì–´ê°€ëŠ” ìŒì‹ ë­ìˆì–´?\", \"required\": [\"ê¹€ì¹˜ì°Œê°œ\", \"ë°±ì¢…ì› ì°¸ì¹˜ê¹€ì¹˜ì°Œê°œ\", \"ê¹€ì¹˜ë³¶ìŒë°¥\"]},\n",
    "    {\"query\": \"ì•„ì¹¨ì— ë¹¨ë¦¬ ë§Œë“¤ ìˆ˜ ìˆëŠ” ê±° ì¶”ì²œ\", \"required\": [\"ê¹€ì¹˜ë³¶ìŒë°¥\"]},\n",
    "    {\"query\": \"ì°Œê°œ ì¢…ë¥˜ ì•Œë ¤ì¤˜\", \"required\": [\"ê¹€ì¹˜ì°Œê°œ\", \"ëœì¥ì°Œê°œ\", \"ìˆœë‘ë¶€ì°Œê°œ\"]},\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“ í‰ê°€ ë°ì´í„°ì…‹: {len(my_custom_dataset)}ê°œ ì§ˆë¬¸\")\n",
    "print()\n",
    "\n",
    "custom_results = []\n",
    "for i, test in enumerate(my_custom_dataset, 1):\n",
    "    query = test[\"query\"]\n",
    "    required = test[\"required\"]\n",
    "    \n",
    "    # ê²€ìƒ‰\n",
    "    results = eval_vectorstore.similarity_search(query, k=5)\n",
    "    \n",
    "    # í‰ê°€\n",
    "    precision, rel_count = calculate_context_precision(query, results, embeddings, threshold=0.3)\n",
    "    recall, found_count = calculate_context_recall(required, results)\n",
    "    \n",
    "    print(f\"[ì§ˆë¬¸ {i}] \\\"{query}\\\"\")\n",
    "    print(f\"   ì •ë‹µ: {len(required)}ê°œ ë ˆì‹œí”¼\")\n",
    "    print(f\"   Precision: {precision:.2f}, Recall: {recall:.2f}\")\n",
    "    print()\n",
    "    \n",
    "    custom_results.append({\n",
    "        \"ì§ˆë¬¸\": query,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall\n",
    "    })\n",
    "\n",
    "# í‰ê·  ê³„ì‚°\n",
    "avg_precision = sum(r[\"Precision\"] for r in custom_results) / len(custom_results)\n",
    "avg_recall = sum(r[\"Recall\"] for r in custom_results) / len(custom_results)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"ğŸ“Š í‰ê·  ì„±ëŠ¥: Precision={avg_precision:.2f}, Recall={avg_recall:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Part 3: Gradio UIë¡œ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…\n",
    "\n",
    "### í•µì‹¬ ì•„ì´ë””ì–´\n",
    "\n",
    "**\"3ì¤„ ì½”ë“œë¡œ RAG ì±—ë´‡ UI ë§Œë“¤ê¸°!\"**\n",
    "\n",
    "### Gradioë€?\n",
    "\n",
    "- Python í•¨ìˆ˜ë¥¼ **ì›¹ UIë¡œ ë³€í™˜**í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "- Colabì—ì„œ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥ (Public URL ìƒì„±)\n",
    "- **âš ï¸ Kaggleì—ì„œëŠ” ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤**\n",
    "\n",
    "### í™˜ê²½ë³„ ì‹¤í–‰ ë°©ë²•\n",
    "\n",
    "| í™˜ê²½ | ì‹¤í–‰ ë°©ë²• | ë¹„ê³  |\n",
    "|------|-----------|------|\n",
    "| **Colab** | `share=True` â†’ Public URL ìƒì„± | âœ… ì •ìƒ ì‘ë™ |\n",
    "| **Kaggle** | âŒ Gradio ë¯¸ì§€ì› | í•¨ìˆ˜ ì§ì ‘ í˜¸ì¶œ ì‚¬ìš© |\n",
    "| **ë¡œì»¬** | `localhost:7860` | âœ… ì •ìƒ ì‘ë™ |\n",
    "\n",
    "### Kaggle ì‚¬ìš©ìë¥¼ ìœ„í•œ ëŒ€ì•ˆ\n",
    "\n",
    "Gradio ëŒ€ì‹  í•¨ìˆ˜ë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì„¸ìš”:\n",
    "\n",
    "```python\n",
    "# Gradio ëŒ€ì‹  ì´ë ‡ê²Œ ì‚¬ìš©\n",
    "result = rag_chatbot(\n",
    "    query=\"ë§¤ìš´ êµ­ë¬¼ ìš”ë¦¬ ì¶”ì²œí•´ì¤˜\", \n",
    "    strategy=\"Naive RAG\", \n",
    "    top_k=3\n",
    ")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "### ì‹¤ìŠµ: RAG ì±—ë´‡ UI ë§Œë“¤ê¸°\n",
    "\n",
    "ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´:\n",
    "- **Colab**: ì±—ë´‡ UI + Public URL\n",
    "- **Kaggle**: í•¨ìˆ˜ ì§ì ‘ í˜¸ì¶œ ì•ˆë‚´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio ë° LLM ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install -q gradio transformers accelerate\n",
    "\n",
    "# EXAONE ëª¨ë¸ ë¡œë“œ\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "print(\"ğŸ”§ EXAONE ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "model_name = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"âœ… EXAONE ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# RAG ë‹µë³€ ìƒì„± í•¨ìˆ˜\n",
    "def generate_answer(query, context):\n",
    "    \"\"\"\n",
    "    EXAONEìœ¼ë¡œ RAG ë‹µë³€ ìƒì„±\n",
    "    \n",
    "    Args:\n",
    "        query: ì‚¬ìš©ì ì§ˆë¬¸\n",
    "        context: ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"ë‹¹ì‹ ì€ í•œì‹ ë ˆì‹œí”¼ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ê²€ìƒ‰ëœ ë ˆì‹œí”¼ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\n",
    "\n",
    "[ê²€ìƒ‰ëœ ë ˆì‹œí”¼]\n",
    "{context}\n",
    "\n",
    "[ì§ˆë¬¸]\n",
    "{query}\n",
    "\n",
    "[ë‹µë³€]\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # prompt ì œê±°í•˜ê³  ë‹µë³€ë§Œ ì¶”ì¶œ\n",
    "    if \"[ë‹µë³€]\" in answer:\n",
    "        answer = answer.split(\"[ë‹µë³€]\")[-1].strip()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# RAG ì±—ë´‡ í•¨ìˆ˜\n",
    "def rag_chatbot(query, top_k, chunk_size):\n",
    "    \"\"\"\n",
    "    ì‹¤ì œ RAG ì±—ë´‡ (ê²€ìƒ‰ + LLM ë‹µë³€)\n",
    "    \n",
    "    Args:\n",
    "        query: ì‚¬ìš©ì ì§ˆë¬¸\n",
    "        top_k: ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜\n",
    "        chunk_size: Chunk í¬ê¸°\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (ë™ì )\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=int(chunk_size),\n",
    "            chunk_overlap=20,\n",
    "            length_function=len\n",
    "        )\n",
    "        \n",
    "        documents = []\n",
    "        for doc_data in recipe_docs_raw:\n",
    "            chunks = text_splitter.split_text(doc_data[\"content\"])\n",
    "            for chunk in chunks:\n",
    "                documents.append(Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\"recipe_name\": doc_data[\"recipe_name\"]}\n",
    "                ))\n",
    "        \n",
    "        temp_vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "        \n",
    "        # 2. ê²€ìƒ‰\n",
    "        results = temp_vectorstore.similarity_search(query, k=int(top_k))\n",
    "        \n",
    "        if not results:\n",
    "            return \"ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "        \n",
    "        # 3. Context ìƒì„±\n",
    "        context = \"\"\n",
    "        for doc in results:\n",
    "            recipe_name = doc.metadata.get('recipe_name', 'Unknown')\n",
    "            context += f\"[{recipe_name}]\\n{doc.page_content}\\n\\n\"\n",
    "        \n",
    "        # 4. LLM ë‹µë³€ ìƒì„±\n",
    "        answer = generate_answer(query, context)\n",
    "        \n",
    "        # 5. ì¶œë ¥ í¬ë§·\n",
    "        output = f\"**ğŸ’¬ ë‹µë³€:**\\n\\n{answer}\\n\\n\"\n",
    "        output += \"---\\n\\n\"\n",
    "        output += f\"**ğŸ“š ì°¸ê³ í•œ ë ˆì‹œí”¼ ({len(results)}ê°œ):**\\n\"\n",
    "        for i, doc in enumerate(results, 1):\n",
    "            recipe = doc.metadata.get('recipe_name', 'Unknown')\n",
    "            output += f\"{i}. {recipe}\\n\"\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"ì˜¤ë¥˜ ë°œìƒ: {str(e)}\"\n",
    "\n",
    "# Gradio UI ìƒì„±\n",
    "interface = gr.Interface(\n",
    "    fn=rag_chatbot,\n",
    "    inputs=[\n",
    "        gr.Textbox(\n",
    "            label=\"ì§ˆë¬¸\",\n",
    "            placeholder=\"ì˜ˆ: ê¹€ì¹˜ì°Œê°œ ë§Œë“œëŠ” ë²• ì•Œë ¤ì¤˜\",\n",
    "            lines=2\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            minimum=1,\n",
    "            maximum=5,\n",
    "            value=3,\n",
    "            step=1,\n",
    "            label=\"Top-K (ê²€ìƒ‰ ê°œìˆ˜)\"\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            minimum=100,\n",
    "            maximum=1000,\n",
    "            value=400,\n",
    "            step=100,\n",
    "            label=\"Chunk Size\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=gr.Textbox(\n",
    "        label=\"ë‹µë³€\",\n",
    "        lines=15\n",
    "    ),\n",
    "    title=\"ğŸ³ í•œì‹ ë ˆì‹œí”¼ RAG ì±—ë´‡\",\n",
    "    description=\"ì§ˆë¬¸í•˜ë©´ EXAONEì´ ë ˆì‹œí”¼ ì •ë³´ë¥¼ ê²€ìƒ‰í•´ì„œ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤!\",\n",
    "    examples=[\n",
    "        [\"ê¹€ì¹˜ì°Œê°œ ë§Œë“œëŠ” ë²• ì•Œë ¤ì¤˜\", 3, 400],\n",
    "        [\"ê³ ê¸° ë“¤ì–´ê°€ëŠ” ë§¤ìš´ ìš”ë¦¬ ì¶”ì²œí•´ì¤˜\", 3, 400],\n",
    "        [\"ê°„ë‹¨í•œ ì•„ì¹¨ ë©”ë‰´ ì¶”ì²œ\", 5, 400],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "# Colab: share=Trueë¡œ Public URL ìƒì„±\n",
    "# Kaggle: ì‘ë™ ì•ˆ í•¨ (í•¨ìˆ˜ ì§ì ‘ í˜¸ì¶œ ì‚¬ìš©)\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Gradio UI í™œìš© íŒ\n",
    "\n",
    "**1. Colabì—ì„œ Public URL ìƒì„±í•˜ê¸°**\n",
    "\n",
    "```python\n",
    "interface.launch(share=True)  # â† ì´ë ‡ê²Œë§Œ í•˜ë©´ ë!\n",
    "```\n",
    "\n",
    "ì‹¤í–‰í•˜ë©´:\n",
    "```\n",
    "Running on public URL: https://abc123.gradio.live\n",
    "```\n",
    "\n",
    "ì´ ë§í¬ë¥¼ ë³µì‚¬í•´ì„œ ëˆ„êµ¬ì—ê²Œë‚˜ ë³´ë‚¼ ìˆ˜ ìˆì–´ìš”! (72ì‹œê°„ ìœ íš¨)\n",
    "\n",
    "**2. âš ï¸ Kaggleì—ì„œëŠ” Gradioê°€ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤**\n",
    "\n",
    "Kaggle í™˜ê²½ì—ì„œëŠ” í•¨ìˆ˜ë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì„¸ìš”:\n",
    "\n",
    "```python\n",
    "# Gradio ëŒ€ì‹  ì´ë ‡ê²Œ ì‚¬ìš©\n",
    "result = rag_chatbot(\n",
    "    query=\"ë§¤ìš´ êµ­ë¬¼ ìš”ë¦¬ ì¶”ì²œí•´ì¤˜\", \n",
    "    strategy=\"Naive RAG\", \n",
    "    top_k=3\n",
    ")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "**3. ë” ë§ì€ ê¸°ëŠ¥ ì¶”ê°€í•˜ê¸°**\n",
    "\n",
    "- ì´ë¯¸ì§€ ì—…ë¡œë“œ: `gr.Image()`\n",
    "- íŒŒì¼ ì—…ë¡œë“œ: `gr.File()`\n",
    "- ì˜¤ë””ì˜¤: `gr.Audio()`\n",
    "- ì±„íŒ… UI: `gr.ChatInterface()`\n",
    "\n",
    "### ë¦¬ì†ŒìŠ¤\n",
    "\n",
    "- [Gradio ê³µì‹ ë¬¸ì„œ](https://gradio.app)\n",
    "- [Gradio ì˜ˆì œ ëª¨ìŒ](https://gradio.app/demos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Part 4: ìµœì¢… ì •ë¦¬ & Day 3 Preview\n",
    "\n",
    "### ì˜¤ëŠ˜ ë°°ìš´ ê²ƒ ì²´í¬ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "**Part 1: íŒŒë¼ë¯¸í„° íŠœë‹**\n",
    "- [ ] Chunk Sizeê°€ ê²€ìƒ‰ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ì´í•´\n",
    "- [ ] Top-K Precision/Recall íŠ¸ë ˆì´ë“œì˜¤í”„ ì²´í—˜\n",
    "- [ ] Threshold ì¡°ì • íš¨ê³¼ í™•ì¸\n",
    "- [ ] ì§ˆë¬¸ íƒ€ì…ë³„ ìµœì  íŒŒë¼ë¯¸í„° ì¡°í•© ë°œê²¬\n",
    "\n",
    "**Part 2: ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹**\n",
    "- [ ] ë‚˜ë§Œì˜ í‰ê°€ ë°ì´í„°ì…‹ ì‘ì„±\n",
    "- [ ] ìë™ í‰ê°€ ì‹¤í–‰\n",
    "- [ ] ê²°ê³¼ ë¶„ì„ ë° ê°œì„ ì  íŒŒì•…\n",
    "\n",
    "**Part 3: Gradio UI**\n",
    "- [ ] 3ì¤„ ì½”ë“œë¡œ UI ë§Œë“¤ê¸°\n",
    "- [ ] Colabì—ì„œ ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸\n",
    "- [ ] (ì„ íƒ) Public URL ìƒì„± ë° ê³µìœ \n",
    "\n",
    "---\n",
    "\n",
    "### Day 2 ì™„ì „í•œ ì—¬ì • (01-08ë²ˆ)\n",
    "\n",
    "```\n",
    "01ë²ˆ: Naive RAG ë² ì´ìŠ¤ë¼ì¸\n",
    "  â†’ ë³µí•© ì§ˆë¬¸ ì‹¤íŒ¨ âŒ\n",
    "\n",
    "02ë²ˆ: ì‹¤íŒ¨ ì›ì¸ ë¶„ì„\n",
    "  â†’ 5ê°€ì§€ ì¼€ì´ìŠ¤ ì§„ë‹¨ ğŸ”\n",
    "\n",
    "03ë²ˆ: Multi-Query Retrieval\n",
    "  â†’ ì§ˆë¬¸ ë¶„í•´ë¡œ í•´ê²° âœ…\n",
    "\n",
    "04ë²ˆ: Metadata Filtering\n",
    "  â†’ ë™ìŒì´ì˜ì–´ í•´ê²° âœ…\n",
    "\n",
    "05ë²ˆ: Hybrid + Reranking\n",
    "  â†’ ê²€ìƒ‰ ì •í™•ë„ ê·¹ëŒ€í™” âœ…\n",
    "\n",
    "06ë²ˆ: Corrective RAG (CRAG)\n",
    "  â†’ ê²€ìƒ‰ ê²€ì¦ + í´ë°± âœ…\n",
    "\n",
    "07ë²ˆ: RAG Evaluation (RAGAS)\n",
    "  â†’ ì •ëŸ‰ì  í‰ê°€ë¡œ ìµœì„  ì„ íƒ âœ…\n",
    "\n",
    "08ë²ˆ: Tuning & Customization â† ì—¬ê¸°!\n",
    "  â†’ ì§ì ‘ ë§Œì§€ê³ , ì‹¤í—˜í•˜ê³ , ë‚˜ë§Œì˜ ê²ƒ ë§Œë“¤ê¸° âœ…âœ…âœ…\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ Day 3 Preview: Agent + LangGraph\n",
    "\n",
    "**Day 2ì—ì„œ ë°°ìš´ ê²ƒ:**\n",
    "- \"ì–´ë–»ê²Œ\" RAGë¥¼ ë§Œë“œëŠ”ê°€? (ê¸°ë²•ë“¤)\n",
    "- \"ì–´ë–»ê²Œ\" í‰ê°€í•˜ëŠ”ê°€? (RAGAS)\n",
    "- \"ì–´ë–»ê²Œ\" íŠœë‹í•˜ëŠ”ê°€? (íŒŒë¼ë¯¸í„°)\n",
    "\n",
    "**Day 3ì—ì„œ ë°°ìš¸ ê²ƒ:**\n",
    "- \"ì–¸ì œ\" ì–´ë–¤ ë„êµ¬ë¥¼ ì“¸ê¹Œ? (Agent ê²°ì •)\n",
    "- \"ì–´ë–»ê²Œ\" ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ë§Œë“¤ê¹Œ? (LangGraph)\n",
    "- \"ì–´ë–»ê²Œ\" ì—¬ëŸ¬ Agentë¥¼ í˜‘ì—…ì‹œí‚¬ê¹Œ? (Multi-Agent)\n",
    "\n",
    "**ì˜ˆì‹œ: Day 3 Agent**\n",
    "\n",
    "```\n",
    "ì§ˆë¬¸: \"ê¹€ì¹˜ì°Œê°œ ë§Œë“œëŠ” ë²• ì•Œë ¤ì¤˜, ê·¸ë¦¬ê³  ë°±ì¢…ì› ë²„ì „ë„ ê¶ê¸ˆí•´\"\n",
    "\n",
    "Agent ì‚¬ê³  ê³¼ì •:\n",
    "1. ì§ˆë¬¸ ë¶„ì„ â†’ \"ë ˆì‹œí”¼ + ë³€í˜• ë²„ì „\" í•„ìš”\n",
    "2. ë„êµ¬ ì„ íƒ:\n",
    "   - RAG ê²€ìƒ‰ (ì •í†µ ë ˆì‹œí”¼)\n",
    "   - RAG ê²€ìƒ‰ (ë°±ì¢…ì› ë²„ì „)\n",
    "3. ê²°ê³¼ í†µí•© â†’ ìµœì¢… ë‹µë³€\n",
    "\n",
    "â†’ LangGraphë¡œ ì´ íë¦„ì„ ìë™í™”!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ ì‹¤ë¬´ í™œìš© ê°€ì´ë“œ\n",
    "\n",
    "**ìƒˆ ë„ë©”ì¸ì— RAG ì ìš©í•  ë•Œ:**\n",
    "1. **01-02ë²ˆ**: Naive RAGë¡œ ë² ì´ìŠ¤ë¼ì¸ êµ¬ì¶•\n",
    "2. **08ë²ˆ**: íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í—˜ (chunk, k, threshold)\n",
    "3. **07ë²ˆ**: ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ìœ¼ë¡œ í‰ê°€\n",
    "4. **03-06ë²ˆ**: ì„±ëŠ¥ ë‚®ìœ¼ë©´ ê³ ê¸‰ ê¸°ë²• ì ìš©\n",
    "5. **08ë²ˆ**: Gradioë¡œ í”„ë¡œí† íƒ€ì… â†’ ì´í•´ê´€ê³„ì ë°ëª¨\n",
    "\n",
    "**íŒŒë¼ë¯¸í„° íŠœë‹ ê°€ì´ë“œ:**\n",
    "\n",
    "- Chunk Size:\n",
    "  - ë‹¨ìˆœ ì§ˆë¬¸: 200-300ì\n",
    "  - ë³µí•© ì§ˆë¬¸: 500-700ì\n",
    "  - ê¸´ ë¬¸ë§¥: 1000ì+\n",
    "\n",
    "- Top-K:\n",
    "  - ë¹ ë¥¸ ì‘ë‹µ: k=1-3\n",
    "  - ì •í™•ë„ ìš°ì„ : k=5-7\n",
    "  - í¬ê´„ ê²€ìƒ‰: k=10+\n",
    "\n",
    "- Threshold:\n",
    "  - ì—„ê²©í•œ ë„ë©”ì¸: 0.5-0.7\n",
    "  - ì¼ë°˜ ë„ë©”ì¸: 0.3-0.5\n",
    "  - íƒìƒ‰ì  ê²€ìƒ‰: 0.2-0.3\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‰ Day 2 ì™„ì „ ì •ë³µ!\n",
    "\n",
    "**ì—¬ëŸ¬ë¶„ì€ ì´ì œ:**\n",
    "- âœ… Advanced RAG ëª¨ë“  ê¸°ë²• ë§ˆìŠ¤í„°\n",
    "- âœ… íŒŒë¼ë¯¸í„° íŠœë‹ ëŠ¥ë ¥ ë³´ìœ \n",
    "- âœ… í‰ê°€ ë° ìµœì í™” ê°€ëŠ¥\n",
    "- âœ… ì»¤ìŠ¤í…€ RAG ì‹œìŠ¤í…œ êµ¬ì¶• ê°€ëŠ¥\n",
    "- âœ… í”„ë¡œí† íƒ€ì… UIê¹Œì§€ ì™„ì„±!\n",
    "\n",
    "**ë‹¤ìŒ ë‹¨ê³„:**\n",
    "- Day 3ì—ì„œ LangGraph Agentë¡œ ë§Œë‚˜ìš”! ğŸš€\n",
    "\n",
    "**ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±:**\n",
    "- GitHub Issuesì— ë‚¨ê²¨ì£¼ì„¸ìš”!\n",
    "- ì‹¤ë¬´ì—ì„œ ì¨ë³´ê³  ê²½í—˜ ê³µìœ í•´ì£¼ì„¸ìš”!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
