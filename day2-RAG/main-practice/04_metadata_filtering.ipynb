{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Metadata Filtering - íŒŒìŠ¤í…” ë¬¸ì œ ì •êµí•˜ê²Œ í•´ê²°! (30ë¶„)\n",
    "\n",
    "## ğŸ¯ í•™ìŠµ ëª©í‘œ\n",
    "- **03ë²ˆì—ì„œ í•´ê²°í•œ íŒŒìŠ¤í…” ì¿¼ë¦¬ë¥¼ ë©”íƒ€ë°ì´í„° í•„í„°ë¡œ ë” ì •êµí•˜ê²Œ ê°œì„ **\n",
    "- í•„í„° ì—†ìŒ vs íƒ€ì… í•„í„° vs ë³µí•© í•„í„° ë‹¨ê³„ë³„ ë¹„êµ\n",
    "- ê²€ìƒ‰ ì •í™•ë„ ê°œì„  íš¨ê³¼ë¥¼ ìˆ˜ì¹˜ë¡œ í™•ì¸\n",
    "- Day1 íŒŒì¸íŠœë‹ ëª¨ë¸ë¡œ ì‹¤ì œ ë‹µë³€ í’ˆì§ˆ ì°¨ì´ ì²´í—˜\n",
    "\n",
    "## ğŸ“‹ ì‹¤ìŠµ êµ¬ì„±\n",
    "1. **íŒŒìŠ¤í…” ë¬¸ì œ + ë©”íƒ€ë°ì´í„° í™œìš©** (10ë¶„) - 03ë²ˆ í™˜ê²½ ì¬ì‚¬ìš©\n",
    "2. **í•„í„°ë§ ë‹¨ê³„ë³„ ë¹„êµ** (15ë¶„) - í•„í„° ì—†ìŒ â†’ íƒ€ì… í•„í„° â†’ ë³µí•© í•„í„°\n",
    "3. **Before/After ì´ì •ë¦¬** (5ë¶„) - ì •í™•ë„ ê°œì„  íš¨ê³¼ ìˆ˜ì¹˜í™”\n",
    "\n",
    "---\n",
    "\n",
    "> ğŸ’¡ **í•µì‹¬ ì•„ì´ë””ì–´**: \"íŒŒìŠ¤í…” ì˜ˆì•½ ë³€ê²½\"ê³¼ \"ì£¼ë§ ëœ ë¶ë¹„ëŠ” ë¯¸ìˆ ê´€ ì‹œê°„\" ì§ˆë¬¸ì—ì„œ ë©”íƒ€ë°ì´í„° í•„í„°ë§ì´ ì–´ë–»ê²Œ ê²€ìƒ‰ ì •í™•ë„ë¥¼ íšê¸°ì ìœ¼ë¡œ ê°œì„ í•˜ëŠ”ì§€ ì§ì ‘ ì²´í—˜í•´ë³´ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° import (03ë²ˆê³¼ ë™ì¼)\n",
    "!pip install -q langchain-community faiss-cpu sentence-transformers matplotlib pandas numpy transformers torch\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ğŸ¤– Day1 íŒŒì¸íŠœë‹ ëª¨ë¸ ê´€ë ¨ import\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import Field\n",
    "\n",
    "# LangChain ê´€ë ¨ import\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from collections import defaultdict\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "import matplotlib.font_manager as fm\n",
    "import platform\n",
    "\n",
    "if platform.system() == 'Darwin':  # macOS\n",
    "    plt.rcParams['font.family'] = ['AppleGothic']\n",
    "elif platform.system() == 'Windows':  # Windows\n",
    "    plt.rcParams['font.family'] = ['Malgun Gothic']\n",
    "else:  # Linux/Colab\n",
    "    plt.rcParams['font.family'] = ['NanumGothic', 'DejaVu Sans']\n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì • ì™„ë£Œ!\")\n",
    "\n",
    "# Day 1 íŒŒì¸íŠœë‹ ëª¨ë¸ í´ë˜ìŠ¤ (03ë²ˆê³¼ ë™ì¼)\n",
    "class Day1FinetunedLLM(LLM):\n",
    "    \"\"\"Day 1 íŒŒì¸íŠœë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” LLM í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    # Pydantic í•„ë“œ ì„ ì–¸\n",
    "    model_name: str = Field(default=\"ryanu/my-exaone-raft-model\")\n",
    "    tokenizer: Any = Field(default=None)\n",
    "    model: Any = Field(default=None)\n",
    "    \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "    \n",
    "    def __init__(self, model_name: str = \"ryanu/my-exaone-raft-model\", **kwargs):\n",
    "        super().__init__(model_name=model_name, **kwargs)\n",
    "        print(f\"ğŸ¯ Day 1 íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ: {self.model_name}\")\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"day1_finetuned_llm\"\n",
    "    \n",
    "    def _call(self, prompt: str, stop=None, run_manager=None, **kwargs) -> str:\n",
    "        \"\"\"ì‹¤ì œ ëª¨ë¸ ì¶”ë¡ \"\"\"\n",
    "        # EXAONE í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©\n",
    "        formatted_prompt = f\"[|system|]ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.[|endofturn|]\\n[|user|]{prompt}[|endofturn|]\\n[|assistant|]\"\n",
    "        \n",
    "        inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs, max_new_tokens=512, temperature=0.7, do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id, eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = self.tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
    "        return response if response else \"ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "print(\"âœ… Day1FinetunedLLM í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# íŒŒìŠ¤í…” ì½”í¼ìŠ¤ êµ¬ì„± (ê¸°ë³¸ ë¬¸ì„œ)\ndef create_pastel_corpus():\n    \"\"\"ê¸°ë³¸ íŒŒìŠ¤í…” ë¬¸ì„œë“¤\"\"\"\n    return [\n        {\n            \"content\": \"ê°•ë‚¨ íŒŒìŠ¤í…” í˜¸í…”ì€ ê³ ê¸‰ìŠ¤ëŸ¬ìš´ ë¶€í‹°í¬ í˜¸í…”ì…ë‹ˆë‹¤. ëª¨ë“  ê°ì‹¤ì€ íŒŒìŠ¤í…” í†¤ì˜ ì¸í…Œë¦¬ì–´ë¡œ ê¾¸ë©°ì ¸ ìˆìœ¼ë©°, ë„“ì€ ì°½ë¬¸ì„ í†µí•´ ê°•ë‚¨ì˜ ì•¼ê²½ì„ ê°ìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\",\n            \"metadata\": {\"type\": \"hotel\", \"city\": \"ì„œìš¸\", \"name\": \"íŒŒìŠ¤í…”í˜¸í…”\", \"source\": \"official\", \"updated_at\": \"2024-03-15\"}\n        },\n        {\n            \"content\": \"íŒŒìŠ¤í…” í˜¸í…” ì˜ˆì•½ ë³€ê²½ì„ ì›í•˜ì‹œë©´ ì²´í¬ì¸ 3ì¼ ì „ê¹Œì§€ ë¬´ë£Œë¡œ ë³€ê²½ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë‹¹ì¼ ë³€ê²½ì‹œì—ëŠ” ì¶”ê°€ ìš”ê¸ˆì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\",\n            \"metadata\": {\"type\": \"hotel\", \"city\": \"ì„œìš¸\", \"name\": \"íŒŒìŠ¤í…”í˜¸í…”\", \"source\": \"official\", \"updated_at\": \"2024-03-15\"}\n        },\n        {\n            \"content\": \"êµ­ë¦½í˜„ëŒ€ë¯¸ìˆ ê´€ ì„œìš¸ê´€ì€ ì£¼ë§ë³´ë‹¤ í‰ì¼ ì˜¤í›„ 2-4ì‹œê°€ ê°€ì¥ í•œì í•©ë‹ˆë‹¤. íŠ¹íˆ í™”ìš”ì¼ê³¼ ëª©ìš”ì¼ì— ë°©ë¬¸í•˜ì‹œë©´ ì—¬ìœ ë¡­ê²Œ ê´€ëŒí•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\",\n            \"metadata\": {\"type\": \"museum\", \"city\": \"ì„œìš¸\", \"name\": \"êµ­ë¦½í˜„ëŒ€ë¯¸ìˆ ê´€\", \"source\": \"official\", \"updated_at\": \"2024-04-01\"}\n        },\n        {\n            \"content\": \"ì„œìš¸ì‹œë¦½ë¯¸ìˆ ê´€ì€ ì£¼ë§ ì˜¤ì „ 10ì‹œ-12ì‹œê°€ ë¹„êµì  ëœ ë¶ë¹•ë‹ˆë‹¤. íŠ¹ë³„ì „ì‹œê°€ ìˆì„ ë•ŒëŠ” í‰ì¼ ëŠ¦ì€ ì˜¤í›„ë¥¼ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.\",\n            \"metadata\": {\"type\": \"museum\", \"city\": \"ì„œìš¸\", \"name\": \"ì„œìš¸ì‹œë¦½ë¯¸ìˆ ê´€\", \"source\": \"official\", \"updated_at\": \"2024-04-05\"}\n        },\n        {\n            \"content\": \"íŒŒìŠ¤í…” í†¤ ê·¸ë¦¼ì„ ê·¸ë¦¬ëŠ” ë¸”ë¡œê·¸ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ì€ ìˆ˜ì±„í™”ë¡œ íŒŒìŠ¤í…” ì»¬ëŸ¬ì˜ ê½ƒì„ ê·¸ë ¤ë³´ì•˜ì–´ìš”. ì—°í•œ í•‘í¬ì™€ ë¼ë²¤ë” ìƒ‰ìƒì´ ì •ë§ ì˜ˆì˜ê²Œ ë‚˜ì™”ìŠµë‹ˆë‹¤.\",\n            \"metadata\": {\"type\": \"blog\", \"category\": \"art\", \"city\": \"N/A\", \"source\": \"blog\", \"updated_at\": \"2024-02-01\"}\n        },\n        {\n            \"content\": \"íŒŒìŠ¤í…” ê³„ì—´ ì˜· ì½”ë”” ì¶”ì²œí•´ë“œë ¤ìš”! ë´„ì—ëŠ” ì—°í•œ ë…¸ë€ìƒ‰ê³¼ ë¯¼íŠ¸ìƒ‰ ì¡°í•©ì´ íŠ¸ë Œë””í•©ë‹ˆë‹¤. íŒŒìŠ¤í…” í•‘í¬ ì›í”¼ìŠ¤ë„ ë¡œë§¨í‹±í•œ ëŠë‚Œìœ¼ë¡œ ì¢‹ì•„ìš”.\",\n            \"metadata\": {\"type\": \"blog\", \"category\": \"fashion\", \"city\": \"N/A\", \"source\": \"blog\", \"updated_at\": \"2024-01-15\"}\n        }\n    ]\n\ndef create_heavy_noise():\n    \"\"\"ëŒ€ëŸ‰ ì¡ìŒ ë¬¸ì„œ ìƒì„±\"\"\"\n    noise = []\n    \n    # íŒ¬ì¹´í˜ ë£¨ë¨¸ë“¤\n    for city in [\"ì„œìš¸\", \"ë¶€ì‚°\", \"ì¸ì²œ\", \"ëŒ€êµ¬\", \"ëŒ€ì „\"] * 3:\n        noise.append({\n            \"content\": f\"\"\"íŒŒìŠ¤í…” í˜¸í…” ì˜ˆì•½ ë³€ê²½ ë£¨ë¨¸(ë¹„ê³µì‹, {city})\n\níŒ¬ì¹´í˜ ì†Œë¬¸: ì˜ˆì•½ ë³€ê²½ì€ 7ì¼ ì „ê¹Œì§€ë§Œ ê°€ëŠ¥? ìœ„ì•½ê¸ˆ ì •ì±…ë„ ë°”ë€Œì—ˆë‹¤ëŠ” í›„ê¸° ë§ìŒ\níŒŒìŠ¤í…” ì˜ˆì•½ ë³€ê²½ ê´€ë ¨ ë¬¸ì˜ ê¸‰ì¦ ì¤‘ (í™•ì¸ í•„ìš”)\"\"\",\n            \"metadata\": {\"type\": \"hotel\", \"name\": \"íŒŒìŠ¤í…”í˜¸í…”\", \"city\": city, \"source\": \"fan_cafe\", \"updated_at\": \"2024-03-01\"}\n        })\n    \n    # êµ¬ì •ì±…ë“¤\n    for year in [\"2017\", \"2018\", \"2019\", \"2020\"]:\n        noise.append({\n            \"content\": f\"\"\"íŒŒìŠ¤í…” í˜¸í…” ì˜ˆì•½ ë³€ê²½ ê³µì‹ ì•ˆë‚´({year})\n\nì˜ˆì•½ ë³€ê²½: íˆ¬ìˆ™ 10ì¼ ì „ê¹Œì§€ ê°€ëŠ¥\nìœ„ì•½ê¸ˆ: 5ì¼ ì „ë¶€í„° 1ë°• ìš”ê¸ˆ 50% ë¶€ê³¼\"\"\",\n            \"metadata\": {\"type\": \"hotel\", \"name\": \"íŒŒìŠ¤í…”í˜¸í…”\", \"city\": \"ì„œìš¸\", \"source\": \"official\", \"updated_at\": f\"{year}-05-01\"}\n        })\n    \n    # íƒ€ì§€ì—­ ë¯¸ìˆ ê´€ë“¤\n    for city in [\"ë¶€ì‚°\", \"ëŒ€êµ¬\", \"ê´‘ì£¼\", \"ëŒ€ì „\", \"ìš¸ì‚°\"] * 2:\n        noise.append({\n            \"content\": f\"\"\"ì—¬í–‰ ë¸”ë¡œê·¸: {city} ë¯¸ìˆ ê´€ ì£¼ë§ í•œì‚°í•œ ì‹œê°„\n\nì£¼ë§ í•œì‚°í•œ ì‹œê°„: ì˜¤ì „ 10~12ì‹œ ì¶”ì²œ\nì£¼ë§ ëœ ë¶ë¹„ëŠ” ì‹œê°„ëŒ€ ì²´í¬ í•„ìˆ˜!\"\"\",\n            \"metadata\": {\"type\": \"blog\", \"category\": \"travel\", \"city\": city, \"source\": \"blog\", \"updated_at\": \"2024-06-01\"}\n        })\n    \n    # ë™ìŒì´ì˜ì–´ ì—…ì¢…ë“¤\n    businesses = [\n        (\"íŒŒìŠ¤í…” í—¤ì–´ì‚´ë¡±\", \"salon\", \"ì˜ˆì•½ ë³€ê²½: ì „ë‚ ê¹Œì§€ë§Œ ê°€ëŠ¥\"),\n        (\"íŒŒìŠ¤í…” ì¹´í˜\", \"cafe\", \"ë‹¨ì²´ ì˜ˆì•½ ë³€ê²½: 3ì¼ ì „ ì—°ë½\"),\n        (\"íŒŒìŠ¤í…” ë„¤ì¼ìƒµ\", \"nail_shop\", \"ì˜ˆì•½ ë³€ê²½: 2ì¼ ì „ê¹Œì§€\")\n    ]\n    \n    for name, biz_type, policy in businesses:\n        for city in [\"ì„œìš¸\", \"ë¶€ì‚°\", \"ì¸ì²œ\"]:\n            noise.append({\n                \"content\": f\"\"\"{name} ì˜ˆì•½ ì•ˆë‚´ ({city}ì )\n\n{policy}\nì£¼ë§ ì„±ìˆ˜ê¸° ë³„ë„ ì •ì±… ì ìš©\"\"\",\n                \"metadata\": {\"type\": biz_type, \"name\": name, \"city\": city, \"source\": \"official\", \"updated_at\": \"2024-05-01\"}\n            })\n    \n    return noise\n\n# ì „ì²´ ë¬¸ì„œ ìƒì„±\nbase_docs = create_pastel_corpus()\nnoise_docs = create_heavy_noise()\nall_docs = base_docs + noise_docs\n\nprint(f\"ğŸ“Š ì´ ë¬¸ì„œ: ê¸°ë³¸ {len(base_docs)}ê°œ + ì¡ìŒ {len(noise_docs)}ê°œ = {len(all_docs)}ê°œ\")\n\n# ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•\nembeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nsplitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n\nlc_documents = [Document(page_content=doc[\"content\"], metadata=doc[\"metadata\"]) for doc in all_docs]\nchunks = splitter.split_documents(lc_documents)\nvectorstore = FAISS.from_documents(chunks, embeddings)\n\nprint(f\"âœ… ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ! (ì²­í¬ ìˆ˜: {len(chunks)})\")\n\n# Day1 ëª¨ë¸ ì´ˆê¸°í™”\nday1_llm = Day1FinetunedLLM()\n\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import RetrievalQA\n\ntemplate = \"\"\"ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n\nì»¨í…ìŠ¤íŠ¸: {context}\n\nì§ˆë¬¸: {question}\n\në‹µë³€:\"\"\"\nprompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n\nprint(\"âœ… ëŒ€ëŸ‰ ì¡ìŒ í™˜ê²½ êµ¬ì¶• ì™„ë£Œ!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ¯ ê°„ê²°í•œ í•„í„°ë§ ë¹„êµ\n\ndef search_with_filter(vs, query, topk=1, fetch_k=30, meta_filter=None):\n    \"\"\"ì‚¬í›„ í•„í„°ë§ ê²€ìƒ‰\"\"\"\n    docs = vs.similarity_search(query, k=fetch_k)\n    \n    if meta_filter:\n        def passes_filter(doc):\n            for key, value in meta_filter.items():\n                if key == \"updated_after\":\n                    if doc.metadata.get(\"updated_at\", \"\") < value:\n                        return False\n                else:\n                    if doc.metadata.get(key) != value:\n                        return False\n            return True\n        docs = [doc for doc in docs if passes_filter(doc)]\n    \n    return docs[:topk]\n\ndef compare_filtering():\n    \"\"\"í•„í„°ë§ ì „í›„ ë¹„êµ\"\"\"\n    \n    tests = [\n        {\n            \"query\": \"íŒŒìŠ¤í…” ì˜ˆì•½ ë³€ê²½\",\n            \"filter\": {\"type\": \"hotel\", \"city\": \"ì„œìš¸\", \"source\": \"official\", \"updated_after\": \"2023-01-01\"},\n            \"desc\": \"í˜¸í…” ì˜ˆì•½ ì •ì±…\"\n        },\n        {\n            \"query\": \"ì£¼ë§ ëœ ë¶ë¹„ëŠ” ë¯¸ìˆ ê´€ ì‹œê°„\", \n            \"filter\": {\"type\": \"museum\", \"city\": \"ì„œìš¸\", \"source\": \"official\", \"updated_after\": \"2023-01-01\"},\n            \"desc\": \"ë¯¸ìˆ ê´€ ë°©ë¬¸ ì‹œê°„\"\n        }\n    ]\n    \n    print(\"ğŸ¯ ë©”íƒ€ë°ì´í„° í•„í„°ë§ íš¨ê³¼ ë¹„êµ\")\n    print(\"=\"*50)\n    \n    for test in tests:\n        query = test[\"query\"]\n        filter_cond = test[\"filter\"]\n        desc = test[\"desc\"]\n        \n        print(f\"\\nğŸ“‹ {desc}: '{query}'\")\n        print(\"-\" * 40)\n        \n        # í•„í„° ì—†ìŒ\n        no_filter = search_with_filter(vectorstore, query, topk=1, meta_filter=None)\n        print(\"âŒ í•„í„° ì—†ìŒ:\")\n        for doc in no_filter:\n            md = doc.metadata\n            print(f\"  [{md.get('source')}|{md.get('city')}|{md.get('updated_at')}]\")\n            print(f\"  {doc.page_content.strip()}\\n\")\n        \n        # í•„í„° ì ìš©\n        filtered = search_with_filter(vectorstore, query, topk=1, meta_filter=filter_cond)\n        print(\"âœ… í•„í„° ì ìš©:\")\n        if filtered:\n            for doc in filtered:\n                md = doc.metadata\n                print(f\"  [{md.get('source')}|{md.get('city')}|{md.get('updated_at')}]\")\n                print(f\"  {doc.page_content.strip()}\\n\")\n        else:\n            print(\"  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\\n\")\n\ncompare_filtering()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ¯ ì‹¤ì œ RAG ë‹µë³€ ë¹„êµ\n\ndef rag_answer_comparison():\n    \"\"\"í•„í„°ë§ ì „í›„ ì‹¤ì œ RAG ë‹µë³€ ë¹„êµ\"\"\"\n    \n    queries = [\"íŒŒìŠ¤í…” ì˜ˆì•½ ë³€ê²½\", \"ì£¼ë§ ëœ ë¶ë¹„ëŠ” ë¯¸ìˆ ê´€ ì‹œê°„\"]\n    \n    print(\"ğŸ¤– ì‹¤ì œ RAG ë‹µë³€ - í•„í„°ë§ ì „í›„ ë¹„êµ\")\n    print(\"=\"*50)\n    \n    for query in queries:\n        print(f\"\\nğŸ“‹ ì§ˆë¬¸: '{query}'\")\n        print(\"-\" * 30)\n        \n        # í•„í„° ì¡°ê±´ ì„¤ì •\n        if \"ì˜ˆì•½\" in query:\n            filter_cond = {\"type\": \"hotel\", \"city\": \"ì„œìš¸\", \"source\": \"official\", \"updated_after\": \"2023-01-01\"}\n        else:\n            filter_cond = {\"type\": \"museum\", \"city\": \"ì„œìš¸\", \"source\": \"official\", \"updated_after\": \"2023-01-01\"}\n        \n        # ë¬´í•„í„° RAG\n        qa_no_filter = RetrievalQA.from_chain_type(\n            llm=day1_llm,\n            chain_type=\"stuff\", \n            retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2}),\n            chain_type_kwargs={\"prompt\": prompt}\n        )\n        \n        print(\"âŒ í•„í„° ì—†ìŒ ë‹µë³€:\")\n        try:\n            result = qa_no_filter({\"query\": query})\n            answer = result.get('result', 'ë‹µë³€ ìƒì„± ì‹¤íŒ¨')\n            print(f\"  {answer.strip()}\\n\")\n        except Exception as e:\n            print(f\"  [ì˜¤ë¥˜: {str(e)}]\\n\")\n        \n        # í•„í„° ì ìš© RAG  \n        filtered_docs = [doc for doc in lc_documents if all(\n            doc.metadata.get(k) == v if k != \"updated_after\" else doc.metadata.get(\"updated_at\", \"\") >= v\n            for k, v in filter_cond.items()\n        )]\n        \n        if filtered_docs:\n            filtered_vectorstore = FAISS.from_documents(filtered_docs, embeddings)\n            qa_filtered = RetrievalQA.from_chain_type(\n                llm=day1_llm,\n                chain_type=\"stuff\",\n                retriever=filtered_vectorstore.as_retriever(search_kwargs={\"k\": 2}),\n                chain_type_kwargs={\"prompt\": prompt}\n            )\n            \n            print(\"âœ… í•„í„° ì ìš© ë‹µë³€:\")\n            try:\n                result = qa_filtered({\"query\": query})\n                answer = result.get('result', 'ë‹µë³€ ìƒì„± ì‹¤íŒ¨')\n                print(f\"  {answer.strip()}\\n\")\n            except Exception as e:\n                print(f\"  [ì˜¤ë¥˜: {str(e)}]\\n\")\n\nrag_answer_comparison()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## ğŸ¯ ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì™„ë£Œ!\n\n### ğŸ“Š í•µì‹¬ í•™ìŠµ ì„±ê³¼\n- **Before**: ğŸš¨ íŒ¬ì¹´í˜ ë£¨ë¨¸, êµ¬ì •ì±…, íƒ€ì§€ì—­ ì •ë³´ í˜¼ì¬\n- **After**: âœ… 4ì°¨ì› í•„í„°ë§ìœ¼ë¡œ ì •í™•í•œ ê³µì‹ ì •ë³´ë§Œ ì¶”ì¶œ\n\n### ğŸ’¡ í•„í„°ë§ ì°¨ì›\n1. **type**: hotel/museum - ì—…ì¢…ë³„ ì •í™•í•œ ë¶„ë¥˜\n2. **city**: ì„œìš¸ - ì§€ì—­ ê¸°ë°˜ ê´€ë ¨ì„± í–¥ìƒ  \n3. **source**: official - ê³µì‹ ì •ë³´ë§Œ ì‹ ë¢°ì„± í™•ë³´\n4. **updated_after**: 2023+ - ìµœì‹  ì •ë³´ë¡œ ì •í™•ë„ ë³´ì¥\n\n### ğŸš€ ë‹¤ìŒ ë‹¨ê³„\n**05. Hybrid Search**: BM25 + Vector Search ê²°í•©ìœ¼ë¡œ ë”ìš± ì •êµí•œ ê²€ìƒ‰!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ì‹¤ìŠµ ì •ë¦¬\n",
    "\n",
    "### âœ… ì™„ë£Œëœ ê¸°ëŠ¥ë“¤\n",
    "1. **Time-based Filtering**: ì—°ë„, ìµœì‹  ê¸°ê°„ë³„ ë¬¸ì„œ í•„í„°ë§\n",
    "2. **Category-based Filtering**: ì¹´í…Œê³ ë¦¬, ë‚œì´ë„ë³„ ì •í™•í•œ í•„í„°ë§  \n",
    "3. **Dynamic Filter Selection**: ì¿¼ë¦¬ ì˜ë„ ë¶„ì„ ê¸°ë°˜ ìë™ í•„í„° ì„ íƒ\n",
    "4. **Performance Comparison**: í•„í„°ë§ ë°©ë²•ë³„ ì„±ëŠ¥ ë¶„ì„\n",
    "\n",
    "### ğŸš€ ì„±ëŠ¥ ê°œì„  íš¨ê³¼\n",
    "- **ì •í™•ë„ í–¥ìƒ**: ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë§Œ ì„ ë³„í•˜ì—¬ ê²€ìƒ‰ í’ˆì§ˆ ê°œì„ \n",
    "- **ì‘ë‹µ ì†ë„**: í•„í„°ë§ìœ¼ë¡œ ê²€ìƒ‰ ëŒ€ìƒ ì¶•ì†Œí•˜ì—¬ ì†ë„ í–¥ìƒ\n",
    "- **ì‚¬ìš©ì ê²½í—˜**: ì˜ë„ì— ë§ëŠ” ê²°ê³¼ ì œê³µìœ¼ë¡œ ë§Œì¡±ë„ ì¦ê°€\n",
    "\n",
    "### ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: 05. Hybrid Search & Re-ranking\n",
    "- BM25 + Vector Search ê²°í•©\n",
    "- Reciprocal Rank Fusion\n",
    "- Cross-encoder Re-ranking\n",
    "- Multi-stage Retrieval Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "*ğŸ’¡ **ì‹¤ë¬´ íŒ**: ë©”íƒ€ë°ì´í„° ì„¤ê³„ëŠ” RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ì…ë‹ˆë‹¤. ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ì„ ë°˜ì˜í•œ í’ë¶€í•˜ê³  êµ¬ì¡°í™”ëœ ë©”íƒ€ë°ì´í„°ë¥¼ ì„¤ê³„í•˜ì„¸ìš”.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}