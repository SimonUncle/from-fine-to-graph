{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Day 2 ì‹¤ìŠµ 1: Naive RAG ë² ì´ìŠ¤ë¼ì¸ êµ¬ì¶•\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- **ê¸°ë³¸ RAG íŒŒì´í”„ë¼ì¸** êµ¬í˜„ ë° ì´í•´\n",
    "- **ë²¡í„° ê²€ìƒ‰**ì˜ ê¸°ë³¸ ì›ë¦¬ ì²´í—˜\n",
    "- **ì„±ëŠ¥ ì¸¡ì • ì‹œìŠ¤í…œ** êµ¬ì¶•ìœ¼ë¡œ ê°œì„ ì  íŒŒì•…\n",
    "- **ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥** ê¸°ë¡ìœ¼ë¡œ í–¥í›„ ë¹„êµ ê¸°ì¤€ ë§ˆë ¨\n",
    "\n",
    "### ğŸ’¡ Naive RAGì˜ í•µì‹¬\n",
    "**\"ê°€ì¥ ë‹¨ìˆœí•˜ì§€ë§Œ ë™ì‘í•˜ëŠ”\" RAG ì‹œìŠ¤í…œ**ì„ ë§Œë“œëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.\n",
    "\n",
    "ë³µì¡í•œ ê¸°ë²• ì—†ì´ë„ ë¬¸ì„œ ê²€ìƒ‰-ë‹µë³€ ìƒì„±ì´ ì–´ë–»ê²Œ ì´ë£¨ì–´ì§€ëŠ”ì§€ ì§ì ‘ ì²´í—˜í•´ë³´ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 2 ì‹¤ìŠµ 1: Naive RAGë¥¼ ìœ„í•œ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "print(\"ğŸš€ Day 2 ì‹¤ìŠµ 1: Naive RAG ë² ì´ìŠ¤ë¼ì¸ êµ¬ì¶•\")\n",
    "print(\"ğŸ“¦ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì¤‘...\")\n",
    "\n",
    "# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë° ê²€ìƒ‰ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "!pip install -q faiss-cpu  # FAISS: Facebook AI Similarity Search (ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰)\n",
    "!pip install -q langchain langchain-community langchain-openai  # LangChain RAG í”„ë ˆì„ì›Œí¬\n",
    "!pip install -q sentence-transformers  # ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸\n",
    "\n",
    "# ë°ì´í„° ì²˜ë¦¬ ë° ì‹œê°í™”\n",
    "!pip install -q pandas numpy matplotlib seaborn plotly\n",
    "!pip install -q tqdm  # ì§„í–‰ ìƒí™© í‘œì‹œ\n",
    "\n",
    "# í•œêµ­ì–´ ìì—°ì–´ì²˜ë¦¬ (í† í°í™” ë“±)\n",
    "!pip install -q konlpy\n",
    "\n",
    "# ë¬¸ì„œ ë¡œë”© ë° ì²˜ë¦¬\n",
    "!pip install -q beautifulsoup4 requests\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!\")\n",
    "print(\"ğŸ’¡ ì´ì œ Naive RAGì˜ í•µì‹¬ êµ¬ì„±ìš”ì†Œë¥¼ ë‹¨ê³„ë³„ë¡œ êµ¬ì¶•í•´ë³´ê² ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from datetime import datetime\n",
    "import time\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LangChain í•µì‹¬ ì»´í¬ë„ŒíŠ¸\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Sentence Transformers (ì„ë² ë”© ëª¨ë¸)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (matplotlib) - RAG ì„±ëŠ¥ ë¶„ì„ ì°¨íŠ¸ì—ì„œ í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šë„ë¡ ì„¤ì •\n",
    "# ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì°¨íŠ¸, ì‘ë‹µ ì‹œê°„ ë¶„ì„ ë“±ì—ì„œ í•œê¸€ í‘œì‹œë¥¼ ìœ„í•´ í•„ìš”\n",
    "print(\"ğŸ”§ í•œê¸€ í°íŠ¸ ì„¤ì • ì¤‘...\")\n",
    "!apt-get update -qq\n",
    "!apt-get install fonts-nanum -qq > /dev/null\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# ë‚˜ëˆ”ë°”ë¥¸ê³ ë”• í°íŠ¸ ê²½ë¡œ ì„¤ì •\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "# í°íŠ¸ ë§¤ë‹ˆì €ì— í°íŠ¸ ì¶”ê°€ - RAG ì„±ëŠ¥ ë¶„ì„ ê·¸ë˜í”„ì—ì„œ í•œê¸€ í‘œì‹œë¥¼ ìœ„í•´ í•„ìš”\n",
    "fm.fontManager.addfont(fontpath)\n",
    "\n",
    "# matplotlib ì„¤ì • ì—…ë°ì´íŠ¸ - ëª¨ë“  ì„±ëŠ¥ ë¶„ì„ ì°¨íŠ¸ì—ì„œ í•œê¸€ì´ ì •ìƒì ìœ¼ë¡œ í‘œì‹œë¨\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'NanumBarunGothic',  # ê¸°ë³¸ í°íŠ¸ë¥¼ ë‚˜ëˆ”ë°”ë¥¸ê³ ë”•ìœ¼ë¡œ ì„¤ì •\n",
    "    'axes.unicode_minus': False         # ìŒìˆ˜ ê¸°í˜¸ í‘œì‹œ ë¬¸ì œ í•´ê²°\n",
    "})\n",
    "\n",
    "# ì‹œê°í™” ìŠ¤íƒ€ì¼ ì„¤ì • - ê¹”ë”í•œ RAG ì„±ëŠ¥ ë¶„ì„ ì°¨íŠ¸ë¥¼ ìœ„í•œ ì„¤ì •\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")  # êµ¬ë³„í•˜ê¸° ì‰¬ìš´ ìƒ‰ìƒ íŒ”ë ˆíŠ¸\n",
    "\n",
    "print(\"âœ… í•œê¸€ í°íŠ¸ ì„¤ì • ì™„ë£Œ - RAG ì„±ëŠ¥ ì°¨íŠ¸ì—ì„œ í•œê¸€ì´ ì •ìƒ í‘œì‹œë©ë‹ˆë‹¤\")\n",
    "print(\"ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ìƒ˜í”Œ ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "\n",
    "### ğŸ“š ì‹¤ìŠµìš© í•œêµ­ì–´ ë¬¸ì„œ ì»¬ë ‰ì…˜\n",
    "ë‹¤ì–‘í•œ ì£¼ì œì˜ í•œêµ­ì–´ ë¬¸ì„œë¥¼ ì¤€ë¹„í•˜ì—¬ Naive RAGì˜ ê²€ìƒ‰ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n",
    "- AI/ê¸°ìˆ  ê´€ë ¨ ë¬¸ì„œ\n",
    "- ì •ì±…/ì œë„ ê´€ë ¨ ë¬¸ì„œ  \n",
    "- ì¼ë°˜ ìƒì‹ ë¬¸ì„œ\n",
    "- ì‹œê°„ì  ì •ë³´ê°€ í¬í•¨ëœ ë¬¸ì„œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_documents() -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    RAG ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ í•œêµ­ì–´ ìƒ˜í”Œ ë¬¸ì„œ ìƒì„±\n",
    "    \n",
    "    ë‹¤ì–‘í•œ ë„ë©”ì¸ê³¼ ë‚œì´ë„ì˜ ë¬¸ì„œë¥¼ í¬í•¨í•˜ì—¬ Naive RAGì˜\n",
    "    ê°•ì ê³¼ ì•½ì ì„ ëª…í™•íˆ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±\n",
    "    \n",
    "    Returns:\n",
    "        ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ì œëª©, ë‚´ìš©, ë©”íƒ€ë°ì´í„° í¬í•¨)\n",
    "    \"\"\"\n",
    "    \n",
    "    documents = [\n",
    "        {\n",
    "            \"title\": \"ChatGPTì™€ ìƒì„±í˜• AIì˜ ë“±ì¥\",\n",
    "            \"content\": \"\"\"\n",
    "            2022ë…„ 11ì›” OpenAIì—ì„œ ê³µê°œí•œ ChatGPTëŠ” ëŒ€í™”í˜• ì¸ê³µì§€ëŠ¥ ì„œë¹„ìŠ¤ë¡œ í° í™”ì œë¥¼ ëª¨ì•˜ìŠµë‹ˆë‹¤. \n",
    "            GPT-3.5 ê¸°ë°˜ìœ¼ë¡œ ê°œë°œëœ ChatGPTëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ê°€ ê°€ëŠ¥í•˜ë©°, ë‹¤ì–‘í•œ ì§ˆë¬¸ì— ëŒ€í•´ \n",
    "            ìƒì„¸í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤. ì´í›„ 2023ë…„ 3ì›”ì—ëŠ” ë”ìš± ë°œì „ëœ GPT-4ê°€ ì¶œì‹œë˜ì–´\n",
    "            ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ê¹Œì§€ ì§€ì›í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒì„±í˜• AI ê¸°ìˆ ì€ êµìœ¡, ì˜ë£Œ, ë²•ë¥  ë“±\n",
    "            ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë˜ê³  ìˆìœ¼ë©°, íŠ¹íˆ ì½˜í…ì¸  ìƒì„±ê³¼ ì½”ë”© ì§€ì› ë¶„ì•¼ì—ì„œ í˜ì‹ ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.\n",
    "            \"\"\",\n",
    "            \"metadata\": {\n",
    "                \"category\": \"technology\",\n",
    "                \"year\": 2023,\n",
    "                \"source\": \"tech_news\",\n",
    "                \"keywords\": [\"ChatGPT\", \"OpenAI\", \"GPT-4\", \"ìƒì„±í˜•AI\", \"ì¸ê³µì§€ëŠ¥\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"í•œêµ­ì˜ AI êµ­ê°€ì „ëµê³¼ ì •ì±… ë°©í–¥\",\n",
    "            \"content\": \"\"\"\n",
    "            ëŒ€í•œë¯¼êµ­ ì •ë¶€ëŠ” 2024ë…„ 'AI ê°•êµ­ ì½”ë¦¬ì•„' ë¹„ì „ì„ ë°œí‘œí•˜ë©° ì¸ê³µì§€ëŠ¥ ë¶„ì•¼ì˜ ê¸€ë¡œë²Œ ë¦¬ë”ì‹­ \n",
    "            í™•ë³´ë¥¼ ìœ„í•œ ì¢…í•©ê³„íšì„ ìˆ˜ë¦½í–ˆìŠµë‹ˆë‹¤. ì£¼ìš” ì •ì±…ìœ¼ë¡œëŠ” AI ë°˜ë„ì²´ ìƒíƒœê³„ êµ¬ì¶•, \n",
    "            AI ë°ì´í„° ëŒ êµ¬ì¶•, K-í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ í™•ì‚° ë“±ì´ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ì •ë¶€ëŠ” 2027ë…„ê¹Œì§€\n",
    "            AI ë¶„ì•¼ì— ì´ 9ì¡° 9ì²œì–µì›ì„ íˆ¬ìí•˜ê¸°ë¡œ ê²°ì •í–ˆìŠµë‹ˆë‹¤. ë˜í•œ AI ìœ¤ë¦¬ ê°€ì´ë“œë¼ì¸ì„ ë§ˆë ¨í•˜ì—¬\n",
    "            ì•ˆì „í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” AI í™œìš© í™˜ê²½ì„ ì¡°ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤. êµìœ¡ ë¶„ì•¼ì—ì„œëŠ” AI ë””ì§€í„¸ êµê³¼ì„œ\n",
    "            ë„ì…ì„ í†µí•´ ê°œì¸ ë§ì¶¤í˜• í•™ìŠµì„ ì§€ì›í•  ê³„íšì…ë‹ˆë‹¤.\n",
    "            \"\"\",\n",
    "            \"metadata\": {\n",
    "                \"category\": \"policy\",\n",
    "                \"year\": 2024,\n",
    "                \"source\": \"government\",\n",
    "                \"keywords\": [\"AIì •ì±…\", \"êµ­ê°€ì „ëµ\", \"íˆ¬ìê³„íš\", \"ìœ¤ë¦¬ê°€ì´ë“œë¼ì¸\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì \",\n",
    "            \"content\": \"\"\"\n",
    "            ë¨¸ì‹ ëŸ¬ë‹(Machine Learning)ì€ ì»´í“¨í„°ê°€ ë°ì´í„°ë¥¼ í†µí•´ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.\n",
    "            ì „í†µì ì¸ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œëŠ” ì„ í˜•íšŒê·€, ì˜ì‚¬ê²°ì •íŠ¸ë¦¬, SVM, ëœë¤í¬ë ˆìŠ¤íŠ¸ ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "            ë”¥ëŸ¬ë‹(Deep Learning)ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•˜ìœ„ ë¶„ì•¼ë¡œ, ì¸ê³µì‹ ê²½ë§ì„ ì—¬ëŸ¬ ì¸µìœ¼ë¡œ ìŒ“ì€ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "            ë”¥ëŸ¬ë‹ì€ ì´ë¯¸ì§€ ì¸ì‹, ìŒì„± ì¸ì‹, ìì—°ì–´ ì²˜ë¦¬ ë“± ë³µì¡í•œ íŒ¨í„´ ì¸ì‹ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\n",
    "            ë¨¸ì‹ ëŸ¬ë‹ì€ ìƒëŒ€ì ìœ¼ë¡œ ì ì€ ë°ì´í„°ë¡œë„ í•™ìŠµì´ ê°€ëŠ¥í•˜ì§€ë§Œ, ë”¥ëŸ¬ë‹ì€ ëŒ€ëŸ‰ì˜ ë°ì´í„°ì™€ \n",
    "            ë†’ì€ ì»´í“¨íŒ… íŒŒì›Œê°€ í•„ìš”í•©ë‹ˆë‹¤. ìµœê·¼ì—ëŠ” transformer ì•„í‚¤í…ì²˜ ê¸°ë°˜ì˜ ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸ì´ \n",
    "            ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ í˜ì‹ ì„ ê°€ì ¸ì˜¤ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "            \"\"\",\n",
    "            \"metadata\": {\n",
    "                \"category\": \"education\",\n",
    "                \"year\": 2023,\n",
    "                \"source\": \"textbook\",\n",
    "                \"keywords\": [\"ë¨¸ì‹ ëŸ¬ë‹\", \"ë”¥ëŸ¬ë‹\", \"ì‹ ê²½ë§\", \"ì•Œê³ ë¦¬ì¦˜\", \"ML\", \"DL\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"RAG(Retrieval-Augmented Generation) ê¸°ìˆ  ì†Œê°œ\",\n",
    "            \"content\": \"\"\"\n",
    "            RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±(Retrieval-Augmented Generation)ì˜ ì¤„ì„ë§ë¡œ, \n",
    "            ì™¸ë¶€ ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•œ í›„ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
    "            ì „í†µì ì¸ ì–¸ì–´ëª¨ë¸ì€ í•™ìŠµ ë°ì´í„°ì—ë§Œ ì˜ì¡´í•˜ì§€ë§Œ, RAGëŠ” ì‹¤ì‹œê°„ìœ¼ë¡œ ìµœì‹  ì •ë³´ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "            RAG ì‹œìŠ¤í…œì€ í¬ê²Œ ê²€ìƒ‰ê¸°(Retriever)ì™€ ìƒì„±ê¸°(Generator)ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.\n",
    "            ê²€ìƒ‰ê¸°ëŠ” ë²¡í„° ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ê³ , ìƒì„±ê¸°ëŠ” ê²€ìƒ‰ëœ ë§¥ë½ì„ í™œìš©í•´ ë‹µë³€ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "            ì´ ë°©ì‹ì€ í• ë£¨ì‹œë„¤ì´ì…˜ì„ ì¤„ì´ê³ , ì¶œì²˜ë¥¼ ëª…í™•íˆ í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "            ìµœê·¼ì—ëŠ” Naive RAGì—ì„œ Advanced RAG, Modular RAGë¡œ ë°œì „í•˜ë©° \n",
    "            ë”ìš± ì •êµí•œ ê²€ìƒ‰ê³¼ ìƒì„±ì´ ê°€ëŠ¥í•´ì¡ŒìŠµë‹ˆë‹¤.\n",
    "            \"\"\",\n",
    "            \"metadata\": {\n",
    "                \"category\": \"technology\",\n",
    "                \"year\": 2024,\n",
    "                \"source\": \"research\",\n",
    "                \"keywords\": [\"RAG\", \"ê²€ìƒ‰ì¦ê°•ìƒì„±\", \"retrieval\", \"ë²¡í„°ê²€ìƒ‰\", \"í• ë£¨ì‹œë„¤ì´ì…˜\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë° ê¸°ì´ˆ\",\n",
    "            \"content\": \"\"\"\n",
    "            íŒŒì´ì¬(Python)ì€ 1991ë…„ ê·€ë„ ë°˜ ë¡œì„¬ì´ ê°œë°œí•œ ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\n",
    "            íŒŒì´ì¬ì˜ ì² í•™ì€ 'ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ ì½”ë“œ'ë¡œ, 'The Zen of Python'ì— ì˜ ë‚˜íƒ€ë‚˜ ìˆìŠµë‹ˆë‹¤.\n",
    "            íŒŒì´ì¬ì˜ ì£¼ìš” íŠ¹ì§•ìœ¼ë¡œëŠ” ì¸í„°í”„ë¦¬í„° ì–¸ì–´, ë™ì  íƒ€ì´í•‘, ê°ì²´ì§€í–¥ í”„ë¡œê·¸ë˜ë° ì§€ì› ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "            ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ë¶„ì•¼ì—ì„œëŠ” NumPy, Pandas, Matplotlib ë“±ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•µì‹¬ì ìœ¼ë¡œ ì‚¬ìš©ë˜ë©°,\n",
    "            ì›¹ ê°œë°œì—ì„œëŠ” Django, Flask ê°™ì€ í”„ë ˆì„ì›Œí¬ê°€ ì¸ê¸°ì…ë‹ˆë‹¤.\n",
    "            ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì•¼ì—ì„œëŠ” scikit-learn, TensorFlow, PyTorch ë“±ì´ ë„ë¦¬ í™œìš©ë©ë‹ˆë‹¤.\n",
    "            íŒŒì´ì¬ì€ ë¬¸ë²•ì´ ì§ê´€ì ì´ì–´ì„œ ì´ˆë³´ìê°€ ë°°ìš°ê¸° ì‰½ê³ , ë™ì‹œì— ê³ ê¸‰ ê¸°ëŠ¥ë„ í’ë¶€í•´ì„œ\n",
    "            ì „ë¬¸ê°€ë“¤ë„ ì„ í˜¸í•˜ëŠ” ì–¸ì–´ì…ë‹ˆë‹¤.\n",
    "            \"\"\",\n",
    "            \"metadata\": {\n",
    "                \"category\": \"programming\",\n",
    "                \"year\": 2023,\n",
    "                \"source\": \"tutorial\",\n",
    "                \"keywords\": [\"Python\", \"íŒŒì´ì¬\", \"í”„ë¡œê·¸ë˜ë°\", \"ë¼ì´ë¸ŒëŸ¬ë¦¬\", \"ê°œë°œ\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì™€ ì„ë² ë”©\",\n",
    "            \"content\": \"\"\"\n",
    "            ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ê³ ì°¨ì› ë²¡í„° ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤.\n",
    "            ê¸°ì¡´ ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤ì™€ ë‹¬ë¦¬ ë²¡í„° ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "            ëŒ€í‘œì ì¸ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¡œëŠ” Pinecone, Weaviate, Chroma, FAISS ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "            ì„ë² ë”©(Embedding)ì€ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë“±ì˜ ë°ì´í„°ë¥¼ ê³ ì •ëœ í¬ê¸°ì˜ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\n",
    "            í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸ë¡œëŠ” BERT, RoBERTa, Sentence-BERT, OpenAIì˜ text-embedding-ada-002 ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "            ë²¡í„° ê²€ìƒ‰ì€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„, ìœ í´ë¦¬ë“œ ê±°ë¦¬, ë‚´ì  ë“±ì˜ ê±°ë¦¬ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ìœ ì‚¬í•œ ë²¡í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
    "            ì´ ê¸°ìˆ ì€ ì¶”ì²œ ì‹œìŠ¤í…œ, ê²€ìƒ‰ ì—”ì§„, ê·¸ë¦¬ê³  RAG ì‹œìŠ¤í…œì—ì„œ í•µì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "            \"\"\",\n",
    "            \"metadata\": {\n",
    "                \"category\": \"technology\",\n",
    "                \"year\": 2024,\n",
    "                \"source\": \"technical_guide\",\n",
    "                \"keywords\": [\"ë²¡í„°ë°ì´í„°ë² ì´ìŠ¤\", \"ì„ë² ë”©\", \"FAISS\", \"ìœ ì‚¬ë„ê²€ìƒ‰\", \"embedding\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"2024ë…„ AI íŠ¸ë Œë“œì™€ ì „ë§\",\n",
    "            \"content\": \"\"\"\n",
    "            2024ë…„ ì¸ê³µì§€ëŠ¥ ë¶„ì•¼ì˜ ì£¼ìš” íŠ¸ë Œë“œëŠ” ë©€í‹°ëª¨ë‹¬ AI, AGI ì—°êµ¬, AI ì—ì´ì „íŠ¸ ë“±ì…ë‹ˆë‹¤.\n",
    "            ë©€í‹°ëª¨ë‹¬ AIëŠ” í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ìŒì„± ë“± ë‹¤ì–‘í•œ í˜•íƒœì˜ ë°ì´í„°ë¥¼ ë™ì‹œì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ê¸°ìˆ ë¡œ,\n",
    "            GPT-4V, Claude 3, Gemini ë“±ì˜ ëª¨ë¸ì—ì„œ êµ¬í˜„ë˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "            AI ì—ì´ì „íŠ¸ëŠ” ììœ¨ì ìœ¼ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³  ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì‹œìŠ¤í…œìœ¼ë¡œ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "            ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì˜ ë°œì „ë„ ëˆˆì— ë„ëŠ”ë°, Llama 2, Mistral, Qwen ë“±ì´ ìƒìš© ëª¨ë¸ì— ê·¼ì ‘í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\n",
    "            AI ì•ˆì „ì„±ê³¼ ì •ë ¬(Alignment) ì—°êµ¬ë„ ì¤‘ìš”í•œ í™”ë‘ë¡œ, Constitutional AI, RLHF ë“±ì˜ ê¸°ë²•ì´ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "            ê¸°ì—…ë“¤ì€ AIë¥¼ í™œìš©í•œ ì—…ë¬´ ìë™í™”ì™€ ìƒì‚°ì„± í–¥ìƒì— ì§‘ì¤‘í•˜ê³  ìˆìœ¼ë©°,\n",
    "            íŠ¹íˆ ì½”ë“œ ìƒì„±, ë¬¸ì„œ ì‘ì„±, ê³ ê° ì„œë¹„ìŠ¤ ë“± ë¶„ì•¼ì—ì„œ ì‹¤ìš©í™”ê°€ ê°€ì†í™”ë˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "            \"\"\",\n",
    "            \"metadata\": {\n",
    "                \"category\": \"trend\",\n",
    "                \"year\": 2024,\n",
    "                \"source\": \"industry_report\",\n",
    "                \"keywords\": [\"AIíŠ¸ë Œë“œ\", \"ë©€í‹°ëª¨ë‹¬\", \"AGI\", \"ì—ì´ì „íŠ¸\", \"ì˜¤í”ˆì†ŒìŠ¤\"]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# ìƒ˜í”Œ ë¬¸ì„œ ìƒì„±\n",
    "sample_docs = create_sample_documents()\n",
    "\n",
    "print(f\"ğŸ“š ìƒ˜í”Œ ë¬¸ì„œ ì¤€ë¹„ ì™„ë£Œ: {len(sample_docs)}ê°œ ë¬¸ì„œ\")\n",
    "print(\"\\nğŸ“‹ ë¬¸ì„œ ëª©ë¡:\")\n",
    "for i, doc in enumerate(sample_docs, 1):\n",
    "    print(f\"  {i}. {doc['title']} ({doc['metadata']['category']}, {doc['metadata']['year']})\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ì™€ ì—°ë„ì˜ ë¬¸ì„œë¡œ Naive RAGì˜ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë¬¸ì„œ ì „ì²˜ë¦¬ ë° ì²­í‚¹\n",
    "\n",
    "### âœ‚ï¸ í…ìŠ¤íŠ¸ ë¶„í•  (Text Chunking)\n",
    "ê¸´ ë¬¸ì„œë¥¼ ê²€ìƒ‰ì— ì í•©í•œ í¬ê¸°ë¡œ ë¶„í• í•©ë‹ˆë‹¤.\n",
    "- **Chunk í¬ê¸°**: ë„ˆë¬´ ì‘ìœ¼ë©´ ë¬¸ë§¥ ì†ì‹¤, ë„ˆë¬´ í¬ë©´ ë…¸ì´ì¦ˆ ì¦ê°€\n",
    "- **Overlap**: ì²­í¬ ê°„ ì¤‘ë³µìœ¼ë¡œ ë¬¸ë§¥ ì—°ê²°ì„± ìœ ì§€\n",
    "- **ë¶„í•  ê¸°ì¤€**: ë¬¸ì¥, ë‹¨ë½ ë“± ì˜ë¯¸ ë‹¨ìœ„ ê³ ë ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_documents(documents: List[Dict[str, Any]], \n",
    "                        chunk_size: int = 500, \n",
    "                        chunk_overlap: int = 50) -> List[Document]:\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œë“¤ì„ RAGì— ì í•©í•˜ê²Œ ì „ì²˜ë¦¬í•˜ê³  ì²­í‚¹í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        documents: ì›ë³¸ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "        chunk_size: ê° ì²­í¬ì˜ ìµœëŒ€ ê¸€ì ìˆ˜\n",
    "        chunk_overlap: ì²­í¬ ê°„ ì¤‘ë³µ ê¸€ì ìˆ˜\n",
    "        \n",
    "    Returns:\n",
    "        LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ğŸ“ ë¬¸ì„œ ì „ì²˜ë¦¬ ì‹œì‘...\")\n",
    "    print(f\"  ì²­í¬ í¬ê¸°: {chunk_size} ê¸€ì\")\n",
    "    print(f\"  ì²­í¬ ì¤‘ë³µ: {chunk_overlap} ê¸€ì\")\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •\n",
    "    # RecursiveCharacterTextSplitter: ë¬¸ë§¥ì„ ê³ ë ¤í•œ ìŠ¤ë§ˆíŠ¸í•œ ë¶„í• \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],  # ë¶„í•  ìš°ì„ ìˆœìœ„\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    processed_docs = []\n",
    "    chunk_stats = {\"total_chunks\": 0, \"avg_chunk_size\": 0, \"chunks_per_doc\": []}\n",
    "    \n",
    "    for doc_idx, doc in enumerate(tqdm(documents, desc=\"ë¬¸ì„œ ì²­í‚¹\")):\n",
    "        # ì œëª©ê³¼ ë‚´ìš©ì„ ê²°í•©í•˜ì—¬ ë” í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸ ì œê³µ\n",
    "        full_text = f\"ì œëª©: {doc['title']}\\n\\n{doc['content'].strip()}\"\n",
    "        \n",
    "        # ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• \n",
    "        chunks = text_splitter.split_text(full_text)\n",
    "        \n",
    "        # ê° ì²­í¬ë¥¼ LangChain Documentë¡œ ë³€í™˜\n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            # ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€ ì •ë³´ í¬í•¨\n",
    "            metadata = doc['metadata'].copy()\n",
    "            metadata.update({\n",
    "                \"doc_id\": doc_idx,\n",
    "                \"doc_title\": doc['title'],\n",
    "                \"chunk_id\": chunk_idx,\n",
    "                \"chunk_size\": len(chunk),\n",
    "                \"total_chunks_in_doc\": len(chunks)\n",
    "            })\n",
    "            \n",
    "            processed_docs.append(\n",
    "                Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        chunk_stats[\"chunks_per_doc\"].append(len(chunks))\n",
    "    \n",
    "    # í†µê³„ ì •ë³´ ê³„ì‚°\n",
    "    chunk_stats[\"total_chunks\"] = len(processed_docs)\n",
    "    chunk_stats[\"avg_chunk_size\"] = np.mean([len(doc.page_content) for doc in processed_docs])\n",
    "    \n",
    "    print(f\"\\nâœ… ë¬¸ì„œ ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "    print(f\"  ğŸ“Š ì „ì²˜ë¦¬ ê²°ê³¼:\")\n",
    "    print(f\"    - ì´ ì²­í¬ ìˆ˜: {chunk_stats['total_chunks']}ê°œ\")\n",
    "    print(f\"    - í‰ê·  ì²­í¬ í¬ê¸°: {chunk_stats['avg_chunk_size']:.1f} ê¸€ì\")\n",
    "    print(f\"    - ë¬¸ì„œë‹¹ ì²­í¬ ìˆ˜: {np.mean(chunk_stats['chunks_per_doc']):.1f}ê°œ\")\n",
    "    print(f\"    - ìµœëŒ€ ì²­í¬ ìˆ˜: {max(chunk_stats['chunks_per_doc'])}ê°œ\")\n",
    "    \n",
    "    return processed_docs, chunk_stats\n",
    "\n",
    "# ë¬¸ì„œ ì „ì²˜ë¦¬ ì‹¤í–‰\n",
    "processed_documents, chunking_stats = preprocess_documents(\n",
    "    sample_docs,\n",
    "    chunk_size=400,  # Naive RAGì—ì„œëŠ” ì ë‹¹í•œ í¬ê¸°ë¡œ ì„¤ì •\n",
    "    chunk_overlap=50  # ë¬¸ë§¥ ì—°ê²°ì„±ì„ ìœ„í•œ ì¤‘ë³µ\n",
    ")\n",
    "\n",
    "# ìƒ˜í”Œ ì²­í¬ í™•ì¸\n",
    "print(f\"\\nğŸ“‹ ìƒ˜í”Œ ì²­í¬ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"ğŸ“„ ì²« ë²ˆì§¸ ì²­í¬:\")\n",
    "print(f\"{processed_documents[0].page_content[:200]}...\")\n",
    "print(f\"\\nğŸ·ï¸ ë©”íƒ€ë°ì´í„°:\")\n",
    "for key, value in list(processed_documents[0].metadata.items())[:5]:\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ì„ë² ë”© ëª¨ë¸ ì„¤ì • ë° ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•\n",
    "\n",
    "### ğŸ§  ì„ë² ë”© ëª¨ë¸ ì„ íƒ\n",
    "í•œêµ­ì–´ ë¬¸ì„œì— ì í•©í•œ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë²¡í„° í‘œí˜„ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "- **all-MiniLM-L6-v2**: ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸\n",
    "- **FAISS**: Facebookì˜ íš¨ìœ¨ì ì¸ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_vector_store(documents: List[Document], \n",
    "                      embedding_model_name: str = \"BAAI/bge-m3\") -> FAISS:\n",
    "    \"\"\"\n",
    "    ì„ë² ë”© ëª¨ë¸ì„ ì„¤ì •í•˜ê³  FAISS ë²¡í„°ìŠ¤í† ì–´ë¥¼ êµ¬ì¶•í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        documents: ì „ì²˜ë¦¬ëœ Document ë¦¬ìŠ¤íŠ¸\n",
    "        embedding_model_name: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ëª…\n",
    "        \n",
    "    Returns:\n",
    "        FAISS ë²¡í„°ìŠ¤í† ì–´ ê°ì²´\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ğŸ§  ì„ë² ë”© ëª¨ë¸ ì„¤ì • ì¤‘...\")\n",
    "    print(f\"  ëª¨ë¸: {embedding_model_name}\")\n",
    "    \n",
    "    # HuggingFace ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "    # all-MiniLM-L6-v2: 384ì°¨ì›, ë¹ ë¥¸ ì†ë„, ê´œì°®ì€ ì„±ëŠ¥ì˜ ê· í˜•\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        model_kwargs={'device': 'cpu'},  # GPUê°€ ì—†ëŠ” í™˜ê²½ì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥\n",
    "        encode_kwargs={'normalize_embeddings': True}  # ë²¡í„° ì •ê·œí™”ë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ìµœì í™”\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "    \n",
    "    # ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•\n",
    "    print(f\"ğŸ” FAISS ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±\n",
    "    # ëª¨ë“  ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ì—¬ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings\n",
    "    )\n",
    "    \n",
    "    build_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"âœ… ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ!\")\n",
    "    print(f\"  ğŸ“Š êµ¬ì¶• ì •ë³´:\")\n",
    "    print(f\"    - ì¸ë±ì‹±ëœ ë¬¸ì„œ: {len(documents)}ê°œ\")\n",
    "    print(f\"    - ì„ë² ë”© ì°¨ì›: {embeddings.client.get_sentence_embedding_dimension()}ì°¨ì›\")\n",
    "    print(f\"    - êµ¬ì¶• ì‹œê°„: {build_time:.2f}ì´ˆ\")\n",
    "    print(f\"    - ë¬¸ì„œë‹¹ í‰ê·  ì‹œê°„: {build_time/len(documents):.3f}ì´ˆ\")\n",
    "    \n",
    "    return vectorstore, embeddings\n",
    "\n",
    "# ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•\n",
    "vector_store, embedding_model = setup_vector_store(processed_documents)\n",
    "\n",
    "# ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ (ì¬ì‚¬ìš©ì„ ìœ„í•´)\n",
    "vector_store.save_local(\"naive_rag_vectorstore\")\n",
    "print(f\"\\nğŸ’¾ ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ: './naive_rag_vectorstore'\")\n",
    "print(f\"ğŸ’¡ ì €ì¥ëœ ë²¡í„°ìŠ¤í† ì–´ëŠ” ë‹¤ìŒ ì‹¤ìŠµì—ì„œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Naive RAG íŒŒì´í”„ë¼ì¸ êµ¬í˜„\n",
    "\n",
    "### ğŸ”„ ê¸°ë³¸ RAG ì›Œí¬í”Œë¡œìš°\n",
    "1. **ì¿¼ë¦¬ ì„ë² ë”©**: ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜\n",
    "2. **ìœ ì‚¬ë„ ê²€ìƒ‰**: ë²¡í„°ìŠ¤í† ì–´ì—ì„œ Top-K ë¬¸ì„œ ê²€ìƒ‰\n",
    "3. **ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±**: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í”„ë¡¬í”„íŠ¸ì— í¬í•¨\n",
    "4. **ë‹µë³€ ìƒì„±**: LLMì„ í†µí•œ ìµœì¢… ë‹µë³€ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 1 íŒŒì¸íŠœë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ì‹¤ì œ LLM í´ë˜ìŠ¤\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from typing import Optional, List, Any, Dict\n",
    "from pydantic import Field\n",
    "\n",
    "class Day1FinetunedLLM(LLM):\n",
    "    \"\"\"\n",
    "    Day 1ì—ì„œ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì‚¬ìš©í•˜ëŠ” ì‹¤ì œ LLM í´ë˜ìŠ¤\n",
    "    ì—¬ëŸ¬ë¶„ì˜ íŒŒì¸íŠœë‹ ëª¨ë¸ ì£¼ì†Œ: https://huggingface.co/ryanu/my-exaone-raft-model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pydantic field declarations\n",
    "    model_name: str = Field(default=\"ryanu/my-exaone-raft-model\")\n",
    "    base_model: str = Field(default=\"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\")\n",
    "    usage_config: Dict[str, Any] = Field(default_factory=lambda: {\n",
    "        \"max_length\": 512,\n",
    "        \"temperature\": 0.7,\n",
    "        \"do_sample\": True\n",
    "    })\n",
    "    tokenizer: Any = Field(default=None)\n",
    "    model: Any = Field(default=None)\n",
    "    model_loaded: bool = Field(default=False)\n",
    "    \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "    \n",
    "    def __init__(self, model_name: str = \"ryanu/my-exaone-raft-model\", **kwargs):\n",
    "        super().__init__(\n",
    "            model_name=model_name,\n",
    "            base_model=\"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n",
    "            usage_config={\n",
    "                \"max_length\": 512,\n",
    "                \"temperature\": 0.7,\n",
    "                \"do_sample\": True\n",
    "            },\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        print(f\"ğŸ¯ Day 1 íŒŒì¸íŠœë‹ ëª¨ë¸ ì‚¬ìš©: {self.model_name}\")\n",
    "        print(f\"ğŸ“ ëª¨ë¸ ì£¼ì†Œ: https://huggingface.co/{self.model_name}\")\n",
    "        \n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"ì‹¤ì œ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "        print(f\"ğŸ”„ Day 1 íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_name}\")\n",
    "        \n",
    "        # Hugging Faceì—ì„œ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì§ì ‘ ë¡œë“œ\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # íŒ¨ë”© í† í° ì„¤ì •\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        print(f\"âœ… Day 1 íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "        self.model_loaded = True\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"day1_finetuned_exaone_llm\"\n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \n",
    "        # EXAONE ëª¨ë¸ì— ì í•©í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©\n",
    "        formatted_prompt = f\"[|system|]ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.[|endofturn|]\\n[|user|]{prompt}[|endofturn|]\\n[|assistant|]\"\n",
    "        \n",
    "        # ì‹¤ì œ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ë¡œ ì¶”ë¡ \n",
    "        inputs = self.tokenizer(\n",
    "            formatted_prompt, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True,\n",
    "            max_length=1024\n",
    "        )\n",
    "        \n",
    "        # ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.usage_config.get(\"max_length\", 512),\n",
    "                temperature=self.usage_config.get(\"temperature\", 0.7),\n",
    "                do_sample=self.usage_config.get(\"do_sample\", True),\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "        \n",
    "        # ì‘ë‹µ ë””ì½”ë”© (ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œì™¸)\n",
    "        input_length = inputs[\"input_ids\"].shape[1]\n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][input_length:], \n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "        \n",
    "        return response if response else \"ì£„ì†¡í•©ë‹ˆë‹¤. ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "def create_naive_rag_chain(vectorstore: FAISS, llm: LLM, k: int = 3) -> RetrievalQA:\n",
    "    \"\"\"\n",
    "    Naive RAG ì²´ì¸ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: FAISS ë²¡í„°ìŠ¤í† ì–´\n",
    "        llm: ì–¸ì–´ ëª¨ë¸ (Day 1 íŒŒì¸íŠœë‹ ëª¨ë¸)\n",
    "        k: ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜ (Top-K)\n",
    "        \n",
    "    Returns:\n",
    "        RetrievalQA ì²´ì¸\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ğŸ”— Naive RAG ì²´ì¸ ìƒì„± ì¤‘...\")\n",
    "    print(f\"  ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜ (K): {k}\")\n",
    "    \n",
    "    # ë²¡í„°ìŠ¤í† ì–´ë¥¼ ê²€ìƒ‰ê¸°(Retriever)ë¡œ ë³€í™˜\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": k}\n",
    "    )\n",
    "    \n",
    "    # RetrievalQA ì²´ì¸ ìƒì„±\n",
    "    rag_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Naive RAG ì²´ì¸ ìƒì„± ì™„ë£Œ!\")\n",
    "    \n",
    "    return rag_chain\n",
    "\n",
    "# Day 1 íŒŒì¸íŠœë‹ ëª¨ë¸ ë° RAG ì²´ì¸ ìƒì„±\n",
    "print(\"ğŸš€ Day 1 íŒŒì¸íŠœë‹ ëª¨ë¸ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...\")\n",
    "day1_llm = Day1FinetunedLLM(\"ryanu/my-exaone-raft-model\")\n",
    "naive_rag = create_naive_rag_chain(vector_store, day1_llm, k=3)\n",
    "\n",
    "print(f\"\\nğŸ‰ ì‹¤ì œ íŒŒì¸íŠœë‹ ëª¨ë¸ ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ì„±ëŠ¥ ì¸¡ì • ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "\n",
    "### ğŸ“Š RAG ì„±ëŠ¥ í‰ê°€ ë©”íŠ¸ë¦­\n",
    "- **ì‘ë‹µ ì‹œê°„**: ì§ˆë¬¸ë¶€í„° ë‹µë³€ê¹Œì§€ ê±¸ë¦¬ëŠ” ì‹œê°„\n",
    "- **ê²€ìƒ‰ ì •í™•ë„**: ê´€ë ¨ ë¬¸ì„œê°€ ìƒìœ„ì— ê²€ìƒ‰ë˜ëŠ” ë¹„ìœ¨\n",
    "- **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´**: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ì´ ê¸¸ì´\n",
    "- **í† í° ì‚¬ìš©ëŸ‰**: LLM API í˜¸ì¶œ ì‹œ ì‚¬ìš©ë˜ëŠ” í† í° ìˆ˜\n",
    "- **í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€**: ë‹µë³€ì´ ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ì •ë„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPerformanceTracker:\n",
    "    \"\"\"\n",
    "    RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì¶”ì í•˜ê³  ë¶„ì„í•˜ëŠ” í´ë˜ìŠ¤\n",
    "    \n",
    "    ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ì„ ìë™ìœ¼ë¡œ ì¸¡ì •í•˜ê³  ì‹œê°í™”í•˜ì—¬\n",
    "    RAG ì‹œìŠ¤í…œì˜ ê°•ì ê³¼ ì•½ì ì„ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.performance_log = []\n",
    "        self.query_count = 0\n",
    "        \n",
    "    def measure_query_performance(self, \n",
    "                                 rag_chain: RetrievalQA, \n",
    "                                 query: str, \n",
    "                                 expected_keywords: List[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        ë‹¨ì¼ ì¿¼ë¦¬ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜\n",
    "        \n",
    "        Args:\n",
    "            rag_chain: RAG ì²´ì¸\n",
    "            query: ì§ˆë¬¸\n",
    "            expected_keywords: ë‹µë³€ì— í¬í•¨ë˜ì–´ì•¼ í•  í•µì‹¬ í‚¤ì›Œë“œ\n",
    "            \n",
    "        Returns:\n",
    "            ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼\n",
    "        \"\"\"\n",
    "        \n",
    "        self.query_count += 1\n",
    "        \n",
    "        # ì‹œì‘ ì‹œê°„ ê¸°ë¡\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # RAG ì²´ì¸ ì‹¤í–‰\n",
    "        result = rag_chain({\"query\": query})\n",
    "        \n",
    "        # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡\n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        # ê²°ê³¼ ì¶”ì¶œ\n",
    "        answer = result[\"result\"]\n",
    "        source_docs = result[\"source_documents\"]\n",
    "        \n",
    "        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "        metrics = {\n",
    "            \"query_id\": self.query_count,\n",
    "            \"query\": query,\n",
    "            \"answer\": answer,\n",
    "            \"response_time\": response_time,\n",
    "            \"retrieved_docs_count\": len(source_docs),\n",
    "            \"total_context_length\": sum(len(doc.page_content) for doc in source_docs),\n",
    "            \"avg_context_length\": np.mean([len(doc.page_content) for doc in source_docs]) if source_docs else 0,\n",
    "            \"unique_sources\": len(set(doc.metadata.get(\"doc_title\", \"\") for doc in source_docs)),\n",
    "        }\n",
    "        \n",
    "        # í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€ ê³„ì‚°\n",
    "        if expected_keywords:\n",
    "            answer_lower = answer.lower()\n",
    "            covered_keywords = [kw for kw in expected_keywords if kw.lower() in answer_lower]\n",
    "            metrics[\"keyword_coverage\"] = len(covered_keywords) / len(expected_keywords)\n",
    "            metrics[\"covered_keywords\"] = covered_keywords\n",
    "        else:\n",
    "            metrics[\"keyword_coverage\"] = None\n",
    "        \n",
    "        # ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´\n",
    "        doc_info = []\n",
    "        for i, doc in enumerate(source_docs):\n",
    "            doc_info.append({\n",
    "                \"rank\": i + 1,\n",
    "                \"title\": doc.metadata.get(\"doc_title\", \"Unknown\"),\n",
    "                \"category\": doc.metadata.get(\"category\", \"Unknown\"),\n",
    "                \"chunk_size\": len(doc.page_content),\n",
    "                \"content_preview\": doc.page_content[:100] + \"...\"\n",
    "            })\n",
    "        \n",
    "        metrics[\"retrieved_documents\"] = doc_info\n",
    "        metrics[\"timestamp\"] = datetime.now().isoformat()\n",
    "        \n",
    "        # ë¡œê·¸ì— ì¶”ê°€\n",
    "        self.performance_log.append(metrics)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def get_performance_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        ì „ì²´ ì„±ëŠ¥ ìš”ì•½ í†µê³„ ìƒì„±\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.performance_log:\n",
    "            return {\"message\": \"ì„±ëŠ¥ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\"}\n",
    "        \n",
    "        response_times = [log[\"response_time\"] for log in self.performance_log]\n",
    "        context_lengths = [log[\"total_context_length\"] for log in self.performance_log]\n",
    "        keyword_coverages = [log[\"keyword_coverage\"] for log in self.performance_log if log[\"keyword_coverage\"] is not None]\n",
    "        \n",
    "        summary = {\n",
    "            \"total_queries\": len(self.performance_log),\n",
    "            \"avg_response_time\": np.mean(response_times),\n",
    "            \"min_response_time\": np.min(response_times),\n",
    "            \"max_response_time\": np.max(response_times),\n",
    "            \"avg_context_length\": np.mean(context_lengths),\n",
    "            \"avg_keyword_coverage\": np.mean(keyword_coverages) if keyword_coverages else None,\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def save_performance_log(self, filename: str = \"naive_rag_performance.json\"):\n",
    "        \"\"\"\n",
    "        ì„±ëŠ¥ ë¡œê·¸ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "        \"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                \"logs\": self.performance_log,\n",
    "                \"summary\": self.get_performance_summary()\n",
    "            }, f, ensure_ascii=False, indent=2, default=str)\n",
    "        \n",
    "        print(f\"ğŸ’¾ ì„±ëŠ¥ ë¡œê·¸ ì €ì¥ ì™„ë£Œ: {filename}\")\n",
    "\n",
    "# ì„±ëŠ¥ ì¶”ì ê¸° ì´ˆê¸°í™”\n",
    "performance_tracker = RAGPerformanceTracker()\n",
    "\n",
    "print(\"ğŸ“Š RAG ì„±ëŠ¥ ì¸¡ì • ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(\"ğŸ’¡ ì´ì œ ë‹¤ì–‘í•œ ì§ˆë¬¸ìœ¼ë¡œ Naive RAGì˜ ì„±ëŠ¥ì„ ì²´ê³„ì ìœ¼ë¡œ ì¸¡ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Naive RAG ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "### ğŸ§ª ë‹¤ì–‘í•œ ì§ˆë¬¸ ìœ í˜•ìœ¼ë¡œ í…ŒìŠ¤íŠ¸\n",
    "Naive RAGì˜ ê°•ì ê³¼ ì•½ì ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ìœ í˜•ì˜ ì§ˆë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì„¸íŠ¸ ì •ì˜\n",
    "test_queries = [\n",
    "    {\n",
    "        \"query\": \"ChatGPTëŠ” ì–¸ì œ ì¶œì‹œë˜ì—ˆë‚˜ìš”?\",\n",
    "        \"expected_keywords\": [\"2022ë…„\", \"11ì›”\", \"OpenAI\", \"ì¶œì‹œ\"],\n",
    "        \"difficulty\": \"easy\",\n",
    "        \"type\": \"factual\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"RAG ê¸°ìˆ ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”\",\n",
    "        \"expected_keywords\": [\"ê²€ìƒ‰\", \"ì¦ê°•\", \"ìƒì„±\", \"ë²¡í„°\", \"Retrieval\"],\n",
    "        \"difficulty\": \"medium\",\n",
    "        \"type\": \"explanation\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "        \"expected_keywords\": [\"ë¨¸ì‹ ëŸ¬ë‹\", \"ë”¥ëŸ¬ë‹\", \"ì‹ ê²½ë§\", \"ì°¨ì´\", \"ë°ì´í„°\"],\n",
    "        \"difficulty\": \"medium\",\n",
    "        \"type\": \"comparison\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"íŒŒì´ì¬ìœ¼ë¡œ í•  ìˆ˜ ìˆëŠ” ì¼ë“¤ì„ ì•Œë ¤ì£¼ì„¸ìš”\",\n",
    "        \"expected_keywords\": [\"íŒŒì´ì¬\", \"ë°ì´í„°\", \"ì›¹\", \"ê°œë°œ\", \"ë¼ì´ë¸ŒëŸ¬ë¦¬\"],\n",
    "        \"difficulty\": \"medium\",\n",
    "        \"type\": \"application\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"2024ë…„ AI ì •ì±…ì— ëŒ€í•œ íˆ¬ì ê³„íšì€?\",\n",
    "        \"expected_keywords\": [\"2024\", \"ì •ì±…\", \"íˆ¬ì\", \"9ì¡°\", \"AIê°•êµ­\"],\n",
    "        \"difficulty\": \"hard\",\n",
    "        \"type\": \"specific_temporal\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?\",\n",
    "        \"expected_keywords\": [\"ë²¡í„°\", \"ë°ì´í„°ë² ì´ìŠ¤\", \"ìœ ì‚¬ë„\", \"ì„ë² ë”©\", \"ê²€ìƒ‰\"],\n",
    "        \"difficulty\": \"hard\",\n",
    "        \"type\": \"technical\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def run_performance_test(rag_chain: RetrievalQA, \n",
    "                        tracker: RAGPerformanceTracker, \n",
    "                        test_queries: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì„¸íŠ¸ë¡œ RAG ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€\n",
    "    \n",
    "    Args:\n",
    "        rag_chain: RAG ì²´ì¸\n",
    "        tracker: ì„±ëŠ¥ ì¶”ì ê¸°\n",
    "        test_queries: í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸\n",
    "        \n",
    "    Returns:\n",
    "        ì¸¡ì • ê²°ê³¼ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ğŸ§ª Naive RAG ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘!\")\n",
    "    print(f\"ğŸ“ ì´ {len(test_queries)}ê°œ ì§ˆë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸...\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    test_results = []\n",
    "    \n",
    "    for i, test_case in enumerate(tqdm(test_queries, desc=\"ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\")):\n",
    "        query = test_case[\"query\"]\n",
    "        expected_keywords = test_case[\"expected_keywords\"]\n",
    "        \n",
    "        print(f\"\\nğŸ” ì§ˆë¬¸ {i+1}: {query}\")\n",
    "        \n",
    "        # ì„±ëŠ¥ ì¸¡ì •\n",
    "        metrics = tracker.measure_query_performance(\n",
    "            rag_chain, query, expected_keywords\n",
    "        )\n",
    "        \n",
    "        # ì¶”ê°€ ì •ë³´ ì €ì¥\n",
    "        metrics.update({\n",
    "            \"difficulty\": test_case[\"difficulty\"],\n",
    "            \"type\": test_case[\"type\"]\n",
    "        })\n",
    "        \n",
    "        test_results.append(metrics)\n",
    "        \n",
    "        # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°\n",
    "        print(f\"  â±ï¸ ì‘ë‹µì‹œê°„: {metrics['response_time']:.3f}ì´ˆ\")\n",
    "        print(f\"  ğŸ“„ ê²€ìƒ‰ë¬¸ì„œ: {metrics['retrieved_docs_count']}ê°œ\")\n",
    "        print(f\"  ğŸ“ ì»¨í…ìŠ¤íŠ¸: {metrics['total_context_length']}ì\")\n",
    "        if metrics['keyword_coverage'] is not None:\n",
    "            print(f\"  ğŸ¯ í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€: {metrics['keyword_coverage']:.1%}\")\n",
    "        print(f\"  ğŸ’¬ ë‹µë³€: {metrics['answer'][:100]}...\")\n",
    "        \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"âœ… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "print(\"ğŸš€ Naive RAG ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤!\")\n",
    "test_results = run_performance_test(naive_rag, performance_tracker, test_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ì„±ëŠ¥ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”\n",
    "\n",
    "### ğŸ“Š Naive RAGì˜ ê°•ì ê³¼ ì•½ì  íŒŒì•…\n",
    "ì¸¡ì •ëœ ì„±ëŠ¥ ë°ì´í„°ë¥¼ ì‹œê°í™”í•˜ì—¬ ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­ì„ ì‹ë³„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_analysis_dashboard(test_results: List[Dict[str, Any]], \n",
    "                                        tracker: RAGPerformanceTracker):\n",
    "    import os\n",
    "    import matplotlib as mpl\n",
    "    import matplotlib.font_manager as fm\n",
    "\n",
    "    font_candidates = [\n",
    "        (\"NanumBarunGothic\", \"/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf\"),\n",
    "        (\"NanumGothic\",      \"/usr/share/fonts/truetype/nanum/NanumGothic.ttf\"),\n",
    "    ]\n",
    "    selected = None\n",
    "    for name, path in font_candidates:\n",
    "        if any(f.name == name for f in fm.fontManager.ttflist):\n",
    "            selected = name; break\n",
    "        if os.path.exists(path):\n",
    "            fm.fontManager.addfont(path)\n",
    "            try: fm._load_fontmanager(try_read_cache=False)  # mpl>=3.6\n",
    "            except TypeError: fm._rebuild()                  # fallback\n",
    "            if any(f.name == name for f in fm.fontManager.ttflist):\n",
    "                selected = name; break\n",
    "    if selected is None:\n",
    "        selected = \"DejaVu Sans\"  # ìµœí›„ ë³´ë£¨(í•œê¸€ ë¯¸í¬í•¨ì¼ ìˆ˜ ìˆìŒ)\n",
    "\n",
    "    mpl.rcParams[\"font.family\"] = selected\n",
    "    mpl.rcParams[\"axes.unicode_minus\"] = False\n",
    "    \"\"\"\n",
    "    Naive RAG ì„±ëŠ¥ ë¶„ì„ ëŒ€ì‹œë³´ë“œ ìƒì„±\n",
    "    \n",
    "    ì´ í•¨ìˆ˜ëŠ” ë‹¤ì–‘í•œ ê´€ì ì—ì„œ RAG ì„±ëŠ¥ì„ ì‹œê°í™”í•©ë‹ˆë‹¤:\n",
    "    1. ğŸ“Š ì‘ë‹µ ì‹œê°„ ë¶„ì„: ì§ˆë¬¸ ìœ í˜•ë³„ ì‘ë‹µ ì†ë„ ë¹„êµ\n",
    "    2. ğŸ¯ í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€: ë‹µë³€ ì •í™•ë„ ì¸¡ì •  \n",
    "    3. ğŸ“„ ê²€ìƒ‰ í’ˆì§ˆ: ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜ì™€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„\n",
    "    4. ğŸ” ì§ˆë¬¸ ë‚œì´ë„ë³„ ì„±ëŠ¥: ì‰¬ìš´/ì¤‘ê°„/ì–´ë ¤ìš´ ì§ˆë¬¸ë³„ ì„±ëŠ¥ ì°¨ì´\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ğŸ“Š Naive RAG ì„±ëŠ¥ ë¶„ì„ ëŒ€ì‹œë³´ë“œ ìƒì„± ì¤‘...\")\n",
    "    \n",
    "    # ë°ì´í„° ì¤€ë¹„\n",
    "    df = pd.DataFrame(test_results)\n",
    "    \n",
    "    # ëŒ€ì‹œë³´ë“œ ìƒì„± (2x2 ì„œë¸Œí”Œë¡¯)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('ğŸš€ Naive RAG ì„±ëŠ¥ ë¶„ì„ ëŒ€ì‹œë³´ë“œ', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. ì‘ë‹µ ì‹œê°„ ë¶„ì„ (ì¢Œìƒë‹¨)\n",
    "    # ğŸ“Š ì˜ë¯¸: ì§ˆë¬¸ ìœ í˜•ë³„ë¡œ ì‘ë‹µ ì‹œê°„ì´ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ ë¶„ì„\n",
    "    # - ë³µì¡í•œ ì§ˆë¬¸ì¼ìˆ˜ë¡ ê²€ìƒ‰ê³¼ ìƒì„±ì— ë” ì˜¤ë˜ ê±¸ë¦¬ëŠ”ì§€ í™•ì¸\n",
    "    # - ì¼ê´€ëœ ì‘ë‹µ ì†ë„ë¥¼ ìœ ì§€í•˜ëŠ”ì§€ í‰ê°€\n",
    "    difficulties = df['difficulty'].tolist()\n",
    "    response_times = df['response_time'].tolist()\n",
    "    \n",
    "    difficulty_order = ['easy', 'medium', 'hard']\n",
    "    difficulty_colors = ['lightgreen', 'orange', 'lightcoral']\n",
    "    \n",
    "    for i, diff in enumerate(difficulty_order):\n",
    "        diff_data = df[df['difficulty'] == diff]['response_time']\n",
    "        if not diff_data.empty:\n",
    "            axes[0, 0].bar(i, diff_data.mean(), \n",
    "                          color=difficulty_colors[i], alpha=0.7, \n",
    "                          label=f'{diff.capitalize()} (í‰ê· : {diff_data.mean():.3f}ì´ˆ)')\n",
    "            \n",
    "            # ê°œë³„ ë°ì´í„° ì  í‘œì‹œ\n",
    "            for j, value in enumerate(diff_data):\n",
    "                axes[0, 0].scatter(i + (j-len(diff_data)/2)*0.1, value, \n",
    "                                  color='black', alpha=0.6, s=30)\n",
    "    \n",
    "    axes[0, 0].set_xlabel('ì§ˆë¬¸ ë‚œì´ë„')\n",
    "    axes[0, 0].set_ylabel('ì‘ë‹µ ì‹œê°„ (ì´ˆ)')\n",
    "    axes[0, 0].set_title('â±ï¸ ë‚œì´ë„ë³„ ì‘ë‹µ ì‹œê°„ ë¶„ì„\\n(ì : ê°œë³„ ì§ˆë¬¸, ë§‰ëŒ€: í‰ê· )')\n",
    "    axes[0, 0].set_xticks(range(len(difficulty_order)))\n",
    "    axes[0, 0].set_xticklabels([d.capitalize() for d in difficulty_order])\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€ ë¶„ì„ (ìš°ìƒë‹¨)\n",
    "    # ğŸ“Š ì˜ë¯¸: ë‹µë³€ì´ ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì–¼ë§ˆë‚˜ ì˜ í¬í•¨í•˜ëŠ”ì§€ ì¸¡ì •\n",
    "    # - ë†’ì€ ì»¤ë²„ë¦¬ì§€ = ê´€ë ¨ì„± ë†’ì€ ë‹µë³€\n",
    "    # - ë‚®ì€ ì»¤ë²„ë¦¬ì§€ = ë¶€ì •í™•í•˜ê±°ë‚˜ ë¶ˆì™„ì „í•œ ë‹µë³€\n",
    "    keyword_coverages = [r['keyword_coverage'] for r in test_results if r['keyword_coverage'] is not None]\n",
    "    query_names = [f\"Q{i+1}\" for i in range(len(keyword_coverages))]\n",
    "    \n",
    "    bars = axes[0, 1].bar(query_names, keyword_coverages, \n",
    "                         color=['green' if cov >= 0.8 else 'orange' if cov >= 0.5 else 'red' \n",
    "                               for cov in keyword_coverages], alpha=0.7)\n",
    "    \n",
    "    # ì»¤ë²„ë¦¬ì§€ ìˆ˜ì¹˜ í‘œì‹œ\n",
    "    for bar, cov in zip(bars, keyword_coverages):\n",
    "        height = bar.get_height()\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                        f'{cov:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    axes[0, 1].set_xlabel('ì§ˆë¬¸')\n",
    "    axes[0, 1].set_ylabel('í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€')\n",
    "    axes[0, 1].set_title('ğŸ¯ í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€ ë¶„ì„\\n(ì´ˆë¡: ìš°ìˆ˜ â‰¥80%, ì£¼í™©: ë³´í†µ â‰¥50%, ë¹¨ê°•: ë¶€ì¡± <50%)')\n",
    "    axes[0, 1].set_ylim(0, 1.1)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # í‰ê·  ì»¤ë²„ë¦¬ì§€ ë¼ì¸ ì¶”ê°€\n",
    "    avg_coverage = np.mean(keyword_coverages)\n",
    "    axes[0, 1].axhline(y=avg_coverage, color='blue', linestyle='--', alpha=0.7,\n",
    "                      label=f'í‰ê· : {avg_coverage:.1%}')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # 3. ê²€ìƒ‰ í’ˆì§ˆ ë¶„ì„ (ì¢Œí•˜ë‹¨)\n",
    "    # ğŸ“Š ì˜ë¯¸: ê²€ìƒ‰ëœ ë¬¸ì„œì˜ í’ˆì§ˆê³¼ ë‹¤ì–‘ì„± ë¶„ì„\n",
    "    # - ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: ë„ˆë¬´ ì§§ìœ¼ë©´ ì •ë³´ ë¶€ì¡±, ë„ˆë¬´ ê¸¸ë©´ ë…¸ì´ì¦ˆ\n",
    "    # - ê³ ìœ  ì†ŒìŠ¤ ìˆ˜: ë‹¤ì–‘í•œ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì™”ëŠ”ì§€ í™•ì¸\n",
    "    context_lengths = df['total_context_length'].tolist()\n",
    "    unique_sources = df['unique_sources'].tolist()\n",
    "    \n",
    "    # ì‚°ì ë„ë¡œ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ vs ê³ ìœ  ì†ŒìŠ¤ ìˆ˜ ê´€ê³„ ì‹œê°í™”\n",
    "    scatter = axes[1, 0].scatter(context_lengths, unique_sources, \n",
    "                                c=response_times, cmap='viridis', alpha=0.7, s=100)\n",
    "    \n",
    "    # ì»¬ëŸ¬ë°” ì¶”ê°€ (ì‘ë‹µ ì‹œê°„)\n",
    "    cbar = plt.colorbar(scatter, ax=axes[1, 0])\n",
    "    cbar.set_label('ì‘ë‹µ ì‹œê°„ (ì´ˆ)', rotation=270, labelpad=15)\n",
    "    \n",
    "    # ê° ì ì— ì§ˆë¬¸ ë²ˆí˜¸ í‘œì‹œ\n",
    "    for i, (x, y) in enumerate(zip(context_lengths, unique_sources)):\n",
    "        axes[1, 0].annotate(f'Q{i+1}', (x, y), xytext=(5, 5), \n",
    "                           textcoords='offset points', fontsize=9, alpha=0.8)\n",
    "    \n",
    "    axes[1, 0].set_xlabel('ì´ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ (ë¬¸ì ìˆ˜)')\n",
    "    axes[1, 0].set_ylabel('ê³ ìœ  ë¬¸ì„œ ì†ŒìŠ¤ ìˆ˜')\n",
    "    axes[1, 0].set_title('ğŸ“„ ê²€ìƒ‰ í’ˆì§ˆ ë¶„ì„\\n(ìƒ‰ìƒ: ì‘ë‹µì‹œê°„, ìœ„ì¹˜: ì»¨í…ìŠ¤íŠ¸ vs ë‹¤ì–‘ì„±)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. ì§ˆë¬¸ ìœ í˜•ë³„ ì¢…í•© ì„±ëŠ¥ (ìš°í•˜ë‹¨)\n",
    "    # ğŸ“Š ì˜ë¯¸: ì§ˆë¬¸ ìœ í˜•(ì‚¬ì‹¤, ì„¤ëª…, ë¹„êµ ë“±)ë³„ ì¢…í•© ì„±ëŠ¥ ë¹„êµ\n",
    "    # - ì–´ë–¤ ìœ í˜•ì˜ ì§ˆë¬¸ì— Naive RAGê°€ ê°•í•œì§€/ì•½í•œì§€ íŒŒì•…\n",
    "    # - í–¥í›„ ê°œì„  ë°©í–¥ ê²°ì •ì— ì¤‘ìš”í•œ ì •ë³´\n",
    "    type_performance = df.groupby('type').agg({\n",
    "        'response_time': 'mean',\n",
    "        'keyword_coverage': 'mean',\n",
    "        'total_context_length': 'mean'\n",
    "    }).fillna(0)\n",
    "    \n",
    "    # ì •ê·œí™” (0-1 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ ìš©ì´í•˜ê²Œ)\n",
    "    normalized_perf = type_performance.copy()\n",
    "    normalized_perf['response_time'] = 1 - (normalized_perf['response_time'] - normalized_perf['response_time'].min()) / (normalized_perf['response_time'].max() - normalized_perf['response_time'].min() + 1e-8)  # ì‘ë‹µì‹œê°„ì€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ\n",
    "    normalized_perf['keyword_coverage'] = normalized_perf['keyword_coverage']  # ì´ë¯¸ 0-1 ìŠ¤ì¼€ì¼\n",
    "    normalized_perf['total_context_length'] = (normalized_perf['total_context_length'] - normalized_perf['total_context_length'].min()) / (normalized_perf['total_context_length'].max() - normalized_perf['total_context_length'].min() + 1e-8)\n",
    "    \n",
    "    # íˆíŠ¸ë§µìœ¼ë¡œ í‘œì‹œ\n",
    "    im = axes[1, 1].imshow(normalized_perf.T, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "    \n",
    "    # ì¶• ë¼ë²¨ ì„¤ì •\n",
    "    axes[1, 1].set_xticks(range(len(normalized_perf.index)))\n",
    "    axes[1, 1].set_xticklabels(normalized_perf.index, rotation=45, ha='right')\n",
    "    axes[1, 1].set_yticks(range(len(normalized_perf.columns)))\n",
    "    axes[1, 1].set_yticklabels(['ì‘ë‹µì†ë„', 'í‚¤ì›Œë“œì»¤ë²„ë¦¬ì§€', 'ì»¨í…ìŠ¤íŠ¸ê¸¸ì´'])\n",
    "    \n",
    "    # ìˆ˜ì¹˜ í‘œì‹œ\n",
    "    for i in range(len(normalized_perf.index)):\n",
    "        for j in range(len(normalized_perf.columns)):\n",
    "            text = axes[1, 1].text(i, j, f'{normalized_perf.iloc[i, j]:.2f}',\n",
    "                                  ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "    \n",
    "    axes[1, 1].set_title('ğŸ” ì§ˆë¬¸ ìœ í˜•ë³„ ì¢…í•© ì„±ëŠ¥\\n(1.0: ìµœê³ , 0.0: ìµœì €)')\n",
    "    \n",
    "    # ì»¬ëŸ¬ë°” ì¶”ê°€\n",
    "    cbar2 = plt.colorbar(im, ax=axes[1, 1])\n",
    "    cbar2.set_label('ì •ê·œí™” ì„±ëŠ¥ ì ìˆ˜', rotation=270, labelpad=15)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ê·¸ë˜í”„ ì €ì¥\n",
    "    plt.savefig('naive_rag_performance_dashboard.png', dpi=300, bbox_inches='tight',\n",
    "                facecolor='white', edgecolor='none')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… ì„±ëŠ¥ ë¶„ì„ ëŒ€ì‹œë³´ë“œ ìƒì„± ì™„ë£Œ\")\n",
    "    print(\"ğŸ’¾ ì €ì¥ëœ íŒŒì¼: naive_rag_performance_dashboard.png\")\n",
    "    \n",
    "    # ì„±ëŠ¥ ìš”ì•½ ì¶œë ¥\n",
    "    summary = tracker.get_performance_summary()\n",
    "    print(f\"\\nğŸ“‹ Naive RAG ì„±ëŠ¥ ìš”ì•½:\")\n",
    "    print(f\"  ğŸ“Š ì´ ì¿¼ë¦¬: {summary['total_queries']}ê°œ\")\n",
    "    print(f\"  â±ï¸ í‰ê·  ì‘ë‹µì‹œê°„: {summary['avg_response_time']:.3f}ì´ˆ\")\n",
    "    print(f\"  ğŸ“ í‰ê·  ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {summary['avg_context_length']:.0f}ì\")\n",
    "    if summary['avg_keyword_coverage']:\n",
    "        print(f\"  ğŸ¯ í‰ê·  í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€: {summary['avg_keyword_coverage']:.1%}\")\n",
    "    \n",
    "    return fig, summary\n",
    "\n",
    "# ì„±ëŠ¥ ë¶„ì„ ëŒ€ì‹œë³´ë“œ ìƒì„±\n",
    "dashboard_fig, performance_summary = create_performance_analysis_dashboard(test_results, performance_tracker)\n",
    "\n",
    "# ì„±ëŠ¥ ë¡œê·¸ ì €ì¥\n",
    "performance_tracker.save_performance_log(\"naive_rag_baseline.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Naive RAGì˜ í•œê³„ì  ë¶„ì„\n",
    "\n",
    "### ğŸ” ë°œê²¬ëœ ì£¼ìš” ë¬¸ì œì ë“¤\n",
    "ì¸¡ì • ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ Naive RAGì˜ êµ¬ì¡°ì  í•œê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_naive_rag_limitations(test_results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Naive RAGì˜ í•œê³„ì ì„ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    ì¸¡ì •ëœ ì„±ëŠ¥ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ ê°œì„  ë°©í–¥ì„ ì œì‹œí•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Returns:\n",
    "        í•œê³„ì  ë¶„ì„ ê²°ê³¼ ë° ê°œì„  ì œì•ˆ\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ğŸ” Naive RAG í•œê³„ì  ë¶„ì„ ì¤‘...\")\n",
    "    \n",
    "    limitations = {\n",
    "        \"ê²€ìƒ‰ í’ˆì§ˆ ë¬¸ì œ\": [],\n",
    "        \"ì‘ë‹µ í’ˆì§ˆ ë¬¸ì œ\": [],\n",
    "        \"íš¨ìœ¨ì„± ë¬¸ì œ\": [],\n",
    "        \"í™•ì¥ì„± ë¬¸ì œ\": []\n",
    "    }\n",
    "    \n",
    "    improvements_needed = []\n",
    "    \n",
    "    # 1. í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€ ë¶„ì„\n",
    "    keyword_coverages = [r['keyword_coverage'] for r in test_results if r['keyword_coverage'] is not None]\n",
    "    low_coverage_count = sum(1 for cov in keyword_coverages if cov < 0.6)\n",
    "    \n",
    "    if low_coverage_count > len(keyword_coverages) * 0.3:  # 30% ì´ìƒì´ ë‚®ì€ ì»¤ë²„ë¦¬ì§€\n",
    "        limitations[\"ì‘ë‹µ í’ˆì§ˆ ë¬¸ì œ\"].append({\n",
    "            \"ë¬¸ì œ\": \"ë‚®ì€ í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€\",\n",
    "            \"ì„¤ëª…\": f\"{low_coverage_count}/{len(keyword_coverages)} ì§ˆë¬¸ì—ì„œ 60% ë¯¸ë§Œì˜ í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€\",\n",
    "            \"ì›ì¸\": \"ë‹¨ìˆœ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ì˜ í•œê³„, ë™ì˜ì–´/ìœ ì˜ì–´ ì²˜ë¦¬ ë¶€ì¡±\"\n",
    "        })\n",
    "        improvements_needed.append(\"Multi-Query RAGë¡œ ë‹¤ì–‘í•œ í‘œí˜„ ê²€ìƒ‰\")\n",
    "        improvements_needed.append(\"Hybrid Search (BM25 + Vector) ë„ì…\")\n",
    "    \n",
    "    # 2. ì‘ë‹µ ì‹œê°„ ì¼ê´€ì„± ë¶„ì„\n",
    "    response_times = [r['response_time'] for r in test_results]\n",
    "    time_std = np.std(response_times)\n",
    "    time_mean = np.mean(response_times)\n",
    "    \n",
    "    if time_std > time_mean * 0.5:  # í‘œì¤€í¸ì°¨ê°€ í‰ê· ì˜ 50% ì´ìƒ\n",
    "        limitations[\"íš¨ìœ¨ì„± ë¬¸ì œ\"].append({\n",
    "            \"ë¬¸ì œ\": \"ì‘ë‹µ ì‹œê°„ í¸ì°¨ í¼\",\n",
    "            \"ì„¤ëª…\": f\"ì‘ë‹µì‹œê°„ í¸ì°¨: {time_std:.3f}ì´ˆ (í‰ê· : {time_mean:.3f}ì´ˆ)\",\n",
    "            \"ì›ì¸\": \"ì§ˆë¬¸ ë³µì¡ë„ì— ë”°ë¥¸ ì²˜ë¦¬ ì‹œê°„ ì°¨ì´, ìºì‹± ë¶€ì¬\"\n",
    "        })\n",
    "        improvements_needed.append(\"ì‘ë‹µ ìºì‹± ì‹œìŠ¤í…œ êµ¬ì¶•\")\n",
    "        improvements_needed.append(\"ë™ì  Kê°’ ì¡°ì •ìœ¼ë¡œ ì²˜ë¦¬ ì‹œê°„ ìµœì í™”\")\n",
    "    \n",
    "    # 3. ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„\n",
    "    context_lengths = [r['total_context_length'] for r in test_results]\n",
    "    long_context_count = sum(1 for length in context_lengths if length > 2000)\n",
    "    \n",
    "    if long_context_count > 0:\n",
    "        limitations[\"íš¨ìœ¨ì„± ë¬¸ì œ\"].append({\n",
    "            \"ë¬¸ì œ\": \"ê³¼ë„í•œ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´\",\n",
    "            \"ì„¤ëª…\": f\"{long_context_count}ê°œ ì§ˆë¬¸ì—ì„œ 2000ì ì´ìƒì˜ ì»¨í…ìŠ¤íŠ¸\",\n",
    "            \"ì›ì¸\": \"ê³ ì •ëœ Top-K ê²€ìƒ‰, ì¤‘ë³µ/ê´€ë ¨ì—†ëŠ” ì •ë³´ í¬í•¨\"\n",
    "        })\n",
    "        improvements_needed.append(\"Re-rankingìœ¼ë¡œ ê´€ë ¨ë„ ë†’ì€ ë¬¸ì„œë§Œ ì„ ë³„\")\n",
    "        improvements_needed.append(\"ë™ì  ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¡°ì •\")\n",
    "    \n",
    "    # 4. ë¬¸ì„œ ë‹¤ì–‘ì„± ë¶„ì„\n",
    "    unique_sources = [r['unique_sources'] for r in test_results]\n",
    "    low_diversity_count = sum(1 for sources in unique_sources if sources <= 1)\n",
    "    \n",
    "    if low_diversity_count > 0:\n",
    "        limitations[\"ê²€ìƒ‰ í’ˆì§ˆ ë¬¸ì œ\"].append({\n",
    "            \"ë¬¸ì œ\": \"ë‚®ì€ ì •ë³´ì› ë‹¤ì–‘ì„±\",\n",
    "            \"ì„¤ëª…\": f\"{low_diversity_count}ê°œ ì§ˆë¬¸ì—ì„œ ë‹¨ì¼ ë¬¸ì„œì—ë§Œ ì˜ì¡´\",\n",
    "            \"ì›ì¸\": \"ìœ ì‚¬í•œ ì²­í¬ë“¤ì´ ìƒìœ„ì— ë­í¬ë˜ëŠ” í˜„ìƒ, ë‹¤ì–‘ì„± ê³ ë ¤ ë¶€ì¡±\"\n",
    "        })\n",
    "        improvements_needed.append(\"Diversity Re-ranking ë„ì…\")\n",
    "        improvements_needed.append(\"MMR (Maximal Marginal Relevance) ì ìš©\")\n",
    "    \n",
    "    # 5. ì§ˆë¬¸ ìœ í˜•ë³„ ì„±ëŠ¥ í¸ì°¨ ë¶„ì„\n",
    "    type_coverages = {}\n",
    "    for result in test_results:\n",
    "        if result['keyword_coverage'] is not None:\n",
    "            q_type = result['type']\n",
    "            if q_type not in type_coverages:\n",
    "                type_coverages[q_type] = []\n",
    "            type_coverages[q_type].append(result['keyword_coverage'])\n",
    "    \n",
    "    type_avg_coverage = {k: np.mean(v) for k, v in type_coverages.items()}\n",
    "    worst_type = min(type_avg_coverage, key=type_avg_coverage.get)\n",
    "    best_type = max(type_avg_coverage, key=type_avg_coverage.get)\n",
    "    \n",
    "    if type_avg_coverage[best_type] - type_avg_coverage[worst_type] > 0.3:  # 30% ì´ìƒ ì°¨ì´\n",
    "        limitations[\"ê²€ìƒ‰ í’ˆì§ˆ ë¬¸ì œ\"].append({\n",
    "            \"ë¬¸ì œ\": \"ì§ˆë¬¸ ìœ í˜•ë³„ ì„±ëŠ¥ í¸ì°¨\",\n",
    "            \"ì„¤ëª…\": f\"{worst_type}({type_avg_coverage[worst_type]:.1%}) vs {best_type}({type_avg_coverage[best_type]:.1%})\",\n",
    "            \"ì›ì¸\": \"íšì¼ì  ê²€ìƒ‰ ì „ëµ, ì§ˆë¬¸ ìœ í˜• íŠ¹ì„± ë¯¸ê³ ë ¤\"\n",
    "        })\n",
    "        improvements_needed.append(\"ì§ˆë¬¸ ìœ í˜•ë³„ ë¼ìš°íŒ… ì‹œìŠ¤í…œ\")\n",
    "        improvements_needed.append(\"Modular RAG ì•„í‚¤í…ì²˜ ë„ì…\")\n",
    "    \n",
    "    # 6. í™•ì¥ì„± ë¬¸ì œ (êµ¬ì¡°ì  í•œê³„)\n",
    "    limitations[\"í™•ì¥ì„± ë¬¸ì œ\"].extend([\n",
    "        {\n",
    "            \"ë¬¸ì œ\": \"ë©”íƒ€ë°ì´í„° í™œìš© ë¶€ì¡±\",\n",
    "            \"ì„¤ëª…\": \"ë¬¸ì„œì˜ ì—°ë„, ì¹´í…Œê³ ë¦¬, ì‹ ë¢°ë„ ë“± ë©”íƒ€ì •ë³´ë¥¼ ê²€ìƒ‰ì— í™œìš©í•˜ì§€ ì•ŠìŒ\",\n",
    "            \"ì›ì¸\": \"ìˆœìˆ˜ ë²¡í„° ìœ ì‚¬ë„ì—ë§Œ ì˜ì¡´í•˜ëŠ” ë‹¨ìˆœí•œ êµ¬ì¡°\"\n",
    "        },\n",
    "        {\n",
    "            \"ë¬¸ì œ\": \"ì‹¤ì‹œê°„ ì •ë³´ ì—…ë°ì´íŠ¸ ì–´ë ¤ì›€\",\n",
    "            \"ì„¤ëª…\": \"ìƒˆë¡œìš´ ë¬¸ì„œ ì¶”ê°€ ì‹œ ì „ì²´ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• í•„ìš”\",\n",
    "            \"ì›ì¸\": \"ì •ì  ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¡°\"\n",
    "        },\n",
    "        {\n",
    "            \"ë¬¸ì œ\": \"ë³µí•© ì§ˆë¬¸ ì²˜ë¦¬ í•œê³„\",\n",
    "            \"ì„¤ëª…\": \"ì—¬ëŸ¬ ì£¼ì œê°€ ì„ì¸ ì§ˆë¬¸ì´ë‚˜ ì¶”ë¡ ì´ í•„ìš”í•œ ì§ˆë¬¸ì— ì•½í•¨\",\n",
    "            \"ì›ì¸\": \"ë‹¨ìˆœí•œ ê²€ìƒ‰-ìƒì„± íŒŒì´í”„ë¼ì¸\"\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    # ê°œì„  ì œì•ˆ ìš°ì„ ìˆœìœ„ ì •ë¦¬\n",
    "    improvements_needed.extend([\n",
    "        \"Self-Query Retrieverë¡œ ë©”íƒ€ë°ì´í„° í•„í„°ë§\",\n",
    "        \"Query Decompositionìœ¼ë¡œ ë³µí•© ì§ˆë¬¸ ì²˜ë¦¬\",\n",
    "        \"ì‹¤ì‹œê°„ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ\"\n",
    "    ])\n",
    "    \n",
    "    analysis_result = {\n",
    "        \"limitations\": limitations,\n",
    "        \"improvements_needed\": list(set(improvements_needed)),  # ì¤‘ë³µ ì œê±°\n",
    "        \"next_steps\": [\n",
    "            \"02_naive_failure_analysis.ipynb: ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ìƒì„¸ ë¶„ì„\",\n",
    "            \"03_advanced_query_refinement.ipynb: Query Refinement ê¸°ë²• ì ìš©\",\n",
    "            \"04_metadata_filtering.ipynb: ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§\",\n",
    "            \"05_hybrid_search_rerank.ipynb: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë° ë¦¬ë­í‚¹\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return analysis_result\n",
    "\n",
    "# í•œê³„ì  ë¶„ì„ ì‹¤í–‰\n",
    "limitation_analysis = analyze_naive_rag_limitations(test_results)\n",
    "\n",
    "print(\"\\nğŸ“‹ Naive RAG í•œê³„ì  ë¶„ì„ ê²°ê³¼:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for category, issues in limitation_analysis[\"limitations\"].items():\n",
    "    if issues:  # ë¬¸ì œê°€ ìˆëŠ” ì¹´í…Œê³ ë¦¬ë§Œ ì¶œë ¥\n",
    "        print(f\"\\nğŸ”´ {category}:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  â€¢ {issue['ë¬¸ì œ']}: {issue['ì„¤ëª…']}\")\n",
    "            print(f\"    ì›ì¸: {issue['ì›ì¸']}\")\n",
    "\n",
    "print(f\"\\nğŸ› ï¸ ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­:\")\n",
    "for i, improvement in enumerate(limitation_analysis[\"improvements_needed\"], 1):\n",
    "    print(f\"  {i}. {improvement}\")\n",
    "\n",
    "print(f\"\\nğŸš€ ë‹¤ìŒ ì‹¤ìŠµ ë‹¨ê³„:\")\n",
    "for step in limitation_analysis[\"next_steps\"]:\n",
    "    print(f\"  ğŸ“ {step}\")\n",
    "\n",
    "# ë¶„ì„ ê²°ê³¼ ì €ì¥\n",
    "with open(\"naive_rag_limitations_analysis.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(limitation_analysis, f, ensure_ascii=False, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nğŸ’¾ í•œê³„ì  ë¶„ì„ ê²°ê³¼ ì €ì¥: naive_rag_limitations_analysis.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ğŸ“‹ Naive RAG ë² ì´ìŠ¤ë¼ì¸ ìš”ì•½\n",
    "\n",
    "### âœ… ì™„ë£Œëœ ì‘ì—…\n",
    "1. **í™˜ê²½ ì„¤ì •**: FAISS, LangChain, ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„\n",
    "2. **ë°ì´í„° ì¤€ë¹„**: ë‹¤ì–‘í•œ ë„ë©”ì¸ì˜ í•œêµ­ì–´ ë¬¸ì„œ 7ê°œ\n",
    "3. **ë¬¸ì„œ ì „ì²˜ë¦¬**: 400ì ì²­í¬, 50ì ì˜¤ë²„ë©ìœ¼ë¡œ ë¶„í• \n",
    "4. **ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•**: all-MiniLM-L6-v2 ì„ë² ë”©ìœ¼ë¡œ FAISS ì¸ë±ìŠ¤ ìƒì„±\n",
    "5. **RAG íŒŒì´í”„ë¼ì¸**: ê¸°ë³¸ ê²€ìƒ‰-ìƒì„± ì²´ì¸ êµ¬í˜„\n",
    "6. **ì„±ëŠ¥ ì¸¡ì •**: 6ê°€ì§€ ì§ˆë¬¸ ìœ í˜•ìœ¼ë¡œ ì¢…í•© í‰ê°€\n",
    "7. **ê²°ê³¼ ë¶„ì„**: ê°•ì /ì•½ì  íŒŒì•… ë° ê°œì„  ë°©í–¥ ë„ì¶œ\n",
    "\n",
    "### ğŸ“Š ì¸¡ì •ëœ ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥\n",
    "- **í‰ê·  ì‘ë‹µì‹œê°„**: ê²€ìƒ‰+ìƒì„± í†µí•© ì‹œê°„ ì¸¡ì •\n",
    "- **í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€**: ë‹µë³€ì˜ ì •í™•ë„ ëŒ€ë¦¬ ì§€í‘œ\n",
    "- **ê²€ìƒ‰ í’ˆì§ˆ**: ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´, ë¬¸ì„œ ë‹¤ì–‘ì„±\n",
    "- **ì§ˆë¬¸ ìœ í˜•ë³„ ì„±ëŠ¥**: ì‚¬ì‹¤í˜•, ì„¤ëª…í˜•, ë¹„êµí˜• ë“±\n",
    "\n",
    "### ğŸ” ë°œê²¬ëœ ì£¼ìš” í•œê³„ì \n",
    "- **ê²€ìƒ‰ í’ˆì§ˆ**: ë™ì˜ì–´ ì²˜ë¦¬ ë¶€ì¡±, ë¬¸ì„œ ë‹¤ì–‘ì„± ì œí•œ\n",
    "- **ì‘ë‹µ í’ˆì§ˆ**: í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€ í¸ì°¨, ì§ˆë¬¸ ìœ í˜•ë³„ ì„±ëŠ¥ ì°¨ì´\n",
    "- **íš¨ìœ¨ì„±**: ì‘ë‹µ ì‹œê°„ ì¼ê´€ì„± ë¶€ì¡±, ê³¼ë„í•œ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´\n",
    "- **í™•ì¥ì„±**: ë©”íƒ€ë°ì´í„° ë¯¸í™œìš©, ë³µí•© ì§ˆë¬¸ ì²˜ë¦¬ í•œê³„\n",
    "\n",
    "### ğŸš€ ë‹¤ìŒ ë‹¨ê³„ Preview\n",
    "ë‹¤ìŒ ì‹¤ìŠµì—ì„œëŠ” ì´ëŸ¬í•œ í•œê³„ì ë“¤ì„ í•˜ë‚˜ì”© í•´ê²°í•´ë‚˜ê°‘ë‹ˆë‹¤:\n",
    "- **ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë¶„ì„**: êµ¬ì²´ì ì¸ ë¬¸ì œ ìƒí™© ì¬í˜„\n",
    "- **Advanced RAG**: Query Refinement, Hybrid Search, Re-ranking\n",
    "- **Modular RAG**: ì§€ëŠ¥í˜• ë¼ìš°íŒ…, ì¡°ê±´ë¶€ ì²´ì´ë‹\n",
    "\n",
    "### ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸\n",
    "Naive RAGëŠ” **ë¹ ë¥¸ êµ¬í˜„ê³¼ ëª…í™•í•œ êµ¬ì¡°**ë¼ëŠ” ì¥ì ì´ ìˆì§€ë§Œ, \n",
    "**ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½**ì—ì„œëŠ” ë‹¤ì–‘í•œ ê°œì„  ê¸°ë²•ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ë² ì´ìŠ¤ë¼ì¸ì´ í–¥í›„ ëª¨ë“  ê°œì„  ê¸°ë²•ì˜ **ì„±ëŠ¥ ë¹„êµ ê¸°ì¤€**ì´ ë©ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ìš”ì•½ ë° ë§ˆë¬´ë¦¬\n",
    "print(\"ğŸ‰ Day 2 ì‹¤ìŠµ 1: Naive RAG ë² ì´ìŠ¤ë¼ì¸ êµ¬ì¶• ì™„ë£Œ!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ìƒì„±ëœ íŒŒì¼ë“¤ ìš”ì•½\n",
    "generated_files = [\n",
    "    \"naive_rag_vectorstore/ (FAISS ë²¡í„°ìŠ¤í† ì–´)\",\n",
    "    \"naive_rag_baseline.log (ì„±ëŠ¥ ì¸¡ì • ë¡œê·¸)\",\n",
    "    \"naive_rag_performance_dashboard.png (ì„±ëŠ¥ ë¶„ì„ ì°¨íŠ¸)\",\n",
    "    \"naive_rag_limitations_analysis.json (í•œê³„ì  ë¶„ì„ ê²°ê³¼)\"\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:\")\n",
    "for file in generated_files:\n",
    "    print(f\"  ğŸ“„ {file}\")\n",
    "\n",
    "# ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ ìš”ì•½ ì¶œë ¥\n",
    "if performance_summary:\n",
    "    print(f\"\\nğŸ“Š Naive RAG ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥:\")\n",
    "    print(f\"  â±ï¸ í‰ê·  ì‘ë‹µì‹œê°„: {performance_summary['avg_response_time']:.3f}ì´ˆ\")\n",
    "    print(f\"  ğŸ“ í‰ê·  ì»¨í…ìŠ¤íŠ¸: {performance_summary['avg_context_length']:.0f}ì\")\n",
    "    if performance_summary['avg_keyword_coverage']:\n",
    "        print(f\"  ğŸ¯ í‰ê·  í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€: {performance_summary['avg_keyword_coverage']:.1%}\")\n",
    "    print(f\"  ğŸ“ ì´ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {performance_summary['total_queries']}ê°œ\")\n",
    "\n",
    "print(f\"\\nğŸ”„ ë‹¤ìŒ ì‹¤ìŠµ:\")\n",
    "print(f\"  ğŸ“ 02_naive_failure_analysis.ipynb\")\n",
    "print(f\"     â†’ Naive RAGê°€ ì‹¤íŒ¨í•˜ëŠ” êµ¬ì²´ì ì¸ ì¼€ì´ìŠ¤ë“¤ì„ ë¶„ì„í•˜ê³ \")\n",
    "print(f\"     â†’ ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ ë² ì´ìŠ¤ë¼ì¸ í™•ë¦½ ì™„ë£Œ!\")\n",
    "print(f\"ğŸš€ ì´ì œ Advanced RAG ê¸°ë²•ë“¤ë¡œ ë‹¨ê³„ì  ê°œì„ ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"ğŸ¯ Naive RAG â†’ Advanced RAG â†’ Modular RAG ì—¬ì •ì˜ ì²« ê±¸ìŒ ì™„ì„±! ğŸ¯\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
