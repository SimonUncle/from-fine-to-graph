{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Modular RAG - ê¸°ëŠ¥ ë§›ë³´ê¸°!\n",
    "\n",
    "## ğŸ¯ í•™ìŠµ ëª©í‘œ\n",
    "- **Query Routing**: LLMì´ ì§ˆë¬¸ ìœ í˜•ì„ íŒë³„í•˜ì—¬ ìë™ ë¶„ë¥˜\n",
    "- **Self-RAG Critic**: LLMì´ ìì‹ ì˜ ë‹µë³€ì„ í‰ê°€í•˜ê³  ê°œì„ ì  ì°¾ê¸°\n",
    "- **Retry with Refine**: ë¶€ì¡±í•œ ë¶€ë¶„ì„ ë³´ê°•í•´ì„œ ì¬ì‹œë„í•˜ëŠ” ì§€ëŠ¥í˜• ì‹œìŠ¤í…œ\n",
    "\n",
    "## ğŸ“‹ ì‹¤ìŠµ êµ¬ì„±\n",
    "1. **Query Routing ë§›ë³´ê¸°** (10ë¶„) - ì§ˆë¬¸ì„ single/multi/clarifyë¡œ ë¶„ë¥˜\n",
    "2. **Self-RAG Critic ë§›ë³´ê¸°** (10ë¶„) - ë‹µë³€ì„ ìŠ¤ìŠ¤ë¡œ í‰ê°€í•˜ê³  ê°œì„ ì  ì°¾ê¸°  \n",
    "3. **Retry with Refine ë§›ë³´ê¸°** (10ë¶„) - ëˆ„ë½ëœ ë¶€ë¶„ ë³´ê°•í•´ì„œ ì¬ì‹œë„\n",
    "\n",
    "---\n",
    "\n",
    "> ğŸ’¡ **í•µì‹¬ ì•„ì´ë””ì–´**: LLMì´ ìŠ¤ìŠ¤ë¡œ ë¶„ë¥˜â†’í‰ê°€â†’ë³´ê°•í•˜ëŠ” **ë©”íƒ€ì¸ì§€ ëŠ¥ë ¥**ì„ ì²´í—˜í•´ë³´ì„¸ìš”! 05ë²ˆ í™˜ê²½ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ì„œ ê°„ë‹¨í•œ ê¸°ëŠ¥ë“¤ì„ ë…ë¦½ì ìœ¼ë¡œ ë§›ë´…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05ë²ˆ í™˜ê²½ ì¬ì‚¬ìš© (Day1FinetunedLLM + í•­ê³µ ì½”í¼ìŠ¤)\n",
    "!pip install -q langchain-community faiss-cpu sentence-transformers matplotlib pandas numpy transformers torch rank-bm25\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ğŸ¤– Day1 íŒŒì¸íŠœë‹ ëª¨ë¸ ê´€ë ¨ import\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import Field\n",
    "\n",
    "# LangChain ê´€ë ¨ import\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì • ì™„ë£Œ!\")\n",
    "\n",
    "# Day 1 íŒŒì¸íŠœë‹ ëª¨ë¸ í´ë˜ìŠ¤ (05ë²ˆê³¼ ë™ì¼)\n",
    "class Day1FinetunedLLM(LLM):\n",
    "    \"\"\"Day 1 íŒŒì¸íŠœë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” LLM í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    # Pydantic í•„ë“œ ì„ ì–¸\n",
    "    model_name: str = Field(default=\"ryanu/my-exaone-raft-model\")\n",
    "    tokenizer: Any = Field(default=None)\n",
    "    model: Any = Field(default=None)\n",
    "    \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "    \n",
    "    def __init__(self, model_name: str = \"ryanu/my-exaone-raft-model\", **kwargs):\n",
    "        super().__init__(model_name=model_name, **kwargs)\n",
    "        print(f\"ğŸ¯ Day 1 íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ: {self.model_name}\")\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"day1_finetuned_llm\"\n",
    "    \n",
    "    def _call(self, prompt: str, stop=None, run_manager=None, **kwargs) -> str:\n",
    "        \"\"\"ì‹¤ì œ ëª¨ë¸ ì¶”ë¡ \"\"\"\n",
    "        # EXAONE í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©\n",
    "        formatted_prompt = f\"[|system|]ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.[|endofturn|]\\n[|user|]{prompt}[|endofturn|]\\n[|assistant|]\"\n",
    "        \n",
    "        inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs, max_new_tokens=512, temperature=0.7, do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id, eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = self.tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
    "        return response if response else \"ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "# LLM ì´ˆê¸°í™”\n",
    "llm = Day1FinetunedLLM()\n",
    "\n",
    "print(\"âœ… Day1FinetunedLLM ì´ˆê¸°í™” ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05ë²ˆ í•­ê³µ ì½”í¼ìŠ¤ ì¬ì‚¬ìš© + ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•\n",
    "def create_aviation_corpus():\n",
    "    \"\"\"05ë²ˆê³¼ ë™ì¼í•œ í•­ê³µì‚¬ + ê³µí•­ ì½”í¼ìŠ¤\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"content\": \"\"\"ì—ì–´ë¯¼íŠ¸ í•­ê³µ ìˆ˜í•˜ë¬¼ ì •ì±… (2024-05-10)\n",
    "- ìœ„íƒìˆ˜í•˜ë¬¼: 15kg ë¬´ë£Œ, ì´ˆê³¼ 1kgë‹¹ 8,000ì›\n",
    "- ê¸°ë‚´ìˆ˜í•˜ë¬¼: 7kg 1ê°œ\n",
    "- ë…¸ì„ : êµ­ë‚´ì„  ë™ì¼ ê¸°ì¤€\"\"\",\n",
    "            \"metadata\": {\"type\": \"airline_policy\", \"airline\": \"ì—ì–´ë¯¼íŠ¸\", \"source\": \"official\", \"updated_at\": \"2024-05-10\"}\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"\"\"ê¹€í¬ê³µí•­ ë³´ì•ˆê²€ìƒ‰ í˜¼ì¡ë„ ì•ˆë‚´ (2024-04-15)\n",
    "- ì¼ìš”ì¼: 08~10ì‹œ í˜¼ì¡, 12~14ì‹œëŠ” ë¹„êµì  í•œì‚°\n",
    "- í‰ì¼: ì¶œê·¼ì‹œê°„ëŒ€ 07~09ì‹œ í˜¼ì¡\n",
    "- ì˜¤í›„ ì‹œê°„ëŒ€ëŠ” ì „ë°˜ì ìœ¼ë¡œ ì—¬ìœ ë¡œì›€\"\"\",\n",
    "            \"metadata\": {\"type\": \"airport_info\", \"airport\": \"ê¹€í¬\", \"source\": \"official\", \"updated_at\": \"2024-04-15\"}\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"\"\"ì¸ì²œê³µí•­ ë³´ì•ˆê²€ìƒ‰ í˜¼ì¡ë„ (2024-04-20)\n",
    "- ì£¼ë§: 09~11ì‹œ í”¼í¬, 15~17ì‹œ í•œì‚°\n",
    "- êµ­ì œì„  í„°ë¯¸ë„ì€ ë³„ë„ ì‹œê°„ëŒ€ ì ìš©\"\"\",\n",
    "            \"metadata\": {\"type\": \"airport_info\", \"airport\": \"ì¸ì²œ\", \"source\": \"official\", \"updated_at\": \"2024-04-20\"}\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"\"\"ìŠ¤ì¹´ì´ë¸”ë£¨ í•­ê³µ ìˆ˜í•˜ë¬¼ ì •ì±… (2024-03-01)\n",
    "- ìœ„íƒìˆ˜í•˜ë¬¼: 20kg ë¬´ë£Œ\n",
    "- ê¸°ë‚´ìˆ˜í•˜ë¬¼: 10kg 1ê°œ\"\"\",\n",
    "            \"metadata\": {\"type\": \"airline_policy\", \"airline\": \"ìŠ¤ì¹´ì´ë¸”ë£¨\", \"source\": \"official\", \"updated_at\": \"2024-03-01\"}\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"\"\"ê³µí•­ ë¹ ë¥´ê²Œ í†µê³¼í•˜ëŠ” ì—¬í–‰ íŒ\n",
    "- ìˆ˜í•˜ë¬¼ ê°€ë³ê²Œ ì¤€ë¹„í•˜ê¸°\n",
    "- ì´ë¥¸ ì‹œê°„ ë„ì°© ì¶”ì²œ\n",
    "- ì£¼ë§ ì˜¤ì „ í”¼í¬íƒ€ì„ì€ í”¼í•˜ê¸°\"\"\",\n",
    "            \"metadata\": {\"type\": \"blog\", \"source\": \"blog\", \"updated_at\": \"2024-08-01\"}\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•\n",
    "aviation_docs = create_aviation_corpus()\n",
    "lc_documents = [Document(page_content=doc[\"content\"], metadata=doc[\"metadata\"]) for doc in aviation_docs]\n",
    "\n",
    "# ì„ë² ë”© + ë²¡í„°ìŠ¤í† ì–´ (05ë²ˆê³¼ ë™ì¼)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "vectorstore = FAISS.from_documents(lc_documents, embeddings)\n",
    "\n",
    "# ê°„ë‹¨í•œ RAG ì²´ì¸ìš© ë¦¬íŠ¸ë¦¬ë²„\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "print(f\"âœ… í•­ê³µ ì½”í¼ìŠ¤ ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ! ({len(aviation_docs)}ê°œ ë¬¸ì„œ)\")\n",
    "print(\"\\nğŸ“‹ ë¬¸ì„œ ëª©ë¡:\")\n",
    "for i, doc in enumerate(aviation_docs, 1):\n",
    "    doc_type = doc[\"metadata\"].get(\"type\", \"unknown\")\n",
    "    entity = doc[\"metadata\"].get(\"airline\") or doc[\"metadata\"].get(\"airport\", \"ì¼ë°˜\")\n",
    "    first_line = doc[\"content\"].strip().split('\\n')[0]\n",
    "    print(f\"  {i}. [{doc_type}|{entity}] {first_line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ Step 1: Query Routing ë§›ë³´ê¸° - í•˜ë“œì½”ë”© vs LLM\n",
    "\n",
    "# ğŸ“‹ 1. í•˜ë“œì½”ë”© ë¼ìš°íŒ… (ê·œì¹™ ê¸°ë°˜)\n",
    "def hardcoded_routing(query):\n",
    "    \"\"\"í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨í•œ ë¼ìš°íŒ…\"\"\"\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    if \"ìˆ˜í•˜ë¬¼\" in query_lower or \"ì§\" in query_lower:\n",
    "        return \"baggage_fee_search\", \"ìˆ˜í•˜ë¬¼ ìš”ê¸ˆ ê²€ìƒ‰ í•¨ìˆ˜\"\n",
    "    elif \"ë³´ì•ˆê²€ìƒ‰\" in query_lower or \"ê²€ìƒ‰ëŒ€\" in query_lower:\n",
    "        return \"security_time_search\", \"ë³´ì•ˆê²€ìƒ‰ ì‹œê°„ ê²€ìƒ‰ í•¨ìˆ˜\"  \n",
    "    elif \"ê³„ì‚°\" in query_lower or \"ë”í•˜ê¸°\" in query_lower or \"ë¹¼ê¸°\" in query_lower:\n",
    "        return \"calculator\", \"ê³„ì‚°ê¸° í•¨ìˆ˜\"\n",
    "    elif \"ë‚ ì”¨\" in query_lower:\n",
    "        return \"weather_search\", \"ë‚ ì”¨ ê²€ìƒ‰ í•¨ìˆ˜\"\n",
    "    else:\n",
    "        return \"general_rag_search\", \"ì¼ë°˜ RAG ê²€ìƒ‰ í•¨ìˆ˜\"\n",
    "\n",
    "# ğŸ¤– 2. LLM ë¼ìš°íŒ… (ì˜ë¯¸ ê¸°ë°˜)\n",
    "def llm_routing(llm, query):\n",
    "    \"\"\"LLMì´ ì–´ë–¤ í•¨ìˆ˜ë¡œ ë³´ë‚¼ì§€ ê²°ì •\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"ë‹¤ìŒ ì§ˆë¬¸ì„ ë³´ê³  ì–´ë–¤ í•¨ìˆ˜ë¡œ ë³´ë‚´ì•¼ í• ì§€ ê²°ì •í•˜ì„¸ìš”.\n",
    "\n",
    "ê°€ëŠ¥í•œ í•¨ìˆ˜ë“¤:\n",
    "- baggage_fee_search: ìˆ˜í•˜ë¬¼ ìš”ê¸ˆ ê´€ë ¨\n",
    "- security_time_search: ê³µí•­ ë³´ì•ˆê²€ìƒ‰ ì‹œê°„ ê´€ë ¨  \n",
    "- calculator: ê³„ì‚° ê´€ë ¨\n",
    "- weather_search: ë‚ ì”¨ ê´€ë ¨\n",
    "- general_rag_search: ì¼ë°˜ì ì¸ ì •ë³´ ê²€ìƒ‰\n",
    "\n",
    "ì§ˆë¬¸: {query}\n",
    "\n",
    "ë‹µë³€ í˜•ì‹: í•¨ìˆ˜ëª…|ì´ìœ \n",
    "ì˜ˆ: baggage_fee_search|ìˆ˜í•˜ë¬¼ ìš”ê¸ˆì„ ë¬»ê³  ìˆìŒ\"\"\"\n",
    "\n",
    "    response = llm._call(prompt)\n",
    "    \n",
    "    # ê°„ë‹¨í•˜ê²Œ íŒŒì‹± (|ë¡œ ë¶„ë¦¬)\n",
    "    if \"|\" in response:\n",
    "        func_name, reason = response.split(\"|\", 1)\n",
    "        return func_name.strip(), reason.strip()\n",
    "    else:\n",
    "        return \"general_rag_search\", \"LLM íŒŒì‹± ì‹¤íŒ¨\"\n",
    "\n",
    "# ğŸ” í•˜ë“œì½”ë”© vs LLM ë¼ìš°íŒ… ë¹„êµ í…ŒìŠ¤íŠ¸\n",
    "print(\"ğŸ¯ Query Routing ë§›ë³´ê¸°: í•˜ë“œì½”ë”© vs LLM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_queries = [\n",
    "    \"ì—ì–´ë¯¼íŠ¸ ìˆ˜í•˜ë¬¼ ìš”ê¸ˆ ì•Œë ¤ì¤˜\",\n",
    "    \"ê¹€í¬ê³µí•­ ë³´ì•ˆê²€ìƒ‰ ì–¸ì œê°€ ë¹¨ë¼?\", \n",
    "    \"2+3ì€ ì–¼ë§ˆì•¼?\",\n",
    "    \"ë‚´ì¼ ì„œìš¸ ë‚ ì”¨ ì–´ë•Œ?\",\n",
    "    \"í•­ê³µì‚¬ ì •ì±…ì— ëŒ€í•´ ì•Œê³  ì‹¶ì–´\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ’¡ ê°™ì€ ì§ˆë¬¸, ë‹¤ë¥¸ ë¼ìš°íŒ… ë°©ì‹!\")\n",
    "print()\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"{i}. ì§ˆë¬¸: '{query}'\")\n",
    "    \n",
    "    # í•˜ë“œì½”ë”© ë¼ìš°íŒ…\n",
    "    hard_func, hard_reason = hardcoded_routing(query)\n",
    "    print(f\"   ğŸ”§ í•˜ë“œì½”ë”©: {hard_func} ({hard_reason})\")\n",
    "    \n",
    "    # LLM ë¼ìš°íŒ…  \n",
    "    llm_func, llm_reason = llm_routing(llm, query)\n",
    "    print(f\"   ğŸ¤– LLM: {llm_func} ({llm_reason})\")\n",
    "    \n",
    "    # ê²°ê³¼ ë¹„êµ\n",
    "    if hard_func == llm_func:\n",
    "        print(\"   âœ… ê²°ê³¼ ì¼ì¹˜!\")\n",
    "    else:\n",
    "        print(\"   ğŸ”„ ê²°ê³¼ ë‹¤ë¦„ (LLMì´ ë” ìœ ì—°í•œ ì´í•´)\")\n",
    "    print()\n",
    "\n",
    "print(\"ğŸš€ í•˜ë“œì½”ë”© vs LLM ë¼ìš°íŒ… ë¹„êµ:\")\n",
    "print(\"â€¢ í•˜ë“œì½”ë”©: ë¹ ë¦„, ëª…í™•í•¨, í•˜ì§€ë§Œ ì œí•œì \")\n",
    "print(\"â€¢ LLM: ëŠë¦¼, ìœ ì—°í•¨, ë³µì¡í•œ ìƒí™© ëŒ€ì‘\")\n",
    "print(\"â€¢ ì‹¤ë¬´: ëª…í™•í•œ ì¼€ì´ìŠ¤ëŠ” í•˜ë“œì½”ë”©, ì• ë§¤í•œ ì¼€ì´ìŠ¤ëŠ” LLM!\")\n",
    "\n",
    "print(\"\\nğŸ’¡ í•µì‹¬: 'ë¼ìš°íŒ… = ì ì ˆí•œ í•¨ìˆ˜ ì„ íƒ'\")\n",
    "print(\"â€¢ ìˆ˜í•˜ë¬¼ ë¬¸ì˜ â†’ baggage_fee_search() í˜¸ì¶œ\")\n",
    "print(\"â€¢ ê³„ì‚° ìš”ì²­ â†’ calculator() í˜¸ì¶œ\")\n",
    "print(\"â€¢ ëª©ì ì§€ í•¨ìˆ˜ë§Œ ì •í™•íˆ ì°¾ìœ¼ë©´ ì„±ê³µ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ Step 2: Self-RAG Critic ë§›ë³´ê¸°\n",
    "\n",
    "# (A) ê¸°ë³¸ RAGë¡œ ë‹µë³€ ìƒì„± (Step 1 ë¼ìš°íŒ… ê²°ê³¼ í™œìš© ê°€ëŠ¥)\n",
    "STRICT_PROMPT = PromptTemplate.from_template(\n",
    "\"\"\"ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ë‚´ìš©ë§Œ ê·¼ê±°ë¡œ í•œêµ­ì–´ë¡œ ê°„ê²°íˆ ë‹µí•˜ì„¸ìš”.\n",
    "ì—†ìœ¼ë©´ 'ê·¼ê±° ë¶ˆì¶©ë¶„: (ë¬´ì—‡ì´ í•„ìš”í•œì§€)' í•œ ì¤„ë¡œ ë§í•˜ê³ , í•„ìš”í•œ ì¶”ê°€ì •ë³´ 1~2ê°œë§Œ ë¬¼ì–´ë³´ì„¸ìš”.\n",
    "\n",
    "[ì»¨í…ìŠ¤íŠ¸]\n",
    "{context}\n",
    "\n",
    "[ì§ˆë¬¸]\n",
    "{question}\n",
    "\n",
    "[ë‹µë³€]\"\"\")\n",
    "\n",
    "def answer_with_context(llm, docs, question):\n",
    "    \"\"\"ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ RAG ë‹µë³€\"\"\"\n",
    "    context = \"\\n\\n---\\n\\n\".join([d.page_content[:1000] for d in docs]) or \"(ë¹ˆ ì»¨í…ìŠ¤íŠ¸)\"\n",
    "    prompt_text = STRICT_PROMPT.format(context=context, question=question)\n",
    "    response = llm._call(prompt_text)\n",
    "    return response.strip()\n",
    "\n",
    "# (B) Self-RAG Critic: LLMì´ ë‹µë³€ì„ ìê°€í‰ê°€\n",
    "CRITIC_PROMPT = PromptTemplate.from_template(\n",
    "\"\"\"ë‹¹ì‹ ì€ í’ˆì§ˆ ì‹¬ì‚¬ê´€ì…ë‹ˆë‹¤. ì•„ë˜ë¥¼ ë³´ê³  JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n",
    "\n",
    "- coverage: 0.0~1.0 (ì§ˆë¬¸ ìš”êµ¬ íŒŒì…‹ì„ ì–¼ë§ˆë‚˜ ê·¼ê±°ë¡œ ì»¤ë²„?)\n",
    "- missing_facets: ëˆ„ë½ëœ ì¶• ëª©ë¡\n",
    "- hallucination_risk: \"low\"|\"medium\"|\"high\"\n",
    "- ask: ì‚¬ìš©ìì—ê²Œ ë¬¼ì–´ë³¼ 1~2ê°œ ì§ˆë¬¸\n",
    "\n",
    "[ì§ˆë¬¸]\n",
    "{question}\n",
    "\n",
    "[ë‹µë³€]\n",
    "{answer}\n",
    "\n",
    "[ì»¨í…ìŠ¤íŠ¸ ìš”ì•½]\n",
    "{ctx}\n",
    "\n",
    "JSON:\"\"\")\n",
    "\n",
    "def self_rag_critic(llm, question, answer, docs):\n",
    "    \"\"\"LLMì´ Self-RAGë¡œ ë‹µë³€ í’ˆì§ˆ í‰ê°€\"\"\"\n",
    "    ctx = \"\\n---\\n\".join([d.page_content[:400] for d in docs[:3]])\n",
    "    prompt_text = CRITIC_PROMPT.format(question=question, answer=answer, ctx=ctx)\n",
    "    response = llm._call(prompt_text)\n",
    "    return _safe_json(response, {\n",
    "        \"coverage\": 0.0, \n",
    "        \"missing_facets\": [], \n",
    "        \"hallucination_risk\": \"medium\", \n",
    "        \"ask\": []\n",
    "    })\n",
    "\n",
    "# ğŸ” Self-RAG Critic í…ŒìŠ¤íŠ¸  \n",
    "print(\"ğŸ§  Self-RAG Critic ë§›ë³´ê¸°\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "test_query = \"ê¹€í¬ê³µí•­ì—ì„œ ì—ì–´ë¯¼íŠ¸ íƒ€ê³  ìˆ˜í•˜ë¬¼ ìš”ê¸ˆì´ë‘ ë³´ì•ˆê²€ìƒ‰ ì‹œê°„ë„ ì•Œë ¤ì¤˜\"\n",
    "print(f\"ì§ˆë¬¸: '{test_query}'\\n\")\n",
    "\n",
    "# Step 1ì—ì„œ ë¼ìš°íŒ…í•œ ê²°ê³¼ë„ í™œìš© ê°€ëŠ¥\n",
    "routing_result = llm_route_query(llm, test_query)\n",
    "print(f\"ğŸ“ Step 1 ë¼ìš°íŒ… ê²°ê³¼: {routing_result['route']} (í•µì‹¬ì¶•: {routing_result['facets']})\")\n",
    "\n",
    "# 1ì°¨: ê¸°ë³¸ RAG ë‹µë³€\n",
    "docs = retriever.get_relevant_documents(test_query)\n",
    "print(f\"\\nğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ:\")\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    doc_type = doc.metadata.get(\"type\", \"unknown\")\n",
    "    entity = doc.metadata.get(\"airline\") or doc.metadata.get(\"airport\", \"-\")\n",
    "    print(f\"  {i}. [{doc_type}|{entity}] {doc.page_content[:50]}...\")\n",
    "\n",
    "answer = answer_with_context(llm, docs, test_query)\n",
    "print(f\"\\nğŸ’¬ RAG ë‹µë³€:\\n{answer}\")\n",
    "\n",
    "# 2ì°¨: Self-RAG í‰ê°€\n",
    "critic_result = self_rag_critic(llm, test_query, answer, docs)\n",
    "\n",
    "coverage = critic_result.get(\"coverage\", 0.0)\n",
    "missing_facets = critic_result.get(\"missing_facets\", [])\n",
    "hallucination_risk = critic_result.get(\"hallucination_risk\", \"medium\")\n",
    "ask_questions = critic_result.get(\"ask\", [])\n",
    "\n",
    "print(f\"\\nğŸ¤” LLM Self-RAG í‰ê°€:\")\n",
    "print(f\"   ì»¤ë²„ë¦¬ì§€: {coverage:.1f} (0.0~1.0)\")\n",
    "print(f\"   ëˆ„ë½ëœ ì¶•: {missing_facets}\")\n",
    "print(f\"   í• ë£¨ì‹œë„¤ì´ì…˜ ìœ„í—˜: {hallucination_risk}\")\n",
    "print(f\"   ì¶”ê°€ ì§ˆë¬¸: {ask_questions}\")\n",
    "\n",
    "# í’ˆì§ˆ í•´ì„\n",
    "if coverage >= 0.8 and hallucination_risk == \"low\":\n",
    "    quality = \"âœ… ìš°ìˆ˜\"\n",
    "elif coverage >= 0.6:\n",
    "    quality = \"âš ï¸ ë³´í†µ\"\n",
    "else:\n",
    "    quality = \"âŒ ê°œì„  í•„ìš”\"\n",
    "\n",
    "print(f\"   ì¢…í•© í‰ê°€: {quality}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Self-RAGì˜ ë©”íƒ€ì¸ì§€ ëŠ¥ë ¥:\")\n",
    "print(f\"â€¢ LLMì´ ìŠ¤ìŠ¤ë¡œ ë‹µë³€ì˜ ì™„ì„±ë„ë¥¼ ê°ê´€ í‰ê°€\")\n",
    "print(f\"â€¢ ëˆ„ë½ íŒŒì…‹ì„ ì •í™•íˆ ê°ì§€ (Step 1 ë¼ìš°íŒ…ê³¼ ì—°ê³„)\") \n",
    "print(f\"â€¢ í• ë£¨ì‹œë„¤ì´ì…˜ ìœ„í—˜ë„ë¥¼ ì‚¬ì „ ê²½ê³ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ Step 3: Retry with Refine ë§›ë³´ê¸°\n",
    "\n",
    "# ëˆ„ë½ëœ íŒŒì…‹ìœ¼ë¡œ ì •ë°€ ì¿¼ë¦¬ ìƒì„±\n",
    "REFINE_PROMPT = PromptTemplate.from_template(\n",
    "\"\"\"ì•„ë˜ ì§ˆë¬¸ì˜ ëˆ„ë½ëœ íŒŒì…‹ì„ ë©”ìš°ëŠ” 'ì •ë°€ ê²€ìƒ‰ì¿¼ë¦¬' 2ê°œë§Œ JSON ë°°ì—´ë¡œ ì¶œë ¥í•˜ì„¸ìš”.\n",
    "- ê° ì¿¼ë¦¬ëŠ” íŒŒì…‹ì„ ëª…ì‹œì ìœ¼ë¡œ í¬í•¨(ë¸Œëœë“œ/ë„ì‹œ/ìš”ì¼/ì •ì±…ëª… ë“±)\n",
    "- ì¤‘ë³µ ê¸ˆì§€\n",
    "- ì¿¼ë¦¬ë¡œ ìœ ì¶”í•  ìˆ˜ ìˆëŠ” í•„ìš”í•œ ë‚´ìš©ë§Œ ì ìœ¼ì„¸ìš”\n",
    "\n",
    "[ì§ˆë¬¸]\n",
    "{question}\n",
    "\n",
    "[ëˆ„ë½ íŒŒì…‹]\n",
    "{missing_facets}\n",
    "\n",
    "JSON:\"\"\")\n",
    "\n",
    "def refine_queries(llm, question, missing_facets):\n",
    "    \"\"\"ëˆ„ë½ íŒŒì…‹ìœ¼ë¡œ ë³´ê°• ì¿¼ë¦¬ ìƒì„±\"\"\"\n",
    "    if not missing_facets: \n",
    "        return []\n",
    "    prompt_text = REFINE_PROMPT.format(question=question, missing_facets=missing_facets)\n",
    "    response = llm._call(prompt_text)\n",
    "    arr = _safe_json(response, [])\n",
    "    return [q for q in arr if isinstance(q, str)][:2]\n",
    "\n",
    "def retry_once_with_refine(llm, retriever, question):\n",
    "    \"\"\"1íšŒ ì¬ì‹œë„ë¡œ ë‹µë³€ í’ˆì§ˆ ê°œì„ \"\"\"\n",
    "    \n",
    "    # 1ì°¨ ì‹œë„\n",
    "    print(\"ğŸ”µ 1ì°¨ ì‹œë„:\")\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    answer = answer_with_context(llm, docs, question)\n",
    "    critic_result = self_rag_critic(llm, question, answer, docs)\n",
    "    \n",
    "    print(f\"   ë‹µë³€: {answer}\")\n",
    "    print(f\"   í‰ê°€: ì»¤ë²„ë¦¬ì§€ {critic_result['coverage']:.1f}, ìœ„í—˜ë„ {critic_result['hallucination_risk']}\")\n",
    "    \n",
    "    # í’ˆì§ˆì´ ì¶©ë¶„í•˜ë©´ ì¢…ë£Œ\n",
    "    if critic_result[\"coverage\"] >= 0.8 and critic_result[\"hallucination_risk\"] == \"low\":\n",
    "        print(\"   âœ… í’ˆì§ˆ ì¶©ë¶„! 1ì°¨ ë‹µë³€ìœ¼ë¡œ ì™„ë£Œ\")\n",
    "        return answer\n",
    "    \n",
    "    # ë³´ê°•ì´ í•„ìš”í•˜ë©´ ì¬ì‹œë„\n",
    "    print(\"   âš ï¸ í’ˆì§ˆ ê°œì„  í•„ìš”, ì¬ì‹œë„ ì§„í–‰...\")\n",
    "    \n",
    "    # ë³´ê°• ì¿¼ë¦¬ ìƒì„±\n",
    "    refine_queries_list = refine_queries(llm, question, critic_result[\"missing_facets\"])\n",
    "    if not refine_queries_list:\n",
    "        return f\"ê·¼ê±° ë¶ˆì¶©ë¶„. ì¶”ê°€ì •ë³´ í•„ìš”: {' / '.join(critic_result.get('ask', []))}\"\n",
    "    \n",
    "    print(f\"\\nğŸŸ¢ 2ì°¨ ì‹œë„ (ë³´ê°• ì¿¼ë¦¬: {refine_queries_list}):\")\n",
    "    \n",
    "    # ë³´ê°• ë¬¸ì„œ ìˆ˜ì§‘\n",
    "    additional_docs = []\n",
    "    for refine_query in refine_queries_list:\n",
    "        additional_docs += retriever.get_relevant_documents(refine_query)[:2]\n",
    "    \n",
    "    # ì¤‘ë³µ ì œê±° (ê°„ë‹¨í•˜ê²Œ content ê¸°ì¤€)\n",
    "    seen_content = set()\n",
    "    unique_docs = []\n",
    "    for doc in additional_docs:\n",
    "        content_key = doc.page_content[:100]  # ì• 100ìë¡œ ì¤‘ë³µ íŒë‹¨\n",
    "        if content_key not in seen_content:\n",
    "            seen_content.add(content_key)\n",
    "            unique_docs.append(doc)\n",
    "    \n",
    "    # 2ì°¨ ë‹µë³€ ìƒì„±\n",
    "    final_answer = answer_with_context(llm, unique_docs[:6], question)\n",
    "    final_critic = self_rag_critic(llm, question, final_answer, unique_docs[:6])\n",
    "    \n",
    "    print(f\"   ë‹µë³€: {final_answer}\")\n",
    "    print(f\"   í‰ê°€: ì»¤ë²„ë¦¬ì§€ {final_critic['coverage']:.1f}, ìœ„í—˜ë„ {final_critic['hallucination_risk']}\")\n",
    "    \n",
    "    # ê°œì„  íš¨ê³¼ í‘œì‹œ\n",
    "    improvement = final_critic[\"coverage\"] - critic_result[\"coverage\"]\n",
    "    print(f\"   ğŸ“ˆ ê°œì„ ë„: {improvement:+.1f} í¬ì¸íŠ¸\")\n",
    "    \n",
    "    return final_answer\n",
    "\n",
    "# ğŸ” Retry with Refine ì „ì²´ í…ŒìŠ¤íŠ¸\n",
    "print(\"ğŸ”„ Retry with Refine ë§›ë³´ê¸°\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "complex_query = \"ê¹€í¬ê³µí•­ì—ì„œ ì—ì–´ë¯¼íŠ¸ íƒ€ê³  ìˆ˜í•˜ë¬¼ ìš”ê¸ˆì´ë‘ ë³´ì•ˆê²€ìƒ‰ ì‹œê°„ë„ ì•Œë ¤ì¤˜\"\n",
    "print(f\"ë³µí•© ì§ˆë¬¸: '{complex_query}'\\n\")\n",
    "\n",
    "final_answer = retry_once_with_refine(llm, retriever, complex_query)\n",
    "\n",
    "print(f\"\\nâœ… ìµœì¢… ê²°ê³¼:\\n{final_answer}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Retry with Refine í•µì‹¬:\")\n",
    "print(f\"â€¢ 1ì°¨ ë‹µë³€ì„ Self-RAGê°€ í’ˆì§ˆ í‰ê°€\")\n",
    "print(f\"â€¢ ëˆ„ë½ëœ íŒŒì…‹ì„ LLMì´ ìë™ ê°ì§€\") \n",
    "print(f\"â€¢ ì •ë°€ ê²€ìƒ‰ì¿¼ë¦¬ë¡œ ë³´ê°• ì •ë³´ ìˆ˜ì§‘\")\n",
    "print(f\"â€¢ 2ì°¨ ë‹µë³€ìœ¼ë¡œ í’ˆì§ˆ í–¥ìƒ ë‹¬ì„±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Modular RAG ê¸°ëŠ¥ ë§›ë³´ê¸° ì™„ë£Œ!\n",
    "\n",
    "### ğŸ“Š 3ë‹¨ê³„ ê¸°ëŠ¥ ì²´í—˜ ì„±ê³¼\n",
    "- **ğŸ¯ Query Routing**: LLMì´ ì§ˆë¬¸ì„ single/multi/clarifyë¡œ ìë™ ë¶„ë¥˜\n",
    "- **ğŸ§  Self-RAG Critic**: LLMì´ ìì‹ ì˜ ë‹µë³€ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€ (ì»¤ë²„ë¦¬ì§€/í• ë£¨ì‹œë„¤ì´ì…˜/ëˆ„ë½ íŒŒì…‹)\n",
    "- **ğŸ”„ Retry with Refine**: ë¶€ì¡±í•œ ë¶€ë¶„ì„ ì •ë°€ ê²€ìƒ‰ìœ¼ë¡œ ë³´ê°•í•´ì„œ ì¬ì‹œë„\n",
    "\n",
    "### ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸\n",
    "1. **ë©”íƒ€ì¸ì§€ ëŠ¥ë ¥**: LLMì´ ë¶„ë¥˜â†’í‰ê°€â†’ë³´ê°•ì˜ ê³ ì°¨ì›ì  ì‚¬ê³  ìˆ˜í–‰\n",
    "2. **ìê°€ ê°œì„ **: ìŠ¤ìŠ¤ë¡œ ë¶€ì¡±í•¨ì„ ê°ì§€í•˜ê³  ê°œì„ í•˜ëŠ” ëŠ¥ë ¥\n",
    "3. **ëª¨ë“ˆí™”**: ê° ê¸°ëŠ¥ì´ ë…ë¦½ì ì´ë©´ì„œë„ ìœ ê¸°ì ìœ¼ë¡œ ì—°ê²°\n",
    "\n",
    "### ğŸ›  ì‹¤ë¬´ ì ìš© íŒ\n",
    "- **Query Routing**: coverage ê¸°ì¤€ì„ 0.8â†’0.6ìœ¼ë¡œ ë‚®ì¶° ë¯¼ê°ë„ ì¡°ì ˆ\n",
    "- **Self-RAG**: í• ë£¨ì‹œë„¤ì´ì…˜ ì„ê³„ì¹˜ë¡œ ë‹µë³€ ì‹ ë¢°ì„± ë³´ì¥\n",
    "- **Retry**: k=3â†’k=5ë¡œ ëŠ˜ë ¤ ë³´ê°• ë¬¸ì„œ ìˆ˜ ì¡°ì ˆ\n",
    "\n",
    "### ğŸš€ ë‹¤ìŒ ë‹¨ê³„ í™•ì¥\n",
    "- ë” ì •êµí•œ ë¼ìš°íŒ… ì „ëµ (BM25 vs Vector vs Hybrid ì„ íƒ)\n",
    "- Multi-hop Self-RAG (ì—¬ëŸ¬ ë²ˆ ë°˜ë³µ ê°œì„ )\n",
    "- ì„±ëŠ¥ ê¸°ë°˜ Adaptive Selection (ì˜ ë˜ëŠ” ì „ëµ í•™ìŠµ)\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‰ **06ë²ˆ ëª¨ë“ˆëŸ¬ RAG ë§›ë³´ê¸° ì™„ë£Œ!** ì´ì œ LLMì˜ ë©”íƒ€ì¸ì§€ ëŠ¥ë ¥ì„ í™œìš©í•œ ì§€ëŠ¥í˜• RAG ì‹œìŠ¤í…œì„ ê²½í—˜í•˜ì…¨ìŠµë‹ˆë‹¤!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
