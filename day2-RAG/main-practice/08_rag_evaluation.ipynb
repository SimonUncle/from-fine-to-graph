{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 🎯 Day 2 실습 7: RAG Evaluation (RAGAS) - 시스템 성능 측정\n",
    "\n",
    "## 🎯 학습 목표\n",
    "- **Faithfulness** (사실 정확성) 측정 - 환각 방지\n",
    "- **Answer Relevancy** (답변 관련성) 측정 - 질문에 답했는가\n",
    "- **Context Metrics** (검색 품질) 측정 - Precision & Recall\n",
    "- **1-6번 기법 비교** - 객관적 평가로 최선 선택\n",
    "\n",
    "## 💡 핵심 아이디어\n",
    "\n",
    "```\n",
    "좋은 RAG 시스템을 만드는 것도 중요하지만,\n",
    "얼마나 좋은지 측정하는 것도 중요하다!\n",
    "\n",
    "질문: \"도박 빚 갚아야 하나요?\"\n",
    "\n",
    "답변 A: \"민법 제103조는 공서양속 위반 계약을 무효로 규정합니다.\"\n",
    "  → Faithfulness: 1.0 (사실 정확)\n",
    "  → Relevancy: 0.3 (질문에 직접 답 안함)\n",
    "\n",
    "답변 B: \"도박 빚은 공서양속 위반이므로 무효입니다. 갚을 의무가 없습니다.\"\n",
    "  → Faithfulness: 1.0 (사실 정확)\n",
    "  → Relevancy: 0.9 (질문에 정확히 답함)\n",
    "```\n",
    "\n",
    "## 📋 실습 구성\n",
    "1. **Part 1**: Faithfulness (사실 정확성)\n",
    "2. **Part 2**: Answer Relevancy (답변 관련성)\n",
    "3. **Part 3**: Context Metrics (검색 품질)\n",
    "4. **Part 4**: 1-6번 기법 비교 평가\n",
    "5. **Part 5**: 시각화 & 최종 인사이트\n",
    "\n",
    "---\n",
    "\n",
    "> 💡 **1-7번 완전한 여정!**\n",
    "> - 01-02: 문제 발견 (Naive RAG 실패)\n",
    "> - 03-05: 검색 개선 (Multi-Query, Metadata, Hybrid)\n",
    "> - 06: 검색 검증 (CRAG)\n",
    "> - 07: **시스템 평가** (RAGAS로 정량적 비교)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 설치 및 import\n",
    "!pip install -q langchain-community faiss-cpu sentence-transformers rank-bm25 pandas matplotlib numpy scikit-learn FlagEmbedding\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from rank_bm25 import BM25Okapi\n",
    "from FlagEmbedding import FlagReranker\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import platform\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "print(\"✅ 라이브러리 준비 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05번 법률 문서 재사용 (11개)\n",
    "legal_docs = [\n",
    "    {\n",
    "        \"content\": \"\"\"민법 제103조 (반사회질서의 법률행위)\n",
    "조문: 선량한 풍속 기타 사회질서에 위반한 사항을 내용으로 하는 법률행위는 무효로 한다.\n",
    "적용: 공서양속 위반 계약, 불법 원인 급여\n",
    "판례: 대법원 2015다234567 - 도박 채무 계약 무효\n",
    "키워드: 공서양속, 무효, 반사회적\"\"\",\n",
    "        \"law\": \"민법 제103조\",\n",
    "        \"code\": \"103\",\n",
    "        \"category\": \"민법\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"민법 제750조 (불법행위의 내용)\n",
    "조문: 고의 또는 과실로 인한 위법행위로 타인에게 손해를 가한 자는 그 손해를 배상할 책임이 있다.\n",
    "적용: 교통사고, 명예훼손, 재산권 침해\n",
    "판례: 대법원 2018다567890 - 의료과실 손해배상\n",
    "키워드: 불법행위, 손해배상, 고의과실\"\"\",\n",
    "        \"law\": \"민법 제750조\",\n",
    "        \"code\": \"750\",\n",
    "        \"category\": \"민법\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"형법 제257조 (상해죄)\n",
    "조문: 사람의 신체를 상해한 자는 7년 이하의 징역, 10년 이하의 자격정지 또는 1천만원 이하의 벌금에 처한다.\n",
    "적용: 폭행으로 인한 신체 손상\n",
    "판례: 대법원 2019도12345 - 상해죄 성립 요건\n",
    "키워드: 상해, 신체, 폭행\"\"\",\n",
    "        \"law\": \"형법 제257조\",\n",
    "        \"code\": \"257\",\n",
    "        \"category\": \"형법\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"형법 제329조 (절도)\n",
    "조문: 타인의 재물을 절취한 자는 6년 이하의 징역 또는 1천만원 이하의 벌금에 처한다.\n",
    "적용: 재물 절도, 소매치기\n",
    "판례: 대법원 2017도98765 - 절도죄 기수시기\n",
    "키워드: 절도, 재물, 절취\"\"\",\n",
    "        \"law\": \"형법 제329조\",\n",
    "        \"code\": \"329\",\n",
    "        \"category\": \"형법\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"상법 제15조 (상호등록)\n",
    "조문: 상인은 그 상호를 등기하여야 한다. 등기한 상호는 이를 양도할 수 있다.\n",
    "적용: 상호 등록, 상호권 보호\n",
    "판례: 대법원 2016다11111 - 상호 부정사용 금지\n",
    "키워드: 상호, 등록, 상호권\"\"\",\n",
    "        \"law\": \"상법 제15조\",\n",
    "        \"code\": \"15\",\n",
    "        \"category\": \"상법\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Document 객체 생성\n",
    "documents = [Document(\n",
    "    page_content=doc[\"content\"],\n",
    "    metadata={\"law\": doc[\"law\"], \"category\": doc[\"category\"]}\n",
    ") for doc in legal_docs]\n",
    "\n",
    "# 임베딩 모델 + 벡터스토어 (05번과 동일)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# 법률용 토크나이저 (05번과 동일)\n",
    "def legal_tokenizer(text):\n",
    "    text = re.sub(r'(제\\d+조)', r' \\1 ', text)\n",
    "    text = re.sub(r'(\\d{4}[다도가나]\\d+)', r' \\1 ', text)\n",
    "    text = re.sub(r'\\b(\\d+)\\b', r' \\1 ', text)\n",
    "    tokens = [t for t in text.split() if t.strip()]\n",
    "    return tokens\n",
    "\n",
    "# BM25 인덱스 (05번과 동일)\n",
    "tokenized_corpus = [legal_tokenizer(doc.page_content) for doc in documents]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Reranker 로드 (05번과 동일)\n",
    "print(\"🔄 Reranker 모델 로딩...\")\n",
    "reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True)\n",
    "\n",
    "print(\"✅ 법률 문서 환경 준비 완료!\")\n",
    "print(f\"   문서 수: {len(documents)}개\")\n",
    "print(f\"   카테고리: 민법, 형법, 상법\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harbmjq4tnk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 모델 로드 (EXAONE 2.4B)\n",
    "!pip install -q transformers torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "print(\"🤖 LLM 모델 로딩 중...\")\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained('LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct', trust_remote_code=True)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map='auto'\n",
    ")\n",
    "print(\"✅ LLM 준비 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ix2hgqzzh6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faithfulness 평가 함수 (자세한 주석)\n",
    "\n",
    "def calculate_faithfulness(answer, context_docs):\n",
    "    \"\"\"\n",
    "    키워드 기반 Faithfulness 계산\n",
    "    \n",
    "    핵심 아이디어: 답변의 단어들이 검색된 문서에 있는가?\n",
    "    \n",
    "    Args:\n",
    "        answer: LLM이 생성한 답변\n",
    "        context_docs: 검색된 문서 리스트\n",
    "    \n",
    "    Returns:\n",
    "        faithfulness_score: 0.0 ~ 1.0 (1.0 = 완벽)\n",
    "    \"\"\"\n",
    "    # 1️⃣ 답변을 토큰(단어)으로 분해\n",
    "    # 예: \"민법 제103조에 따르면...\" → ['민법', '제103조', '따르면', ...]\n",
    "    answer_tokens = set(legal_tokenizer(answer))\n",
    "    \n",
    "    # 2️⃣ 의미 없는 조사 제거\n",
    "    # 예: \"은\", \"는\", \"이\", \"가\" 등은 평가 대상 아님\n",
    "    stopwords = {'은', '는', '이', '가', '을', '를', '의', '에', '로', '와', '과', '도'}\n",
    "    answer_tokens = answer_tokens - stopwords\n",
    "    \n",
    "    # 3️⃣ 답변이 비어있으면 1.0 (문제 없음)\n",
    "    if len(answer_tokens) == 0:\n",
    "        return 1.0\n",
    "    \n",
    "    # 4️⃣ 검색된 문서들을 하나의 텍스트로 합침\n",
    "    context_text = \" \".join([doc.page_content for doc in context_docs])\n",
    "    context_tokens = set(legal_tokenizer(context_text))\n",
    "    \n",
    "    # 5️⃣ 답변 토큰 중 문서에 있는 토큰 찾기\n",
    "    # 예: 답변 ['민법', '제103조', '무효'] vs 문서 ['민법', '제103조', '선량한', ...]\n",
    "    #     → grounded = ['민법', '제103조'] (2개 근거 있음)\n",
    "    grounded_tokens = answer_tokens & context_tokens\n",
    "    \n",
    "    # 6️⃣ Faithfulness 계산\n",
    "    # 예: 답변 토큰 10개 중 8개가 문서에 있음 → 8/10 = 0.8 (80%)\n",
    "    # \n",
    "    # 💡 해석:\n",
    "    #   - 1.0 (100%): 모든 단어가 문서에 근거 (환각 없음) ✅\n",
    "    #   - 0.8 (80%): 대부분 근거 있음 (약간의 추론 포함)\n",
    "    #   - 0.5 (50%): 절반만 근거 (환각 위험) ⚠️\n",
    "    #   - 0.0 (0%): 완전 환각 ❌\n",
    "    return len(grounded_tokens) / len(answer_tokens)\n",
    "\n",
    "print(\"✅ Faithfulness 함수 준비 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 🔍 Part 1: Faithfulness (사실 정확성)\n",
    "\n",
    "### 핵심 질문\n",
    "**\"생성된 답변이 검색된 문서에 근거하는가? (환각이 없는가?)\"**\n",
    "\n",
    "### Faithfulness란?\n",
    "- LLM이 생성한 답변의 **사실 정확성** 측정\n",
    "- 검색된 문서(Context)에 근거가 없는 내용을 말하면 점수 ↓\n",
    "- 환각(Hallucination) 방지의 핵심 지표\n",
    "\n",
    "### 계산 방법\n",
    "```python\n",
    "Faithfulness = (근거 있는 문장 수) / (전체 답변 문장 수)\n",
    "```\n",
    "\n",
    "### 예시\n",
    "**질문**: \"도박 빚 갚아야 하나요?\"\n",
    "\n",
    "**답변 A** (좋은 예):\n",
    "```\n",
    "\"민법 제103조에 따르면, 공서양속 위반 계약은 무효입니다.\n",
    "도박 채무는 공서양속 위반이므로 무효이며, 갚을 의무가 없습니다.\"\n",
    "\n",
    "Faithfulness:\n",
    "  - \"민법 제103조\" ✅ (문서에 있음)\n",
    "  - \"공서양속 위반 계약 무효\" ✅ (문서에 있음)\n",
    "  - \"도박 채무 무효\" ✅ (판례에 있음)\n",
    "  → 3/3 = 1.0 (완벽!)\n",
    "```\n",
    "\n",
    "**답변 B** (나쁜 예 - 환각):\n",
    "```\n",
    "\"도박 빚은 2년 이내에 갚지 않으면 자동 소멸됩니다.\n",
    "또한 이자율은 최대 5%로 제한됩니다.\"\n",
    "\n",
    "Faithfulness:\n",
    "  - \"2년 이내 자동 소멸\" ❌ (문서에 없음 - 환각!)\n",
    "  - \"이자율 5% 제한\" ❌ (문서에 없음 - 환각!)\n",
    "  → 0/2 = 0.0 (최악!)\n",
    "```\n",
    "\n",
    "### 실습: 간단한 Faithfulness 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faithfulness 테스트 (하드코딩 예시)\n",
    "\n",
    "print(\"🧪 Faithfulness 테스트\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# 테스트 케이스\n",
    "query = \"공서양속 위반 계약\"\n",
    "context_docs = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"질문: '{query}'\")\n",
    "print()\n",
    "print(\"검색된 문서:\")\n",
    "for i, doc in enumerate(context_docs, 1):\n",
    "    law = doc.metadata.get('law', 'unknown')\n",
    "    print(f\"   {i}. {law}\")\n",
    "print()\n",
    "\n",
    "# ⚠️ 주의: 여기서는 개념 이해를 위해 하드코딩된 예시 사용\n",
    "# Part 4에서는 EXAONE LLM이 실제로 답변 생성\n",
    "\n",
    "# 답변 A: 좋은 답변 (사실에 근거) - 하드코딩 예시\n",
    "answer_good = \"민법 제103조에 따르면 공서양속 위반 계약은 무효입니다. 도박 채무 계약이 대표적 예시입니다.\"\n",
    "\n",
    "# 답변 B: 나쁜 답변 (환각) - 하드코딩 예시\n",
    "answer_bad = \"계약은 2년 이내에 자동 소멸되며 이자율은 5%로 제한됩니다.\"\n",
    "\n",
    "print(\"### 답변 A (사실 기반) - 하드코딩 예시:\")\n",
    "print(f\"   \\\"{answer_good}\\\"\")\n",
    "faith_good = calculate_faithfulness(answer_good, context_docs)\n",
    "print(f\"   Faithfulness: {faith_good:.3f} ✅\")\n",
    "print()\n",
    "\n",
    "print(\"### 답변 B (환각) - 하드코딩 예시:\")\n",
    "print(f\"   \\\"{answer_bad}\\\"\")\n",
    "faith_bad = calculate_faithfulness(answer_bad, context_docs)\n",
    "print(f\"   Faithfulness: {faith_bad:.3f} ❌\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"💡 Faithfulness가 높을수록 사실에 근거한 답변!\")\n",
    "print(\"   답변 A: 문서에 근거 → 높은 점수\")\n",
    "print(\"   답변 B: 환각 내용 → 낮은 점수\")\n",
    "print()\n",
    "print(\"📌 Part 4에서는 EXAONE LLM이 실제로 답변을 생성합니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎯 실습: Faithfulness 퀴즈\n",
    "\n",
    "### 다음 두 답변 중 어느 것이 더 사실적인가?\n",
    "\n",
    "**질문**: \"도박 빚 갚아야 하나요?\"\n",
    "\n",
    "**검색된 문서**: 민법 제103조 (공서양속 위반 계약 무효)\n",
    "\n",
    "**답변 A:**\n",
    "> \"아니요, 갚을 필요 없습니다. 도박 빚은 민법 제103조의 공서양속 위반 계약으로 무효이므로 법적 의무가 없습니다.\"\n",
    "\n",
    "**답변 B:**\n",
    "> \"도박 빚은 2년 이내에 갚지 않으면 자동으로 소멸됩니다. 이자율은 최대 5%로 제한됩니다.\"\n",
    "\n",
    "어느 답변이 더 높은 Faithfulness 점수를 받을까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 퀴즈: 두 답변의 Faithfulness 비교\n",
    "\n",
    "query = \"도박 빚 갚아야 하나요?\"\n",
    "context_docs = vectorstore.similarity_search(\"민법 제103조 공서양속 위반 계약\", k=3)\n",
    "\n",
    "# 답변 A: 사실 기반\n",
    "answer_A = \"아니요, 갚을 필요 없습니다. 도박 빚은 민법 제103조의 공서양속 위반 계약으로 무효이므로 법적 의무가 없습니다.\"\n",
    "\n",
    "# 답변 B: 환각 (사실이 아님)\n",
    "answer_B = \"도박 빚은 2년 이내에 갚지 않으면 자동으로 소멸됩니다. 이자율은 최대 5%로 제한됩니다.\"\n",
    "\n",
    "print(\"🎯 Faithfulness 퀴즈 결과\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# 평가\n",
    "faith_A = calculate_faithfulness(answer_A, context_docs)\n",
    "faith_B = calculate_faithfulness(answer_B, context_docs)\n",
    "\n",
    "print(f\"답변 A: Faithfulness = {faith_A:.3f}\")\n",
    "print(f\"  → {'✅ 높음 (사실 기반)' if faith_A >= 0.7 else '⚠️ 낮음'}\")\n",
    "print()\n",
    "\n",
    "print(f\"답변 B: Faithfulness = {faith_B:.3f}\")\n",
    "print(f\"  → {'❌ 낮음 (환각)' if faith_B < 0.5 else '⚠️ 중간'}\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "if faith_A > faith_B:\n",
    "    print(\"✅ 정답: 답변 A가 더 사실적입니다!\")\n",
    "    print(f\"   차이: {(faith_A - faith_B):.3f} ({(faith_A - faith_B) / faith_B * 100:.0f}% 더 높음)\")\n",
    "else:\n",
    "    print(\"❌ 예상과 다른 결과\")\n",
    "\n",
    "print()\n",
    "print(\"💡 핵심: Faithfulness가 높을수록 환각이 적고 신뢰할 수 있는 답변!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>📝 정답 및 해설 (클릭)</summary>\n",
    "\n",
    "**정답: 답변 A** ✅\n",
    "\n",
    "### 답변 A 분석 (사실 기반)\n",
    "```\n",
    "\"아니요, 갚을 필요 없습니다. 도박 빚은 민법 제103조의 \n",
    " 공서양속 위반 계약으로 무효이므로 법적 의무가 없습니다.\"\n",
    "```\n",
    "\n",
    "**근거 있는 내용:**\n",
    "- ✅ \"민법 제103조\" → 검색 문서에 있음\n",
    "- ✅ \"공서양속 위반 계약\" → 검색 문서에 있음\n",
    "- ✅ \"무효\" → 검색 문서에 있음\n",
    "- ✅ \"법적 의무 없음\" → 무효의 논리적 귀결\n",
    "\n",
    "**Faithfulness: 0.8~1.0** (매우 높음)\n",
    "\n",
    "---\n",
    "\n",
    "### 답변 B 분석 (환각)\n",
    "```\n",
    "\"도박 빚은 2년 이내에 갚지 않으면 자동으로 소멸됩니다.\n",
    " 이자율은 최대 5%로 제한됩니다.\"\n",
    "```\n",
    "\n",
    "**근거 없는 내용 (환각):**\n",
    "- ❌ \"2년 이내 자동 소멸\" → 검색 문서에 없음 (완전 환각!)\n",
    "- ❌ \"이자율 5% 제한\" → 검색 문서에 없음 (완전 환각!)\n",
    "- ❌ \"도박 빚\" → 검색 문서에 있지만, 관련 없는 내용으로 연결\n",
    "\n",
    "**Faithfulness: 0.0~0.3** (매우 낮음)\n",
    "\n",
    "---\n",
    "\n",
    "### 핵심 교훈\n",
    "\n",
    "**💡 Faithfulness의 중요성:**\n",
    "- 법률, 의료, 금융 등에서는 **환각이 치명적**\n",
    "- 사실에 근거하지 않은 답변은 사용자에게 피해를 줄 수 있음\n",
    "- RAG 시스템에서 Faithfulness **≥ 0.8**을 목표로 해야 함\n",
    "\n",
    "**⚠️ LLM의 특성:**\n",
    "- LLM은 자연스러운 문장을 생성하지만, **사실 여부를 보장하지 않음**\n",
    "- 검색 문서(Context)에 근거하도록 **강제**해야 함\n",
    "- Faithfulness 평가로 **검증** 필수!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6p3gdppt7zr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Relevancy 평가 함수 (자세한 주석)\n",
    "\n",
    "def calculate_answer_relevancy(query, answer, embeddings_model):\n",
    "    \"\"\"\n",
    "    Cosine Similarity 기반 Answer Relevancy 계산\n",
    "    \n",
    "    핵심 아이디어: 질문과 답변이 의미적으로 얼마나 가까운가?\n",
    "    \n",
    "    Args:\n",
    "        query: 질문 (예: \"도박 빚 갚아야 하나요?\")\n",
    "        answer: LLM 답변 (예: \"갚을 필요 없습니다...\")\n",
    "        embeddings_model: 임베딩 모델 (BGE-M3)\n",
    "    \n",
    "    Returns:\n",
    "        similarity: 0.0 ~ 1.0 (1.0 = 완벽한 관련성)\n",
    "    \"\"\"\n",
    "    # 1️⃣ 질문과 답변을 각각 임베딩으로 변환\n",
    "    # 임베딩 = 문장의 의미를 768차원 벡터로 표현\n",
    "    # 예: \"도박 빚 갚아야?\" → [0.23, -0.15, 0.87, ..., 0.45] (768개 숫자)\n",
    "    query_embedding = embeddings_model.embed_query(query)\n",
    "    answer_embedding = embeddings_model.embed_query(answer)\n",
    "    \n",
    "    # 2️⃣ Numpy array로 변환 (계산 편의)\n",
    "    query_vec = np.array(query_embedding).reshape(1, -1)   # (1, 768)\n",
    "    answer_vec = np.array(answer_embedding).reshape(1, -1) # (1, 768)\n",
    "    \n",
    "    # 3️⃣ Cosine Similarity 계산\n",
    "    # 두 벡터 사이의 각도를 측정 (방향이 얼마나 비슷한가?)\n",
    "    # \n",
    "    # 값 범위: -1 ~ 1\n",
    "    #   - 1.0: 완전히 같은 방향 (매우 관련 있음) ✅\n",
    "    #   - 0.5: 어느 정도 관련\n",
    "    #   - 0.0: 무관함\n",
    "    #   - -1.0: 완전 반대 (거의 없음)\n",
    "    # \n",
    "    # 💡 예시:\n",
    "    #   질문: \"도박 빚 갚아야?\" vs 답변: \"갚을 필요 없습니다\" → 0.9 (높음)\n",
    "    #   질문: \"도박 빚 갚아야?\" vs 답변: \"민법 제103조는...\" → 0.3 (낮음)\n",
    "    similarity = cosine_similarity(query_vec, answer_vec)[0][0]\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "print(\"✅ Answer Relevancy 함수 준비 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 🎯 Part 2: Answer Relevancy (답변 관련성)\n",
    "\n",
    "### 핵심 질문\n",
    "**\"답변이 질문에 정확히 답했는가?\"**\n",
    "\n",
    "### Answer Relevancy란?\n",
    "- 생성된 답변이 **원래 질문**과 얼마나 관련있는가?\n",
    "- 사실은 맞지만 질문에 답하지 않으면 점수 ↓\n",
    "- 사용자 경험의 핵심 지표\n",
    "\n",
    "### 계산 방법\n",
    "```python\n",
    "Answer Relevancy = Cosine Similarity(질문 임베딩, 답변 임베딩)\n",
    "```\n",
    "\n",
    "### 예시\n",
    "**질문**: \"도박 빚 갚아야 하나요?\"\n",
    "\n",
    "**답변 A** (나쁜 예 - 관련 없음):\n",
    "```\n",
    "\"민법 제103조는 반사회질서의 법률행위에 관한 조문입니다.\n",
    "선량한 풍속 기타 사회질서에 위반한 사항을 내용으로 하는 법률행위는 무효로 한다.\"\n",
    "\n",
    "Answer Relevancy: 0.3\n",
    "→ 사실은 맞지만, \"갚아야 하나요?\"에 직접 답하지 않음 ❌\n",
    "```\n",
    "\n",
    "**답변 B** (좋은 예 - 직접 답변):\n",
    "```\n",
    "\"아니요, 갚을 필요 없습니다.\n",
    "도박 빚은 민법 제103조의 공서양속 위반 계약으로 무효이므로 법적 의무가 없습니다.\"\n",
    "\n",
    "Answer Relevancy: 0.9\n",
    "→ 질문에 정확히 답함 (Yes/No + 이유) ✅\n",
    "```\n",
    "\n",
    "### 실습: Answer Relevancy 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 📊 Part 3: Context Metrics (검색 품질)\n",
    "\n",
    "### Context Precision (정밀도)\n",
    "**\"검색된 문서 중 실제로 유용한 문서 비율\"**\n",
    "\n",
    "**공식:**\n",
    "```\n",
    "Context Precision = (유용한 문서 수) / (전체 검색 문서 수)\n",
    "```\n",
    "\n",
    "**예시:**\n",
    "- Top-5 검색 결과: [민법103, 민법750, 형법257, 상법15, 형법329]\n",
    "- 질문: \"공서양속 위반 계약\"\n",
    "- 유용한 문서: 민법103 (1개)\n",
    "- Context Precision = 1/5 = 0.2 (20%)\n",
    "\n",
    "**해석:**\n",
    "- **높을수록 좋음** (1.0 = 100% = 모든 검색 결과가 유용)\n",
    "- 낮으면 → 노이즈가 많음 → 검색 정확도 ↓\n",
    "\n",
    "---\n",
    "\n",
    "### Context Recall (재현율)\n",
    "**\"필요한 정보를 얼마나 찾았는가\"**\n",
    "\n",
    "**공식:**\n",
    "```\n",
    "Context Recall = (찾은 필요 문서 수) / (전체 필요 문서 수)\n",
    "```\n",
    "\n",
    "**예시:**\n",
    "- 필요한 문서: [민법103, 판례2015다234567] (2개)\n",
    "- 검색된 문서: [민법103] (1개만 찾음)\n",
    "- Context Recall = 1/2 = 0.5 (50%)\n",
    "\n",
    "**해석:**\n",
    "- **높을수록 좋음** (1.0 = 100% = 필요한 정보 모두 찾음)\n",
    "- 낮으면 → 정보 누락 → 불완전한 답변 가능성 ↑\n",
    "\n",
    "---\n",
    "\n",
    "### Precision vs Recall 트레이드오프\n",
    "\n",
    "|  | **높은 정확도** (Precision ↑) | **낮은 정확도** (Precision ↓) |\n",
    "|---|---|---|\n",
    "| **높은 재현율** (Recall ↑) | 🟢 **이상적!**<br>정확하고 완전함 | 🟡 **노이즈多**<br>정보는 다 있지만 쓸데없는 것도 많음 |\n",
    "| **낮은 재현율** (Recall ↓) | 🟡 **정보 누락**<br>정확하지만 정보 부족 | 🔴 **최악**<br>정확하지도 완전하지도 않음 |\n",
    "\n",
    "**실무 예시:**\n",
    "- **Precision 높음 + Recall 낮음**: Top-1만 검색 (정확하지만 정보 부족)\n",
    "- **Precision 낮음 + Recall 높음**: Top-100 검색 (정보는 다 있지만 노이즈 多)\n",
    "- **둘 다 높음**: 이상적! (정확하면서 완전함)\n",
    "\n",
    "---\n",
    "\n",
    "### 실습: Context Metrics 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context Metrics 평가 함수 (자세한 주석)\n",
    "\n",
    "def calculate_context_precision(query, retrieved_docs, embeddings_model, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Context Precision 계산: 검색된 문서 중 유용한 비율\n",
    "    \n",
    "    Args:\n",
    "        query: 질문 (예: \"공서양속 위반 계약\")\n",
    "        retrieved_docs: 검색된 문서 리스트 (예: Top-5)\n",
    "        embeddings_model: 임베딩 모델 (BGE-M3)\n",
    "        threshold: 유용성 판단 기준 (기본값 0.3)\n",
    "    \n",
    "    Returns:\n",
    "        precision: 정밀도 (0.0 ~ 1.0)\n",
    "        relevant_count: 유용한 문서 개수\n",
    "    \"\"\"\n",
    "    # 1️⃣ 질문을 임베딩으로 변환 (768차원 벡터)\n",
    "    query_embedding = np.array(embeddings_model.embed_query(query)).reshape(1, -1)\n",
    "    \n",
    "    relevant_count = 0\n",
    "    \n",
    "    # 2️⃣ 각 검색된 문서를 평가\n",
    "    for doc in retrieved_docs:\n",
    "        # 문서도 임베딩으로 변환\n",
    "        doc_embedding = np.array(embeddings_model.embed_query(doc.page_content)).reshape(1, -1)\n",
    "        \n",
    "        # 3️⃣ Cosine Similarity 계산 (질문 vs 문서)\n",
    "        # 값 범위: -1 ~ 1 (1에 가까울수록 유사)\n",
    "        similarity = cosine_similarity(query_embedding, doc_embedding)[0][0]\n",
    "        \n",
    "        # 4️⃣ Threshold 기준으로 유용성 판단\n",
    "        # threshold=0.3 의미:\n",
    "        #   - 0.3 이상: 유용한 문서 (질문과 관련 있음)\n",
    "        #   - 0.3 미만: 노이즈 문서 (질문과 관련 없음)\n",
    "        # \n",
    "        # 💡 왜 0.3?\n",
    "        #   - 너무 높으면 (0.8): 거의 모든 문서가 노이즈로 판단 (엄격)\n",
    "        #   - 너무 낮으면 (0.1): 거의 모든 문서가 유용으로 판단 (관대)\n",
    "        #   - 0.3: 적당한 기준 (실험적으로 결정)\n",
    "        if similarity >= threshold:\n",
    "            relevant_count += 1\n",
    "    \n",
    "    # 5️⃣ Precision 계산\n",
    "    # 예: 5개 검색 중 2개 유용 → 2/5 = 0.4 (40%)\n",
    "    precision = relevant_count / len(retrieved_docs) if retrieved_docs else 0.0\n",
    "    \n",
    "    return precision, relevant_count\n",
    "\n",
    "\n",
    "def calculate_context_recall(required_laws, retrieved_docs):\n",
    "    \"\"\"\n",
    "    Context Recall 계산: 필요한 정보를 얼마나 찾았는가\n",
    "    \n",
    "    Args:\n",
    "        required_laws: 답변에 필요한 조문 리스트\n",
    "                      예: [\"민법 제103조\", \"민법 제750조\"]\n",
    "        retrieved_docs: 검색된 문서 리스트\n",
    "    \n",
    "    Returns:\n",
    "        recall: 재현율 (0.0 ~ 1.0)\n",
    "        found_count: 찾은 필요 문서 개수\n",
    "    \"\"\"\n",
    "    # 1️⃣ 검색된 문서들의 조문 추출\n",
    "    # 예: ['민법 제103조', '형법 제257조', ...]\n",
    "    retrieved_laws = [doc.metadata.get('law', '') for doc in retrieved_docs]\n",
    "    \n",
    "    found_count = 0\n",
    "    \n",
    "    # 2️⃣ 필요한 조문이 검색됐는지 확인\n",
    "    for required in required_laws:\n",
    "        if required in retrieved_laws:\n",
    "            found_count += 1\n",
    "    \n",
    "    # 3️⃣ Recall 계산\n",
    "    # 예: 필요 2개 중 1개 찾음 → 1/2 = 0.5 (50%)\n",
    "    # \n",
    "    # 💡 해석:\n",
    "    #   - 1.0 (100%): 필요한 정보 모두 찾음 ✅\n",
    "    #   - 0.5 (50%): 절반만 찾음 (정보 누락 위험)\n",
    "    #   - 0.0 (0%): 아무것도 못 찾음 ❌\n",
    "    recall = found_count / len(required_laws) if required_laws else 0.0\n",
    "    \n",
    "    return recall, found_count\n",
    "\n",
    "print(\"✅ Context Metrics 평가 함수 준비 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ka93h4jx11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context Metrics 테스트 (자세한 해석)\n",
    "\n",
    "query = \"공서양속 위반 계약 무효\"\n",
    "required_laws = [\"민법 제103조\"]  # 이 질문에 필요한 조문\n",
    "retrieved_docs = vectorstore.similarity_search(query, k=5)\n",
    "\n",
    "print(\"🧪 Context Metrics 테스트\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"질문: '{query}'\")\n",
    "print(f\"필요한 조문: {required_laws}\")\n",
    "print()\n",
    "print(\"검색 결과 (Top-5):\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    law = doc.metadata.get('law')\n",
    "    print(f\"   {i}. {law}\")\n",
    "print()\n",
    "\n",
    "# Precision 계산\n",
    "precision, relevant_count = calculate_context_precision(query, retrieved_docs, embeddings, threshold=0.3)\n",
    "\n",
    "print(f\"📊 Context Precision: {precision:.3f} ({precision*100:.0f}%)\")\n",
    "print(f\"   해석: 5개 검색 결과 중 {relevant_count}개가 유용\")\n",
    "print(f\"   {'✅ 높음 (노이즈 적음)' if precision >= 0.6 else '⚠️ 낮음 (노이즈 많음)'}\")\n",
    "print()\n",
    "\n",
    "# Recall 계산\n",
    "recall, found_count = calculate_context_recall(required_laws, retrieved_docs)\n",
    "\n",
    "print(f\"📊 Context Recall: {recall:.3f} ({recall*100:.0f}%)\")\n",
    "print(f\"   해석: {len(required_laws)}개 필요 조문 중 {found_count}개 찾음\")\n",
    "print(f\"   {'✅ 높음 (정보 충분)' if recall >= 0.8 else '⚠️ 낮음 (정보 누락 위험)'}\")\n",
    "print()\n",
    "\n",
    "# 종합 해석\n",
    "print(\"=\" * 70)\n",
    "print(\"💡 종합 해석:\")\n",
    "if precision >= 0.6 and recall >= 0.8:\n",
    "    print(\"   🟢 이상적! 정확하고 완전한 검색\")\n",
    "elif precision >= 0.6:\n",
    "    print(\"   🟡 정확하지만 정보 누락 가능성\")\n",
    "elif recall >= 0.8:\n",
    "    print(\"   🟡 정보는 충분하지만 노이즈 많음\")\n",
    "else:\n",
    "    print(\"   🔴 검색 품질 개선 필요\")\n",
    "print()\n",
    "print(f\"   Precision {precision:.1f} | Recall {recall:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎯 실습: Precision vs Recall 상황 퀴즈\n",
    "\n",
    "### 다음 세 시스템 중 어떤 문제가 있을까요?\n",
    "\n",
    "**시스템 A:**\n",
    "- Context Precision: **0.9** (90%)\n",
    "- Context Recall: **0.3** (30%)\n",
    "\n",
    "**시스템 B:**\n",
    "- Context Precision: **0.3** (30%)\n",
    "- Context Recall: **0.9** (90%)\n",
    "\n",
    "**시스템 C:**\n",
    "- Context Precision: **0.8** (80%)\n",
    "- Context Recall: **0.8** (80%)\n",
    "\n",
    "각 시스템의 문제점은 무엇일까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision vs Recall 시각화\n",
    "\n",
    "systems = [\n",
    "    {\"name\": \"시스템 A\", \"precision\": 0.9, \"recall\": 0.3, \"problem\": \"정보 누락 (낮은 Recall)\"},\n",
    "    {\"name\": \"시스템 B\", \"precision\": 0.3, \"recall\": 0.9, \"problem\": \"노이즈 과다 (낮은 Precision)\"},\n",
    "    {\"name\": \"시스템 C\", \"precision\": 0.8, \"recall\": 0.8, \"problem\": \"이상적 (균형잡힘)\"}\n",
    "]\n",
    "\n",
    "print(\"🎯 Precision vs Recall 분석\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "for sys in systems:\n",
    "    print(f\"{sys['name']}:\")\n",
    "    print(f\"  Precision: {sys['precision']:.1f} | Recall: {sys['recall']:.1f}\")\n",
    "    print(f\"  진단: {sys['problem']}\")\n",
    "    \n",
    "    # 상세 설명\n",
    "    if sys['precision'] > 0.6 and sys['recall'] < 0.5:\n",
    "        print(f\"  ⚠️ 정확하지만 정보 부족 → Top-K ↑ 필요\")\n",
    "    elif sys['precision'] < 0.5 and sys['recall'] > 0.6:\n",
    "        print(f\"  ⚠️ 정보는 많지만 노이즈 多 → Re-ranking 필요\")\n",
    "    elif sys['precision'] >= 0.7 and sys['recall'] >= 0.7:\n",
    "        print(f\"  ✅ 이상적인 균형!\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"💡 핵심: Precision과 Recall 모두 높아야 좋은 시스템!\")\n",
    "print(\"   - Precision 높음 + Recall 낮음 → Top-K를 늘려라\")\n",
    "print(\"   - Precision 낮음 + Recall 높음 → Re-ranking을 추가하라\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>📝 정답 및 해설 (클릭)</summary>\n",
    "\n",
    "### 시스템 A: Precision 0.9, Recall 0.3\n",
    "**문제: 정보 누락 (낮은 Recall)** ⚠️\n",
    "\n",
    "**상황:**\n",
    "- 검색된 문서는 매우 정확함 (90% 유용)\n",
    "- 하지만 필요한 정보의 30%만 찾음 (70% 누락!)\n",
    "\n",
    "**예시:**\n",
    "- 질문: \"도박 빚 관련 법률\"\n",
    "- 필요 문서: [민법 제103조, 판례 2015다234567, 관련 해설]\n",
    "- 검색 결과: [민법 제103조] ← 정확하지만 1개만\n",
    "- 결과: 정확하지만 불완전한 답변\n",
    "\n",
    "**해결책:**\n",
    "- Top-K를 늘려라 (3 → 5 → 10)\n",
    "- Multi-Query로 다양한 각도에서 검색\n",
    "\n",
    "---\n",
    "\n",
    "### 시스템 B: Precision 0.3, Recall 0.9\n",
    "**문제: 노이즈 과다 (낮은 Precision)** ⚠️\n",
    "\n",
    "**상황:**\n",
    "- 필요한 정보는 대부분 찾음 (90%)\n",
    "- 하지만 검색 결과의 70%가 쓸모없는 노이즈\n",
    "\n",
    "**예시:**\n",
    "- 질문: \"도박 빚 관련 법률\"\n",
    "- 검색 결과 10개: [민법103 ✅, 형법1 ❌, 상법1 ❌, 헌법1 ❌, ...]\n",
    "- 결과: 정보는 있지만 LLM이 혼란스러움 (노이즈 多)\n",
    "\n",
    "**해결책:**\n",
    "- Re-ranking 추가 (Cross-Encoder로 정밀 평가)\n",
    "- Threshold 높이기\n",
    "\n",
    "---\n",
    "\n",
    "### 시스템 C: Precision 0.8, Recall 0.8\n",
    "**이상적! (균형잡힌 시스템)** ✅\n",
    "\n",
    "**상황:**\n",
    "- 검색 결과의 80%가 유용 (노이즈 적음)\n",
    "- 필요한 정보의 80% 찾음 (누락 적음)\n",
    "\n",
    "**예시:**\n",
    "- 질문: \"도박 빚 관련 법률\"\n",
    "- 필요 문서: [민법103, 판례2015, 해설]\n",
    "- 검색 결과 5개: [민법103 ✅, 판례2015 ✅, 형법1 ❌, 해설 ✅, 상법1 ❌]\n",
    "- Precision: 3/5 = 0.6 ← 실제로는 더 높게 측정됨\n",
    "- Recall: 3/3 = 1.0\n",
    "\n",
    "**유지 방법:**\n",
    "- Hybrid Search + Reranking 조합\n",
    "- 적절한 Top-K (너무 많지도, 적지도 않게)\n",
    "\n",
    "---\n",
    "\n",
    "### 핵심 교훈\n",
    "\n",
    "| 상황 | Precision | Recall | 문제 | 해결책 |\n",
    "|------|-----------|--------|------|--------|\n",
    "| 시스템 A | 높음 | **낮음** | 정보 누락 | Top-K ↑, Multi-Query |\n",
    "| 시스템 B | **낮음** | 높음 | 노이즈 多 | Re-ranking, Threshold ↑ |\n",
    "| 시스템 C | 높음 | 높음 | 없음 | 유지! |\n",
    "\n",
    "**💡 실무 목표:**\n",
    "- Precision ≥ 0.7\n",
    "- Recall ≥ 0.7\n",
    "- 둘 다 높은 시스템이 **이상적**!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎛️ 실습: Threshold 튜닝\n",
    "\n",
    "### Threshold란?\n",
    "Context Precision 계산 시 **문서의 유용성을 판단하는 기준 점수**\n",
    "\n",
    "```python\n",
    "if cosine_similarity(query, doc) >= threshold:\n",
    "    유용한 문서  # Precision 분자 +1\n",
    "else:\n",
    "    노이즈 문서  # Precision 분자 유지\n",
    "```\n",
    "\n",
    "### Threshold의 영향\n",
    "\n",
    "**Threshold 높음 (0.5~0.9):**\n",
    "- 엄격한 기준 → 적은 문서만 \"유용\"으로 판단\n",
    "- Precision ↓ (유용 문서 수 감소)\n",
    "- 노이즈는 적지만 필요한 문서도 누락 위험\n",
    "\n",
    "**Threshold 낮음 (0.1~0.3):**\n",
    "- 관대한 기준 → 많은 문서를 \"유용\"으로 판단\n",
    "- Precision ↑ (유용 문서 수 증가)\n",
    "- 정보는 많지만 노이즈도 많아짐\n",
    "\n",
    "**Threshold 적정 (0.3~0.5):**\n",
    "- 균형잡힌 기준\n",
    "- 실무에서 일반적으로 **0.3**을 사용\n",
    "\n",
    "### 실습: Threshold를 바꿔보며 Precision 변화 관찰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기를 수정하세요! 👇\n",
    "threshold = 0.3  # 0.1 ~ 0.9 사이 값으로 변경해보세요\n",
    "\n",
    "print(f\"🎛️ Threshold = {threshold}\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# 테스트 케이스\n",
    "test_queries = [\n",
    "    {\"query\": \"공서양속 위반 계약\", \"desc\": \"명확한 법률 질문\"},\n",
    "    {\"query\": \"불법행위 손해배상\", \"desc\": \"특정 조문 질문\"},\n",
    "    {\"query\": \"법률 자문 비용\", \"desc\": \"일반적 법률 용어\"}\n",
    "]\n",
    "\n",
    "precision_scores = []\n",
    "\n",
    "for test in test_queries:\n",
    "    query = test[\"query\"]\n",
    "    desc = test[\"desc\"]\n",
    "    \n",
    "    # 검색\n",
    "    docs = vectorstore.similarity_search(query, k=5)\n",
    "    \n",
    "    # Precision 계산 (threshold 사용)\n",
    "    precision, rel_count = calculate_context_precision(query, docs, embeddings, threshold=threshold)\n",
    "    precision_scores.append(precision)\n",
    "    \n",
    "    print(f\"질문: '{query}'\")\n",
    "    print(f\"  유형: {desc}\")\n",
    "    print(f\"  Precision: {precision:.3f} (유용: {rel_count}/5)\")\n",
    "    print()\n",
    "\n",
    "avg_precision = np.mean(precision_scores)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"📊 평균 Precision: {avg_precision:.3f}\")\n",
    "print()\n",
    "\n",
    "# 결과 해석\n",
    "if threshold >= 0.6:\n",
    "    print(\"⚠️ Threshold가 너무 높습니다!\")\n",
    "    print(\"   → Precision이 낮아짐 (엄격한 기준)\")\n",
    "    print(\"   → 유용한 문서도 노이즈로 분류됨\")\n",
    "    print(\"   → 추천: 0.3~0.5로 낮추세요\")\n",
    "elif threshold <= 0.2:\n",
    "    print(\"⚠️ Threshold가 너무 낮습니다!\")\n",
    "    print(\"   → Precision이 높아짐 (관대한 기준)\")\n",
    "    print(\"   → 하지만 노이즈 문서도 유용으로 분류됨\")\n",
    "    print(\"   → 추천: 0.3~0.5로 높이세요\")\n",
    "else:\n",
    "    print(\"✅ 적절한 Threshold입니다!\")\n",
    "    print(f\"   → 평균 Precision {avg_precision:.1f} (균형 잡힘)\")\n",
    "\n",
    "print()\n",
    "print(\"💡 실험해보세요:\")\n",
    "print(\"   - threshold=0.1: Precision ↑ (관대)\")\n",
    "print(\"   - threshold=0.5: Precision ↓ (엄격)\")\n",
    "print(\"   - threshold=0.9: Precision ↓↓ (매우 엄격)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>📝 실험 결과 예시 (클릭)</summary>\n",
    "\n",
    "### Threshold = 0.1 (매우 낮음)\n",
    "```\n",
    "질문: '공서양속 위반 계약'\n",
    "  Precision: 1.000 (유용: 5/5)  ← 모든 문서가 유용으로 판단\n",
    "\n",
    "질문: '불법행위 손해배상'\n",
    "  Precision: 1.000 (유용: 5/5)\n",
    "\n",
    "질문: '법률 자문 비용'\n",
    "  Precision: 1.000 (유용: 5/5)\n",
    "\n",
    "평균 Precision: 1.000\n",
    "```\n",
    "\n",
    "**문제:**\n",
    "- 거의 모든 문서를 유용으로 판단 (관대한 기준)\n",
    "- Precision은 높지만, 실제로는 노이즈 많음\n",
    "- **과대평가** 위험\n",
    "\n",
    "---\n",
    "\n",
    "### Threshold = 0.3 (적정) ← 추천!\n",
    "```\n",
    "질문: '공서양속 위반 계약'\n",
    "  Precision: 0.600 (유용: 3/5)  ← 적절한 판단\n",
    "\n",
    "질문: '불법행위 손해배상'\n",
    "  Precision: 0.600 (유용: 3/5)\n",
    "\n",
    "질문: '법률 자문 비용'\n",
    "  Precision: 0.400 (유용: 2/5)  ← 일반 용어라 낮음 (정상)\n",
    "\n",
    "평균 Precision: 0.533\n",
    "```\n",
    "\n",
    "**✅ 이상적:**\n",
    "- 적절한 기준으로 유용/노이즈 구분\n",
    "- 명확한 질문은 높은 Precision\n",
    "- 모호한 질문은 낮은 Precision (실제 반영)\n",
    "\n",
    "---\n",
    "\n",
    "### Threshold = 0.7 (매우 높음)\n",
    "```\n",
    "질문: '공서양속 위반 계약'\n",
    "  Precision: 0.200 (유용: 1/5)  ← 너무 엄격\n",
    "\n",
    "질문: '불법행위 손해배상'\n",
    "  Precision: 0.200 (유용: 1/5)\n",
    "\n",
    "질문: '법률 자문 비용'\n",
    "  Precision: 0.000 (유용: 0/5)  ← 아무것도 통과 못함\n",
    "\n",
    "평균 Precision: 0.133\n",
    "```\n",
    "\n",
    "**문제:**\n",
    "- 너무 엄격한 기준 → 유용한 문서도 노이즈로 분류\n",
    "- Precision 매우 낮음\n",
    "- **과소평가** 위험\n",
    "\n",
    "---\n",
    "\n",
    "### 핵심 교훈\n",
    "\n",
    "| Threshold | Precision | 평가 |\n",
    "|-----------|-----------|------|\n",
    "| 0.1 | 높음 (1.0) | 관대 (노이즈 포함) ⚠️ |\n",
    "| **0.3** | **중간 (0.5)** | **적정 (균형)** ✅ |\n",
    "| 0.7 | 낮음 (0.1) | 엄격 (유용 문서 누락) ⚠️ |\n",
    "\n",
    "**💡 실무 가이드:**\n",
    "- **기본값: 0.3** (대부분의 경우)\n",
    "- 법률/의료 (정확도 중요): 0.4~0.5 (약간 엄격)\n",
    "- 일반 QA (포괄성 중요): 0.2~0.3 (약간 관대)\n",
    "- 실험으로 **도메인별 최적값** 찾기!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔢 실습: Top-K 튜닝\n",
    "\n",
    "### Top-K란?\n",
    "검색 시 **상위 몇 개 문서를 가져올지** 결정하는 파라미터\n",
    "\n",
    "```python\n",
    "vectorstore.similarity_search(query, k=3)  # 상위 3개만\n",
    "vectorstore.similarity_search(query, k=10)  # 상위 10개\n",
    "```\n",
    "\n",
    "### Top-K의 영향\n",
    "\n",
    "**Top-K 작음 (k=1~3):**\n",
    "- 적은 문서만 검색 → 빠르고 정확\n",
    "- **Precision ↑** (노이즈 적음)\n",
    "- **Recall ↓** (정보 누락 위험)\n",
    "- 간단한 질문에 적합\n",
    "\n",
    "**Top-K 큼 (k=10~20):**\n",
    "- 많은 문서 검색 → 느리고 노이즈 多\n",
    "- **Precision ↓** (노이즈 많음)\n",
    "- **Recall ↑** (정보 풍부)\n",
    "- 복합 질문에 적합\n",
    "\n",
    "**Top-K 적정 (k=3~5):**\n",
    "- 균형잡힌 검색\n",
    "- 실무에서 일반적으로 **k=3**을 사용\n",
    "\n",
    "### 실습: Top-K를 바꿔보며 Precision/Recall 변화 관찰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기를 수정하세요! 👇\n",
    "top_k = 3  # 1 ~ 10 사이 값으로 변경해보세요\n",
    "\n",
    "print(f\"🔢 Top-K = {top_k}\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# 테스트 케이스 (복합 질문)\n",
    "test_cases = [\n",
    "    {\n",
    "        \"query\": \"공서양속 위반 계약 무효\",\n",
    "        \"required_laws\": [\"민법 제103조\"],\n",
    "        \"desc\": \"단순 질문\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"불법행위 손해배상 민법\",\n",
    "        \"required_laws\": [\"민법 제750조\"],\n",
    "        \"desc\": \"단순 질문\"\n",
    "    }\n",
    "]\n",
    "\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for test in test_cases:\n",
    "    query = test[\"query\"]\n",
    "    required_laws = test[\"required_laws\"]\n",
    "    desc = test[\"desc\"]\n",
    "    \n",
    "    # 검색 (top_k 사용)\n",
    "    docs = vectorstore.similarity_search(query, k=top_k)\n",
    "    \n",
    "    # Precision & Recall 계산\n",
    "    precision, rel_count = calculate_context_precision(query, docs, embeddings, threshold=0.3)\n",
    "    recall, found_count = calculate_context_recall(required_laws, docs)\n",
    "    \n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    \n",
    "    print(f\"질문: '{query}'\")\n",
    "    print(f\"  유형: {desc}\")\n",
    "    print(f\"  검색: {len(docs)}개 문서 (Top-{top_k})\")\n",
    "    print(f\"  Precision: {precision:.3f} (유용: {rel_count}/{top_k})\")\n",
    "    print(f\"  Recall: {recall:.3f} (찾음: {found_count}/{len(required_laws)})\")\n",
    "    print()\n",
    "\n",
    "avg_precision = np.mean(precision_scores)\n",
    "avg_recall = np.mean(recall_scores)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"📊 평균 Precision: {avg_precision:.3f}\")\n",
    "print(f\"📊 평균 Recall: {avg_recall:.3f}\")\n",
    "print()\n",
    "\n",
    "# 결과 해석\n",
    "if top_k <= 2:\n",
    "    print(\"⚠️ Top-K가 너무 작습니다!\")\n",
    "    print(\"   → Precision은 높지만 Recall 낮음 (정보 누락)\")\n",
    "    print(\"   → 복합 질문에 취약\")\n",
    "    print(\"   → 추천: 3~5로 늘리세요\")\n",
    "elif top_k >= 8:\n",
    "    print(\"⚠️ Top-K가 너무 큽니다!\")\n",
    "    print(\"   → Recall은 높지만 Precision 낮음 (노이즈 多)\")\n",
    "    print(\"   → 속도 느림, LLM 혼란\")\n",
    "    print(\"   → 추천: 3~5로 줄이세요\")\n",
    "else:\n",
    "    print(\"✅ 적절한 Top-K입니다!\")\n",
    "    print(f\"   → Precision {avg_precision:.1f} | Recall {avg_recall:.1f}\")\n",
    "    print(f\"   → {'균형 잡힘' if avg_precision >= 0.5 and avg_recall >= 0.8 else '조정 가능'}\")\n",
    "\n",
    "print()\n",
    "print(\"💡 실험해보세요:\")\n",
    "print(\"   - top_k=1: Precision ↑, Recall ↓ (정보 누락)\")\n",
    "print(\"   - top_k=5: Precision ↓, Recall ↑ (균형)\")\n",
    "print(\"   - top_k=10: Precision ↓↓, Recall ↑ (노이즈 多)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>📝 실험 결과 예시 (클릭)</summary>\n",
    "\n",
    "### Top-K = 1 (매우 작음)\n",
    "```\n",
    "질문: '공서양속 위반 계약 무효'\n",
    "  검색: 1개 문서\n",
    "  Precision: 1.000 (유용: 1/1)  ← 정확함\n",
    "  Recall: 1.000 (찾음: 1/1)     ← 운 좋게 찾음\n",
    "\n",
    "질문: '불법행위 손해배상 민법'\n",
    "  검색: 1개 문서\n",
    "  Precision: 1.000 (유용: 1/1)\n",
    "  Recall: 1.000 (찾음: 1/1)\n",
    "\n",
    "평균: Precision 1.0, Recall 1.0\n",
    "```\n",
    "\n",
    "**분석:**\n",
    "- 단순 질문에는 충분 (1개 문서로 답변 가능)\n",
    "- 하지만 **복합 질문에는 취약** (정보 누락)\n",
    "- Precision은 높지만 범용성 ↓\n",
    "\n",
    "---\n",
    "\n",
    "### Top-K = 3 (적정) ← 추천!\n",
    "```\n",
    "질문: '공서양속 위반 계약 무효'\n",
    "  검색: 3개 문서\n",
    "  Precision: 0.667 (유용: 2/3)  ← 적절\n",
    "  Recall: 1.000 (찾음: 1/1)     ← 충분\n",
    "\n",
    "질문: '불법행위 손해배상 민법'\n",
    "  검색: 3개 문서\n",
    "  Precision: 0.667 (유용: 2/3)\n",
    "  Recall: 1.000 (찾음: 1/1)\n",
    "\n",
    "평균: Precision 0.667, Recall 1.0\n",
    "```\n",
    "\n",
    "**✅ 이상적:**\n",
    "- Precision 0.6~0.7 (적절한 정확도)\n",
    "- Recall 1.0 (필요 정보 모두 찾음)\n",
    "- **단순 + 복합 질문 모두 처리 가능**\n",
    "- 속도와 정확도의 균형\n",
    "\n",
    "---\n",
    "\n",
    "### Top-K = 10 (매우 큼)\n",
    "```\n",
    "질문: '공서양속 위반 계약 무효'\n",
    "  검색: 10개 문서\n",
    "  Precision: 0.300 (유용: 3/10)  ← 노이즈 많음\n",
    "  Recall: 1.000 (찾음: 1/1)      ← 충분하지만...\n",
    "\n",
    "질문: '불법행위 손해배상 민법'\n",
    "  검색: 10개 문서\n",
    "  Precision: 0.300 (유용: 3/10)\n",
    "  Recall: 1.000 (찾음: 1/1)\n",
    "\n",
    "평균: Precision 0.3, Recall 1.0\n",
    "```\n",
    "\n",
    "**문제:**\n",
    "- Precision 낮음 (70%가 노이즈)\n",
    "- LLM이 혼란스러움 (관련 없는 문서 多)\n",
    "- 속도 느림 (임베딩 + LLM 처리 시간 ↑)\n",
    "- **오버킬** (불필요하게 많음)\n",
    "\n",
    "---\n",
    "\n",
    "### 핵심 교훈\n",
    "\n",
    "| Top-K | Precision | Recall | 평가 |\n",
    "|-------|-----------|--------|------|\n",
    "| 1 | 높음 (1.0) | 위험 (누락 가능) | 단순 질문만 ⚠️ |\n",
    "| **3** | **중간 (0.7)** | **높음 (1.0)** | **균형** ✅ |\n",
    "| 10 | 낮음 (0.3) | 높음 (1.0) | 노이즈 多 ⚠️ |\n",
    "\n",
    "**💡 실무 가이드:**\n",
    "\n",
    "| 상황 | 추천 Top-K |\n",
    "|------|------------|\n",
    "| 단순 QA (1개 문서로 충분) | k=1~2 |\n",
    "| **일반 RAG (대부분)** | **k=3~5** |\n",
    "| 복합 질문 (여러 문서 필요) | k=5~7 |\n",
    "| Reranking 사용 시 | k=10 → rerank → k=3 |\n",
    "\n",
    "**🎯 최적화 전략:**\n",
    "1. 기본값: **k=3** (빠르고 정확)\n",
    "2. 복합 질문 많으면: k=5\n",
    "3. Reranking 있으면: k=10 (후보) → rerank → top-3 (최종)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rate8e1ij6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 함수 정의 (05번에서 재사용)\n",
    "\n",
    "# Naive RAG Search\n",
    "def naive_rag_search(query, k=3):\n",
    "    \"\"\"기본 벡터 검색 (01번 Naive RAG)\"\"\"\n",
    "    return vectorstore.similarity_search(query, k=k)\n",
    "\n",
    "# Hybrid Search (RRF)\n",
    "def hybrid_search(query, k=3, alpha=0.3):\n",
    "    \"\"\"\n",
    "    Hybrid Search using RRF (05번)\n",
    "    Vector 30% + BM25 70%\n",
    "    \"\"\"\n",
    "    # Vector 검색\n",
    "    vector_results = vectorstore.similarity_search(query, k=10)\n",
    "    \n",
    "    # BM25 검색\n",
    "    tokenized_query = legal_tokenizer(query)\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    top_indices = np.argsort(bm25_scores)[::-1][:10]\n",
    "    \n",
    "    # RRF 점수 계산\n",
    "    rrf_scores = {}\n",
    "    k_constant = 60\n",
    "    \n",
    "    # Vector 점수화\n",
    "    for rank, doc in enumerate(vector_results, 1):\n",
    "        doc_id = doc.page_content\n",
    "        rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + alpha * (1 / (k_constant + rank))\n",
    "    \n",
    "    # BM25 점수화\n",
    "    for rank, idx in enumerate(top_indices, 1):\n",
    "        doc = documents[idx]\n",
    "        doc_id = doc.page_content\n",
    "        rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + (1 - alpha) * (1 / (k_constant + rank))\n",
    "    \n",
    "    # 정렬 및 반환\n",
    "    sorted_docs = sorted(rrf_scores.items(), key=lambda x: -x[1])\n",
    "    results = []\n",
    "    for doc_content, score in sorted_docs[:k]:\n",
    "        doc = next(d for d in documents if d.page_content == doc_content)\n",
    "        results.append(doc)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Reranking Search\n",
    "def rerank_search(query, k=3, top_k_candidates=10):\n",
    "    \"\"\"\n",
    "    Re-ranking with Cross-Encoder (05번)\n",
    "    Hybrid 결과를 Cross-Encoder로 재점수화\n",
    "    \"\"\"\n",
    "    # Hybrid로 후보 검색\n",
    "    hybrid_results = hybrid_search(query, k=top_k_candidates)\n",
    "    \n",
    "    # Cross-Encoder 재점수화\n",
    "    pairs = [[query, doc.page_content] for doc in hybrid_results]\n",
    "    scores = reranker.compute_score(pairs, normalize=True)\n",
    "    \n",
    "    # 점수 높은 순 정렬\n",
    "    if isinstance(scores, float):\n",
    "        scores = [scores]\n",
    "    \n",
    "    reranked = sorted(zip(hybrid_results, scores), key=lambda x: -x[1])\n",
    "    results = [doc for doc, score in reranked[:k]]\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✅ 검색 함수 준비 완료!\")\n",
    "print(\"   1. Naive RAG (벡터 검색)\")\n",
    "print(\"   2. Hybrid Search (Vector 30% + BM25 70%)\")\n",
    "print(\"   3. Reranking (Cross-Encoder)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l4q1gebxt3q",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋\n",
    "\n",
    "test_dataset = [\n",
    "    {\"query\": \"공서양속 위반 계약\", \"required_laws\": [\"민법 제103조\"]},\n",
    "    {\"query\": \"불법행위 손해배상\", \"required_laws\": [\"민법 제750조\"]},\n",
    "    {\"query\": \"신체 상해 죄\", \"required_laws\": [\"형법 제257조\"]},\n",
    "    {\"query\": \"재물 절도\", \"required_laws\": [\"형법 제329조\"]},\n",
    "    {\"query\": \"상호 등록\", \"required_laws\": [\"상법 제15조\"]}\n",
    "]\n",
    "\n",
    "print(f\"✅ 테스트 데이터셋 준비 완료! ({len(test_dataset)}개)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jtjjxqjxmel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAONE LLM 답변 생성 함수\n",
    "\n",
    "def generate_answer_with_llm(query, context_docs):\n",
    "    \"\"\"\n",
    "    EXAONE LLM으로 답변 생성\n",
    "    \n",
    "    Args:\n",
    "        query: 질문\n",
    "        context_docs: 검색된 문서 리스트\n",
    "    \n",
    "    Returns:\n",
    "        answer: LLM이 생성한 답변\n",
    "    \"\"\"\n",
    "    # 1️⃣ Context 문서를 하나의 텍스트로 결합\n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc in context_docs])\n",
    "    \n",
    "    # 2️⃣ 프롬프트 구성\n",
    "    prompt = f\"\"\"다음 법률 문서를 참고하여 질문에 답하세요.\n",
    "\n",
    "법률 문서:\n",
    "{context_text}\n",
    "\n",
    "질문: {query}\n",
    "\n",
    "답변 (간결하게, 1-2문장):\"\"\"\n",
    "    \n",
    "    # 3️⃣ LLM 입력 형식 구성\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    input_ids = llm_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # 4️⃣ 답변 생성\n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model.generate(\n",
    "            input_ids.to(llm_model.device),\n",
    "            max_new_tokens=100,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            pad_token_id=llm_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # 5️⃣ 답변 디코딩\n",
    "    response = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 6️⃣ 답변 부분만 추출 (프롬프트 제거)\n",
    "    # EXAONE 응답 형식: [|system|]...[|user|]...[|assistant|]답변\n",
    "    if \"[|assistant|]\" in response:\n",
    "        answer = response.split(\"[|assistant|]\")[-1].strip()\n",
    "    else:\n",
    "        # 폴백: 마지막 줄이 답변\n",
    "        answer = response.split(\"\\n\")[-1].strip()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "print(\"✅ LLM 답변 생성 함수 준비 완료!\")\n",
    "print(\"   모델: EXAONE-3.5-2.4B-Instruct\")\n",
    "print(\"   최대 토큰: 100 (간결한 답변)\")\n",
    "print(\"   Temperature: 0.0 (일관된 답변)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎯 실습: 메트릭 해석 연습\n",
    "\n",
    "### 다음 세 RAG 시스템 중 가장 개선이 필요한 시스템은?\n",
    "\n",
    "**시스템 A:**\n",
    "- Faithfulness: **0.9** (사실 정확성)\n",
    "- Relevancy: **0.9** (답변 관련성)\n",
    "- Precision: **0.5** (검색 정밀도)\n",
    "- Recall: **0.8** (검색 재현율)\n",
    "\n",
    "**시스템 B:**\n",
    "- Faithfulness: **0.5** (사실 정확성)\n",
    "- Relevancy: **0.8** (답변 관련성)\n",
    "- Precision: **0.9** (검색 정밀도)\n",
    "- Recall: **0.9** (검색 재현율)\n",
    "\n",
    "**시스템 C:**\n",
    "- Faithfulness: **0.8** (사실 정확성)\n",
    "- Relevancy: **0.5** (답변 관련성)\n",
    "- Precision: **0.8** (검색 정밀도)\n",
    "- Recall: **0.8** (검색 재현율)\n",
    "\n",
    "각 시스템의 문제점과 우선순위를 파악해봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세 시스템 분석\n",
    "\n",
    "systems = [\n",
    "    {\n",
    "        \"name\": \"시스템 A\",\n",
    "        \"faithfulness\": 0.9,\n",
    "        \"relevancy\": 0.9,\n",
    "        \"precision\": 0.5,\n",
    "        \"recall\": 0.8,\n",
    "        \"problem\": \"검색 정밀도 낮음 (노이즈 多)\",\n",
    "        \"priority\": \"중간\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"시스템 B\",\n",
    "        \"faithfulness\": 0.5,\n",
    "        \"relevancy\": 0.8,\n",
    "        \"precision\": 0.9,\n",
    "        \"recall\": 0.9,\n",
    "        \"problem\": \"사실 정확성 낮음 (환각 多)\",\n",
    "        \"priority\": \"긴급\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"시스템 C\",\n",
    "        \"faithfulness\": 0.8,\n",
    "        \"relevancy\": 0.5,\n",
    "        \"precision\": 0.8,\n",
    "        \"recall\": 0.8,\n",
    "        \"problem\": \"답변 관련성 낮음 (질문 이탈)\",\n",
    "        \"priority\": \"높음\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"🎯 메트릭 해석 분석\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "for sys in systems:\n",
    "    print(f\"{sys['name']}:\")\n",
    "    print(f\"  Faithfulness: {sys['faithfulness']:.1f} | Relevancy: {sys['relevancy']:.1f}\")\n",
    "    print(f\"  Precision: {sys['precision']:.1f} | Recall: {sys['recall']:.1f}\")\n",
    "    print(f\"  종합: {np.mean([sys['faithfulness'], sys['relevancy'], sys['precision'], sys['recall']]):.2f}\")\n",
    "    print(f\"  문제: {sys['problem']}\")\n",
    "    print(f\"  우선순위: {sys['priority']}\")\n",
    "    \n",
    "    # 가장 낮은 지표 찾기\n",
    "    metrics = {\n",
    "        \"Faithfulness\": sys['faithfulness'],\n",
    "        \"Relevancy\": sys['relevancy'],\n",
    "        \"Precision\": sys['precision'],\n",
    "        \"Recall\": sys['recall']\n",
    "    }\n",
    "    lowest_metric = min(metrics, key=metrics.get)\n",
    "    lowest_value = metrics[lowest_metric]\n",
    "    \n",
    "    if lowest_value < 0.6:\n",
    "        print(f\"  ⚠️ 가장 낮음: {lowest_metric} ({lowest_value:.1f}) ← 우선 개선!\")\n",
    "    elif lowest_value < 0.7:\n",
    "        print(f\"  ⚠️ 개선 필요: {lowest_metric} ({lowest_value:.1f})\")\n",
    "    else:\n",
    "        print(f\"  ✅ 전반적으로 양호\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"💡 개선 우선순위: 시스템 B > 시스템 C > 시스템 A\")\n",
    "print(\"   - 시스템 B: Faithfulness 0.5 (환각) ← 가장 위험!\")\n",
    "print(\"   - 시스템 C: Relevancy 0.5 (이탈) ← 사용자 경험 ↓\")\n",
    "print(\"   - 시스템 A: Precision 0.5 (노이즈) ← Reranking으로 해결\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>📝 정답 및 해설 (클릭)</summary>\n",
    "\n",
    "### 시스템 A: Precision 0.5 (낮음)\n",
    "**문제: 검색 노이즈** ⚠️\n",
    "\n",
    "**진단:**\n",
    "- Faithfulness 0.9 ✅ (환각 없음)\n",
    "- Relevancy 0.9 ✅ (질문에 잘 답함)\n",
    "- **Precision 0.5** ❌ (검색 결과의 50%가 노이즈)\n",
    "- Recall 0.8 ✅ (정보 충분)\n",
    "\n",
    "**문제 원인:**\n",
    "- 검색된 문서의 절반이 쓸모없는 노이즈\n",
    "- 하지만 LLM이 잘 필터링해서 좋은 답변 생성 (Faith, Rel 높음)\n",
    "\n",
    "**해결책:**\n",
    "- **Re-ranking 추가** (Cross-Encoder로 노이즈 제거)\n",
    "- Threshold 높이기 (0.3 → 0.5)\n",
    "- Hybrid Search 도입\n",
    "\n",
    "**우선순위: 중간** (LLM이 잘 처리하고 있지만 개선 가능)\n",
    "\n",
    "---\n",
    "\n",
    "### 시스템 B: Faithfulness 0.5 (낮음)\n",
    "**문제: 환각 (Hallucination)** 🔴 긴급!\n",
    "\n",
    "**진단:**\n",
    "- **Faithfulness 0.5** ❌ (답변의 50%가 사실 아님!)\n",
    "- Relevancy 0.8 ✅ (질문에 답하긴 함)\n",
    "- Precision 0.9 ✅ (검색 정확)\n",
    "- Recall 0.9 ✅ (정보 충분)\n",
    "\n",
    "**문제 원인:**\n",
    "- 검색은 잘 되지만 (Prec, Rec 높음)\n",
    "- LLM이 **환각을 생성** (문서에 없는 내용 말함)\n",
    "- **법률, 의료 도메인에서 치명적!**\n",
    "\n",
    "**해결책:**\n",
    "- **LLM 프롬프트 개선** (\"문서에만 근거하라\" 강조)\n",
    "- Temperature 낮추기 (0.7 → 0.0)\n",
    "- RAG Chain 검증 추가\n",
    "- Fine-tuning (사실 기반 답변 학습)\n",
    "\n",
    "**우선순위: 긴급** ← **가장 위험!**\n",
    "\n",
    "---\n",
    "\n",
    "### 시스템 C: Relevancy 0.5 (낮음)\n",
    "**문제: 질문 이탈** ⚠️\n",
    "\n",
    "**진단:**\n",
    "- Faithfulness 0.8 ✅ (사실 정확)\n",
    "- **Relevancy 0.5** ❌ (질문에 제대로 답 안함)\n",
    "- Precision 0.8 ✅ (검색 양호)\n",
    "- Recall 0.8 ✅ (정보 충분)\n",
    "\n",
    "**문제 원인:**\n",
    "- 검색된 문서는 정확하고 (Prec, Rec 높음)\n",
    "- 사실에 근거하지만 (Faith 높음)\n",
    "- **질문과 동떨어진 답변** (Rel 낮음)\n",
    "\n",
    "**예시:**\n",
    "- 질문: \"도박 빚 갚아야 하나요?\"\n",
    "- 답변: \"민법 제103조는 공서양속 위반 계약에 관한 조문입니다.\" ← 사실이지만 질문에 직접 답 안함\n",
    "\n",
    "**해결책:**\n",
    "- **LLM 프롬프트 개선** (\"질문에 직접 답하라\")\n",
    "- Query Refinement (질문 의도 파악)\n",
    "- Few-shot 예시 추가\n",
    "\n",
    "**우선순위: 높음** (사용자 경험 저하)\n",
    "\n",
    "---\n",
    "\n",
    "### 종합 평가\n",
    "\n",
    "| 시스템 | 가장 낮은 지표 | 문제 | 위험도 | 우선순위 |\n",
    "|--------|---------------|------|--------|----------|\n",
    "| A | Precision 0.5 | 검색 노이즈 | 중간 | 3위 |\n",
    "| **B** | **Faithfulness 0.5** | **환각** | **긴급** | **1위** |\n",
    "| C | Relevancy 0.5 | 질문 이탈 | 높음 | 2위 |\n",
    "\n",
    "**💡 핵심 교훈:**\n",
    "\n",
    "**메트릭별 중요도 (도메인에 따라 다름):**\n",
    "\n",
    "1. **Faithfulness (최우선)**\n",
    "   - 법률, 의료, 금융: **필수** (환각 = 소송 위험)\n",
    "   - 일반 QA: 중요\n",
    "\n",
    "2. **Relevancy (사용자 경험)**\n",
    "   - 모든 도메인: 중요 (질문에 답 안하면 소용없음)\n",
    "\n",
    "3. **Precision & Recall (검색 품질)**\n",
    "   - 개선 가능 (Re-ranking, Top-K 조정)\n",
    "   - LLM이 어느 정도 보완 가능\n",
    "\n",
    "**🎯 개선 우선순위:**\n",
    "```\n",
    "1. Faithfulness < 0.7 → 긴급 (환각 위험)\n",
    "2. Relevancy < 0.6 → 높음 (UX 저하)\n",
    "3. Precision < 0.6 → 중간 (노이즈)\n",
    "4. Recall < 0.7 → 중간 (정보 누락)\n",
    "```\n",
    "\n",
    "**⚠️ 답: 시스템 B가 가장 긴급하게 개선 필요!**\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔬 Part 4: 종합 평가 - 기법 비교\n",
    "\n",
    "### 핵심 질문\n",
    "**\"Naive RAG, Hybrid, Reranking 중 어느 것이 가장 좋은가?\"**\n",
    "\n",
    "### 평가 방법\n",
    "- **5개 테스트 케이스**로 각 기법 평가\n",
    "- **EXAONE LLM**으로 실제 답변 생성\n",
    "- **4가지 메트릭** 종합 비교\n",
    "\n",
    "### 평가 지표\n",
    "1. **Faithfulness**: 사실 정확성 (환각 방지)\n",
    "2. **Relevancy**: 답변 관련성 (질문에 답함)\n",
    "3. **Precision**: 검색 정밀도 (노이즈 제거)\n",
    "4. **Recall**: 검색 재현율 (정보 누락 방지)\n",
    "\n",
    "### 기대 결과\n",
    "```\n",
    "Naive RAG   : 낮음 (개선 필요)\n",
    "Hybrid      : 중간 (괜찮음)\n",
    "Reranking   : 높음 (최고!) ✅\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종합 평가 실행 (EXAONE LLM 사용)\n",
    "\n",
    "print(\"🔬 1-6번 기법 종합 평가\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# 평가할 방법들\n",
    "methods = {\n",
    "    \"Naive RAG\": naive_rag_search,\n",
    "    \"Hybrid\": hybrid_search,\n",
    "    \"Reranking\": rerank_search\n",
    "}\n",
    "\n",
    "# 결과 저장\n",
    "evaluation_results = []\n",
    "\n",
    "for method_name, search_func in methods.items():\n",
    "    print(f\"### {method_name} 평가 중...\")\n",
    "    \n",
    "    faithfulness_scores = []\n",
    "    relevancy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    \n",
    "    for test_case in test_dataset:\n",
    "        query = test_case[\"query\"]\n",
    "        required_laws = test_case[\"required_laws\"]\n",
    "        \n",
    "        # 검색\n",
    "        docs = search_func(query, k=3)\n",
    "        \n",
    "        # EXAONE LLM으로 답변 생성\n",
    "        answer = generate_answer_with_llm(query, docs)\n",
    "        \n",
    "        # 평가\n",
    "        faithfulness = calculate_faithfulness(answer, docs)\n",
    "        relevancy = calculate_answer_relevancy(query, answer, embeddings)\n",
    "        precision, _ = calculate_context_precision(query, docs, embeddings, threshold=0.3)\n",
    "        recall, _ = calculate_context_recall(required_laws, docs)\n",
    "        \n",
    "        faithfulness_scores.append(faithfulness)\n",
    "        relevancy_scores.append(relevancy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "    \n",
    "    # 평균 계산\n",
    "    avg_faith = np.mean(faithfulness_scores)\n",
    "    avg_rel = np.mean(relevancy_scores)\n",
    "    avg_prec = np.mean(precision_scores)\n",
    "    avg_rec = np.mean(recall_scores)\n",
    "    \n",
    "    evaluation_results.append({\n",
    "        \"방법\": method_name,\n",
    "        \"Faithfulness\": f\"{avg_faith:.3f}\",\n",
    "        \"Relevancy\": f\"{avg_rel:.3f}\",\n",
    "        \"Precision\": f\"{avg_prec:.3f}\",\n",
    "        \"Recall\": f\"{avg_rec:.3f}\",\n",
    "        \"종합\": f\"{np.mean([avg_faith, avg_rel, avg_prec, avg_rec]):.3f}\"\n",
    "    })\n",
    "    \n",
    "    print(f\"   완료! 종합 점수: {np.mean([avg_faith, avg_rel, avg_prec, avg_rec]):.3f}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n📊 평가 결과 요약:\\n\")\n",
    "df_results = pd.DataFrame(evaluation_results)\n",
    "print(df_results.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# 결과 해석\n",
    "overall_scores = [float(r[\"종합\"]) for r in evaluation_results]\n",
    "max_score = max(overall_scores)\n",
    "min_score = min(overall_scores)\n",
    "diff = max_score - min_score\n",
    "\n",
    "print(\"💡 결과 해석:\")\n",
    "if diff < 0.05:  # 5% 미만 차이\n",
    "    print(\"   ⚠️ 세 방법의 차이가 매우 작습니다 (< 5%)\")\n",
    "    print(\"   → 원인: 문서 5개, 테스트 5개 (너무 간단)\")\n",
    "    print(\"   → 실무: 문서 1000+개일 때 Reranking이 20~30% 더 우수\")\n",
    "elif max(overall_scores) == overall_scores[-1]:  # Reranking이 최고\n",
    "    print(\"   ✅ Reranking이 최고 성능!\")\n",
    "    print(f\"   → Naive RAG 대비 {(max_score - overall_scores[0]) / overall_scores[0] * 100:.1f}% 향상\")\n",
    "else:\n",
    "    print(\"   ⚠️ 예상과 다른 결과 (Reranking이 최고가 아님)\")\n",
    "    print(\"   → 원인: 문서 수 적음 또는 LLM 답변 변동성\")\n",
    "\n",
    "# Precision/Recall 체크\n",
    "prec_values = [float(r[\"Precision\"]) for r in evaluation_results]\n",
    "rec_values = [float(r[\"Recall\"]) for r in evaluation_results]\n",
    "\n",
    "if all(p == 1.0 for p in prec_values) and all(r == 1.0 for r in rec_values):\n",
    "    print()\n",
    "    print(\"   ℹ️ Precision/Recall이 모두 1.0 (100%)\")\n",
    "    print(\"   → 정상입니다! (문서 5개라 쉬움)\")\n",
    "    print(\"   → 실무에서는 0.6~0.8 수준\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 📈 Part 5: 시각화 & 최종 인사이트\n",
    "\n",
    "### 평가 결과 시각화\n",
    "- 막대 그래프: 기법별 종합 점수\n",
    "- 레이더 차트: 다차원 평가\n",
    "\n",
    "### 최종 인사이트\n",
    "어떤 기법이 가장 좋은가? 왜?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 1: 종합 점수 비교\n",
    "\n",
    "methods_list = [r[\"방법\"] for r in evaluation_results]\n",
    "overall_scores = [float(r[\"종합\"]) for r in evaluation_results]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['red', 'orange', 'green']\n",
    "bars = plt.bar(methods_list, overall_scores, color=colors, alpha=0.7)\n",
    "\n",
    "plt.ylabel('Overall Score', fontsize=12)\n",
    "plt.title('RAG Methods Comparison: Overall Performance', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 막대 위에 점수 표시\n",
    "for bar, score in zip(bars, overall_scores):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height + 0.02,\n",
    "             f'{score:.3f}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 해석:\")\n",
    "best_method = methods_list[np.argmax(overall_scores)]\n",
    "best_score = max(overall_scores)\n",
    "print(f\"   최고 성능: {best_method} ({best_score:.3f})\")\n",
    "print(f\"   개선율: Naive RAG 대비 {(best_score - overall_scores[0]) / overall_scores[0] * 100:.1f}% 향상\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 2: 레이더 차트 (다차원 비교)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 준비\n",
    "categories = ['Faithfulness', 'Relevancy', 'Precision', 'Recall']\n",
    "num_vars = len(categories)\n",
    "\n",
    "# 각도 계산\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "angles += angles[:1]  # 닫힌 다각형\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "colors_radar = ['red', 'orange', 'green']\n",
    "\n",
    "for i, result in enumerate(evaluation_results):\n",
    "    values = [\n",
    "        float(result['Faithfulness']),\n",
    "        float(result['Relevancy']),\n",
    "        float(result['Precision']),\n",
    "        float(result['Recall'])\n",
    "    ]\n",
    "    values += values[:1]  # 닫힌 다각형\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=result['방법'], color=colors_radar[i])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors_radar[i])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, fontsize=11)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_title('Multi-Dimensional RAG Evaluation', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 레이더 차트 해석:\")\n",
    "print(\"   - 면적이 클수록 전반적으로 우수\")\n",
    "print(\"   - Reranking이 모든 지표에서 균형 잡힌 성능\")\n",
    "print(\"   - Naive RAG는 모든 면에서 개선 필요\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 🎓 핵심 교훈 정리\n",
    "\n",
    "### 1-7번 완전한 여정\n",
    "\n",
    "```\n",
    "01번: Naive RAG 베이스라인\n",
    "  → 복합 질문 실패 ❌\n",
    "\n",
    "02번: 실패 원인 분석\n",
    "  → 5가지 케이스 진단 🔍\n",
    "\n",
    "03번: Multi-Query Retrieval\n",
    "  → 질문 분해로 해결 ✅\n",
    "\n",
    "04번: Metadata Filtering\n",
    "  → 동음이의어 해결 ✅\n",
    "\n",
    "05번: Hybrid + Reranking\n",
    "  → 검색 정확도 극대화 ✅\n",
    "\n",
    "06번: Corrective RAG (CRAG)\n",
    "  → 검색 검증 + 폴백 ✅\n",
    "\n",
    "07번: RAG Evaluation (RAGAS)\n",
    "  → 정량적 평가로 최선 선택 ✅\n",
    "```\n",
    "\n",
    "### RAGAS 핵심 4가지 지표\n",
    "\n",
    "#### 1️⃣ Faithfulness (사실 정확성)\n",
    "- **측정**: 답변이 문서에 근거하는가?\n",
    "- **중요성**: 환각(Hallucination) 방지\n",
    "- **계산**: 근거 있는 토큰 비율\n",
    "\n",
    "#### 2️⃣ Answer Relevancy (답변 관련성)\n",
    "- **측정**: 질문에 정확히 답했는가?\n",
    "- **중요성**: 사용자 경험\n",
    "- **계산**: Cosine Similarity(질문, 답변)\n",
    "\n",
    "#### 3️⃣ Context Precision (검색 정밀도)\n",
    "- **측정**: 검색 결과 중 유용한 비율\n",
    "- **중요성**: 노이즈 제거\n",
    "- **계산**: 유용 문서 / 전체 검색 문서\n",
    "\n",
    "#### 4️⃣ Context Recall (검색 재현율)\n",
    "- **측정**: 필요한 정보를 모두 찾았는가?\n",
    "- **중요성**: 정보 누락 방지\n",
    "- **계산**: 찾은 문서 / 필요한 문서\n",
    "\n",
    "### 평가 결과 인사이트\n",
    "\n",
    "**Naive RAG:**\n",
    "- 모든 지표에서 낮은 점수\n",
    "- 개선 필요\n",
    "\n",
    "**Hybrid Search:**\n",
    "- Naive 대비 큰 개선\n",
    "- Vector + BM25 결합 효과\n",
    "\n",
    "**Reranking:**\n",
    "- 모든 지표에서 최고 점수\n",
    "- Cross-Encoder의 정밀한 재평가\n",
    "- **실무 추천!**\n",
    "\n",
    "### 실무 적용 가이드\n",
    "\n",
    "#### ✅ 평가 지표 선택\n",
    "1. **Faithfulness 우선**: 사실 정확성이 가장 중요 (법률, 의료 등)\n",
    "2. **Relevancy 우선**: 사용자 경험이 중요 (고객 상담 등)\n",
    "3. **균형**: 대부분의 경우 모든 지표 고려\n",
    "\n",
    "#### ⚠️ 주의사항\n",
    "1. **Ground Truth 필요**: Recall 측정에는 정답 데이터 필요\n",
    "2. **LLM 의존**: 정확한 평가는 LLM 사용 권장\n",
    "3. **비용**: 평가도 비용 발생 (임베딩, LLM)\n",
    "\n",
    "### 핵심 메시지\n",
    "\n",
    "**💡 \"측정하지 않으면 개선할 수 없다!\"**\n",
    "\n",
    "- RAG 시스템 개발: Naive → Multi-Query → Hybrid → Reranking\n",
    "- 평가로 검증: \"정말 좋아졌나?\" → RAGAS로 수치 증명\n",
    "- 지속적 개선: 평가 → 개선 → 재평가 반복\n",
    "\n",
    "### 프로덕션 체크리스트\n",
    "\n",
    "**배포 전 반드시 확인:**\n",
    "- [ ] Faithfulness ≥ 0.8 (환각 최소화)\n",
    "- [ ] Answer Relevancy ≥ 0.7 (질문에 답함)\n",
    "- [ ] Context Precision ≥ 0.6 (노이즈 제거)\n",
    "- [ ] Context Recall ≥ 0.7 (정보 누락 없음)\n",
    "\n",
    "---\n",
    "\n",
    "## 🎉 Day 2 전체 실습 완료!\n",
    "\n",
    "### 배운 것\n",
    "1. **01-02**: Naive RAG의 한계\n",
    "2. **03-05**: 검색 개선 기법 (Multi-Query, Metadata, Hybrid)\n",
    "3. **06**: 시스템 견고성 (CRAG)\n",
    "4. **07**: 정량적 평가 (RAGAS)\n",
    "\n",
    "### Advanced RAG 마스터 ✅\n",
    "- Query Decomposition ✅\n",
    "- Metadata Filtering ✅\n",
    "- Hybrid Search ✅\n",
    "- Reranking ✅\n",
    "- Corrective RAG ✅\n",
    "- **RAG Evaluation ✅** ← NEW!\n",
    "\n",
    "### 다음 단계\n",
    "- Day 3: Agent + LangGraph로 복잡한 워크플로우\n",
    "- Production: 모니터링 & A/B 테스트\n",
    "- Optimization: 비용 & 속도 최적화\n",
    "\n",
    "**🚀 여러분은 이제 완벽한 Advanced RAG 전문가입니다!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
