{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Day 1 ì‹¤ìŠµ 3: EXAONE ëª¨ë¸ QLoRA íŒŒì¸íŠœë‹\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- EXAONE-3.5-2.4B-Instruct ëª¨ë¸ì„ í™œìš©í•œ ê²½ëŸ‰ íŒŒì¸íŠœë‹\n",
    "- QLoRA (4-bit ì–‘ìí™” + LoRA) ê¸°ë²• ì ìš©\n",
    "- Colab ë¬´ë£Œ í™˜ê²½ì—ì„œì˜ íš¨ìœ¨ì  í•™ìŠµ\n",
    "- GPU ë©”ëª¨ë¦¬ ìµœì í™” ë° ëª¨ë‹ˆí„°ë§\n",
    "- ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë° ê´€ë¦¬\n",
    "\n",
    "## ì‹œê°„: 14:40â€“16:10 (90ë¶„)\n",
    "\n",
    "### ğŸ’¡ QLoRAë€?\n",
    "- **Q**: Quantization (4-bit ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 75% ê°ì†Œ)\n",
    "- **LoRA**: Low-Rank Adaptation (íŒŒë¼ë¯¸í„°ì˜ 0.3%ë§Œ í•™ìŠµ)\n",
    "- **ê²°ê³¼**: ê¸°ì¡´ ëŒ€ë¹„ 1/4 ë©”ëª¨ë¦¬ë¡œ ë¹„ìŠ·í•œ ì„±ëŠ¥ ë‹¬ì„±!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Day 1 ì‹¤ìŠµ 3: QLoRA íŒŒì¸íŠœë‹ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° í™•ì¸\nprint(\"ğŸš€ Day 1 ì‹¤ìŠµ 3: EXAONE ëª¨ë¸ QLoRA íŒŒì¸íŠœë‹\")\nprint(\"ğŸ” íŒŒì¸íŠœë‹ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° í™•ì¸ ì¤‘...\")\n\n# 01, 02ë²ˆì—ì„œ ì„¤ì¹˜ë˜ì§€ ì•Šì€ íŒŒì¸íŠœë‹ ì „ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ë§Œ ì„¤ì¹˜\nprint(\"ğŸ“¦ íŒŒì¸íŠœë‹ ì „ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜...\")\n\n# bitsandbytes ë° PEFT ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ (ë ˆê±°ì‹œ ê²€ì¦ëœ ë°©ë²• ì‚¬ìš©)\n!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n\n# í—ˆê¹…í˜ì´ìŠ¤ Hub ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€ (ëª¨ë¸ ì—…ë¡œë“œìš©)\n!pip install -q -U huggingface_hub\n\n# ê¸°íƒ€ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ (ì´ë¯¸ ì„¤ì¹˜ëœ ê²½ìš° ìŠ¤í‚µë¨)\n!pip install -q datasets jsonlines tqdm\n\nprint(\"âœ… íŒŒì¸íŠœë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!\")\n\n# ì„¤ì¹˜ ê²€ì¦\nprint(\"ğŸ§ª bitsandbytes ê²€ì¦ ì¤‘...\")\ntry:\n    import bitsandbytes as bnb\n    import torch\n    print(f\"âœ… bitsandbytes ë²„ì „: {bnb.__version__}\")\n    \n    if torch.cuda.is_available():\n        print(f\"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}\")\n        # ê°„ë‹¨í•œ CUDA í…ŒìŠ¤íŠ¸\n        test_tensor = torch.randn(10, 10).cuda()\n        print(\"âœ… CUDA í…ì„œ ìƒì„± ì„±ê³µ\")\n    else:\n        print(\"âš ï¸ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n\n    # í—ˆê¹…í˜ì´ìŠ¤ Hub í™•ì¸\n    import huggingface_hub\n    print(f\"âœ… huggingface_hub ë²„ì „: {huggingface_hub.__version__}\")\n        \n    print(\"ğŸ‰ íŒŒì¸íŠœë‹ í™˜ê²½ ì¤€ë¹„ ì™„ë£Œ!\")\n    \nexcept Exception as e:\n    print(f\"âŒ ê²€ì¦ ì‹¤íŒ¨: {e}\")\n    print(\"ğŸ’¡ í•´ê²° ë°©ë²•:\")\n    print(\"1. ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ë‹¤ì‹œ ì‹œì‘\")\n    print(\"2. ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ë³´ì„¸ìš”\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ğŸ“ ì°¸ê³ : 01, 02ë²ˆ ë…¸íŠ¸ë¶ì—ì„œ ì´ë¯¸ ì„¤ì¹˜ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì¬ì‚¬ìš©ë©ë‹ˆë‹¤\")\nprint(\"ğŸ”„ ë‹¤ìŒ ì…€ë¶€í„° íŒŒì¸íŠœë‹ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport jsonlines\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom typing import Dict, List, Any, Optional\nimport warnings\nfrom tqdm import tqdm\n\n# Transformers ê´€ë ¨\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer,\n    TrainerCallback,\n    DataCollatorForLanguageModeling\n)\n\n# PEFT ê´€ë ¨\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n    TaskType,\n    PeftModel\n)\n\n# Dataset\nfrom datasets import Dataset\n\n# í—ˆê¹…í˜ì´ìŠ¤ Hub\nfrom huggingface_hub import HfApi, login, create_repo\n\nwarnings.filterwarnings('ignore')\n\n# í•œê¸€ í°íŠ¸ ì„¤ì • (matplotlib) - í•™ìŠµ ê²°ê³¼ ë¶„ì„ ì°¨íŠ¸ì—ì„œ í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šë„ë¡ ì„¤ì •\n# íŒŒì¸íŠœë‹ ê³¼ì •ì˜ Loss ê³¡ì„ , GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë“±ì˜ ì°¨íŠ¸ì—ì„œ í•œê¸€ í‘œì‹œë¥¼ ìœ„í•´ í•„ìš”\nprint(\"ğŸ”§ í•œê¸€ í°íŠ¸ ì„¤ì • ì¤‘...\")\n!apt-get update -qq\n!apt-get install fonts-nanum -qq > /dev/null\n\nimport matplotlib.font_manager as fm\n\n# ë‚˜ëˆ”ë°”ë¥¸ê³ ë”• í°íŠ¸ ê²½ë¡œ ì„¤ì •\nfontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n# í°íŠ¸ ë§¤ë‹ˆì €ì— í°íŠ¸ ì¶”ê°€ - í•™ìŠµ ë¶„ì„ ê·¸ë˜í”„ì—ì„œ í•œê¸€ í‘œì‹œë¥¼ ìœ„í•´ í•„ìš”\nfm.fontManager.addfont(fontpath)\n\n# matplotlib ì„¤ì • ì—…ë°ì´íŠ¸ - í•™ìŠµ ê³¼ì • ì‹œê°í™”ì—ì„œ í•œê¸€ì´ ì •ìƒì ìœ¼ë¡œ í‘œì‹œë¨\nplt.rcParams.update({\n    'font.family': 'NanumBarunGothic',  # ê¸°ë³¸ í°íŠ¸ë¥¼ ë‚˜ëˆ”ë°”ë¥¸ê³ ë”•ìœ¼ë¡œ ì„¤ì •\n    'axes.unicode_minus': False         # ìŒìˆ˜ ê¸°í˜¸ í‘œì‹œ ë¬¸ì œ í•´ê²° (Loss ê°’ í‘œì‹œì—ì„œ ì¤‘ìš”)\n})\n\nprint(\"âœ… í•œê¸€ í°íŠ¸ ì„¤ì • ì™„ë£Œ - í•™ìŠµ ê²°ê³¼ ì°¨íŠ¸ì—ì„œ í•œê¸€ì´ ì •ìƒ í‘œì‹œë©ë‹ˆë‹¤\")\n\n# GPU ì •ë³´ í™•ì¸ - íŒŒì¸íŠœë‹ì— í•„ìš”í•œ í•˜ë“œì›¨ì–´ í™˜ê²½ ì ê²€\nprint(f\"ğŸ”§ íŒŒì¸íŠœë‹ í™˜ê²½ ì •ë³´:\")\nprint(f\"PyTorch ë²„ì „: {torch.__version__}\")\nprint(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n    for i in range(torch.cuda.device_count()):\n        gpu_name = torch.cuda.get_device_name(i)\n        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n        print(f\"GPU {i}: {gpu_name}\")\n        print(f\"  ì „ì²´ ë©”ëª¨ë¦¬: {gpu_memory:.1f} GB\")\n        \n        # í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ë„ í‘œì‹œ\n        allocated = torch.cuda.memory_allocated(i) / 1024**3\n        print(f\"  í˜„ì¬ ì‚¬ìš© ì¤‘: {allocated:.1f} GB\")\nelse:\n    print(\"âš ï¸ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤ (ë§¤ìš° ëŠë¦¼).\")\n    print(\"   GPU í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\")\n\nprint(\"\\nâœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ import ë° í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "\n",
    "### ğŸ¤– EXAONE-3.5-2.4B-Instruct ëª¨ë¸\n",
    "- **ê°œë°œì‚¬**: LG AI Research\n",
    "- **í¬ê¸°**: 2.4B íŒŒë¼ë¯¸í„°\n",
    "- **íŠ¹ì§•**: í•œêµ­ì–´ íŠ¹í™”, Instruction Following ìµœì í™”\n",
    "- **ì¥ì **: Colab ë¬´ë£Œ í™˜ê²½ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì„¤ì •\n",
    "MODEL_NAME = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n",
    "MAX_LENGTH = 4096  # ìµœëŒ€ í† í° ê¸¸ì´\n",
    "\n",
    "print(f\"ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘: {MODEL_NAME}\")\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "print(\"ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë“œ...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# íŒ¨ë”© í† í° ì„¤ì • (ì—†ëŠ” ê²½ìš°)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"âœ… íŒ¨ë”© í† í°ì„ EOS í† í°ìœ¼ë¡œ ì„¤ì •\")\n",
    "\n",
    "print(f\"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"  ì–´íœ˜ í¬ê¸°: {tokenizer.vocab_size:,}\")\n",
    "print(f\"  íŒ¨ë”© í† í°: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"  EOS í† í°: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. QLoRA ì„¤ì •\n",
    "\n",
    "### âš™ï¸ 4-bit ì–‘ìí™” ì„¤ì •\n",
    "- **nf4**: 4-bit NormalFloat ë°ì´í„° íƒ€ì… ì‚¬ìš©\n",
    "- **double_quant**: ì–‘ìí™” ìƒìˆ˜ë„ ì–‘ìí™” (ë” ë§ì€ ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "- **compute_dtype**: ì—°ì‚° ì‹œ ì‚¬ìš©í•  ë°ì´í„° íƒ€ì…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 4-bit ì–‘ìí™” ì„¤ì •\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,                    # 4-bit ì–‘ìí™” í™œì„±í™”\n    bnb_4bit_use_double_quant=True,       # ì–‘ìí™” ìƒìˆ˜ë„ ì–‘ìí™” (ë” ë§ì€ ë©”ëª¨ë¦¬ ì ˆì•½)\n    bnb_4bit_quant_type=\"nf4\",           # NormalFloat4 ì–‘ìí™” ë°©ì‹\n    bnb_4bit_compute_dtype=torch.bfloat16, # ì—°ì‚°ìš© ë°ì´í„° íƒ€ì…\n)\n\nprint(\"âš™ï¸ 4-bit ì–‘ìí™” ì„¤ì • ì™„ë£Œ:\")\nprint(f\"  ì–‘ìí™” ë°©ì‹: NF4\")\nprint(f\"  Double quantization: í™œì„±í™”\")\nprint(f\"  ì—°ì‚° ë°ì´í„° íƒ€ì…: bfloat16\")\n\n# LoRA ì„¤ì •\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,         # ì–¸ì–´ ëª¨ë¸ë§ íƒœìŠ¤í¬\n    r=8,                                  # LoRA rank (ë‚®ì„ìˆ˜ë¡ íŒŒë¼ë¯¸í„° ì ìŒ)\n    lora_alpha=16,                        # LoRA scaling factor (ë³´í†µ rì˜ 2ë°°)\n    lora_dropout=0.05,                    # LoRA ë“œë¡­ì•„ì›ƒ\n    target_modules=[                      # LoRAë¥¼ ì ìš©í•  ëª¨ë“ˆë“¤\n        \"q_proj\",    # Query projection\n        \"k_proj\",    # Key projection  \n        \"v_proj\",    # Value projection\n        \"out_proj\"   # Output projection\n    ],\n    bias=\"none\",                         # ë°”ì´ì–´ìŠ¤ëŠ” í•™ìŠµí•˜ì§€ ì•ŠìŒ\n)\n\nprint(\"\\nğŸ¯ LoRA ì„¤ì • ì™„ë£Œ:\")\nprint(f\"  Rank (r): {lora_config.r}\")\nprint(f\"  Alpha: {lora_config.lora_alpha}\")\nprint(f\"  Dropout: {lora_config.lora_dropout}\")\nprint(f\"  ëŒ€ìƒ ëª¨ë“ˆ: {lora_config.target_modules}\")\n\n# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ë¹„ìœ¨ ì¶”ì •\ntotal_params_estimate = 2.4e9  # 2.4B íŒŒë¼ë¯¸í„°\nlora_params_estimate = len(lora_config.target_modules) * lora_config.r * 2560 * 2  # ëŒ€ëµì  ì¶”ì •\nlora_ratio = lora_params_estimate / total_params_estimate\n\nprint(f\"\\nğŸ“Š ì˜ˆìƒ í•™ìŠµ íŒŒë¼ë¯¸í„° ë¹„ìœ¨:\")\nprint(f\"  ì „ì²´ íŒŒë¼ë¯¸í„°: ~{total_params_estimate/1e9:.1f}B\")\nprint(f\"  LoRA íŒŒë¼ë¯¸í„°: ~{lora_params_estimate/1e6:.1f}M\")\nprint(f\"  í•™ìŠµ ë¹„ìœ¨: ~{lora_ratio:.3%}\")\nprint(f\"  ë©”ëª¨ë¦¬ ì ˆì•½: ~75% (4-bit ì–‘ìí™”)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ëª¨ë¸ ë¡œë“œ ë° PEFT ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_model_with_qlora(model_name: str, bnb_config: BitsAndBytesConfig, \n                         lora_config: LoraConfig) -> tuple:\n    \"\"\"\n    QLoRAê°€ ì ìš©ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜\n    \n    Returns:\n        (model, trainable_params, total_params)\n    \"\"\"\n    print(\"ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\n    \n    # ëª¨ë¸ ë¡œë“œ (4-bit ì–‘ìí™” ì ìš©)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        device_map=\"auto\",              # ìë™ìœ¼ë¡œ GPUì— ë°°ì¹˜\n        trust_remote_code=True,\n        torch_dtype=torch.bfloat16,\n        # flash_attention_2ëŠ” ì¼ë¶€ í™˜ê²½ì—ì„œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œê±°\n        # attn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else \"eager\"\n    )\n    \n    print(\"âœ… ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n    \n    # Gradient checkpointing í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)\n    model.gradient_checkpointing_enable()\n    \n    # k-bit training ì¤€ë¹„\n    model = prepare_model_for_kbit_training(model)\n    \n    # LoRA ì ìš©\n    print(\"ğŸ”„ LoRA ì ìš© ì¤‘...\")\n    model = get_peft_model(model, lora_config)\n    \n    # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°\n    trainable_params = 0\n    total_params = 0\n    \n    for name, param in model.named_parameters():\n        total_params += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    \n    print(\"âœ… LoRA ì ìš© ì™„ë£Œ\")\n    \n    return model, trainable_params, total_params\n\n# ëª¨ë¸ ì„¤ì • ë° LoRA ì„¤ì •ì€ ì´ì „ ì…€ì—ì„œ ì •ì˜ëœ ë³€ìˆ˜(MODEL_NAME, bnb_config, lora_config)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\nif 'bnb_config' in globals() and 'lora_config' in globals():\n    model, trainable_params, total_params = load_model_with_qlora(\n        MODEL_NAME, bnb_config, lora_config\n    )\n\n    # íŒŒë¼ë¯¸í„° ì •ë³´ ì¶œë ¥\n    print(f\"\\nğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ì •ë³´:\")\n    print(f\"  ì´ íŒŒë¼ë¯¸í„°: {total_params:,} ({total_params/1e9:.2f}B)\")\n    print(f\"  í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,} ({trainable_params/1e6:.2f}M)\")\n    print(f\"  í•™ìŠµ ë¹„ìœ¨: {100 * trainable_params / total_params:.3f}%\")\n\n    # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸\n    if torch.cuda.is_available():\n        print(f\"\\nğŸ’¾ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:\")\n        for i in range(torch.cuda.device_count()):\n            allocated = torch.cuda.memory_allocated(i) / 1024**3\n            cached = torch.cuda.memory_reserved(i) / 1024**3\n            total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n            print(f\"  GPU {i}: {allocated:.1f}GB / {total_memory:.1f}GB (í• ë‹¹ë¨)\")\n            print(f\"          {cached:.1f}GB / {total_memory:.1f}GB (ì˜ˆì•½ë¨)\")\nelse:\n    print(\"âŒ bnb_config ë˜ëŠ” lora_configê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n    print(\"ğŸ’¡ ì´ì „ ì…€ë“¤ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "\n",
    "### ğŸ“ ì „ì²˜ë¦¬ëœ RAFT ë°ì´í„°ì…‹ í™œìš©\n",
    "- ì´ì „ ì‹¤ìŠµì—ì„œ ìƒì„±í•œ í•œêµ­ì–´ RAFT ë°ì´í„°ì…‹ ì‚¬ìš©\n",
    "- Chat template ì ìš©ëœ ë©”ì‹œì§€ í˜•íƒœ\n",
    "- Train/Valid ë¶„í•  ì™„ë£Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_training_data() -> tuple:\n    \"\"\"\n    ì „ì²˜ë¦¬ëœ í•™ìŠµ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜\n    \n    Returns:\n        (train_dataset, valid_dataset, metadata)\n    \"\"\"\n    print(\"ğŸ“‚ í•™ìŠµ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n    \n    try:\n        # Train ë°ì´í„° ë¡œë“œ\n        train_data = []\n        with jsonlines.open(\"processed_data/train_raft_ko.jsonl\", \"r\") as reader:\n            train_data = list(reader)\n        \n        # Valid ë°ì´í„° ë¡œë“œ\n        valid_data = []\n        with jsonlines.open(\"processed_data/valid_raft_ko.jsonl\", \"r\") as reader:\n            valid_data = list(reader)\n        \n        # ë©”íƒ€ë°ì´í„° ë¡œë“œ\n        with open(\"processed_data/metadata.json\", \"r\", encoding=\"utf-8\") as f:\n            metadata = json.load(f)\n        \n        print(f\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ:\")\n        print(f\"  Train: {len(train_data)}ê°œ ìƒ˜í”Œ\")\n        print(f\"  Valid: {len(valid_data)}ê°œ ìƒ˜í”Œ\")\n        \n        return train_data, valid_data, metadata\n        \n    except FileNotFoundError as e:\n        print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}\")\n        print(\"ğŸ’¡ ë¨¼ì € 01_data_preprocessing_and_validation.ipynbë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\")\n        return None, None, None\n\ndef format_chat_template(messages: List[Dict[str, str]]) -> str:\n    \"\"\"\n    ë©”ì‹œì§€ë¥¼ EXAONE ì±„íŒ… í…œí”Œë¦¿ í˜•íƒœë¡œ ë³€í™˜\n    \n    Args:\n        messages: ì±„íŒ… ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸\n        \n    Returns:\n        í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸\n    \"\"\"\n    return tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=False  # Assistant ì‘ë‹µê¹Œì§€ í¬í•¨\n    )\n\ndef create_training_dataset(data: List[Dict], max_samples: Optional[int] = None) -> Dataset:\n    \"\"\"\n    í•™ìŠµìš© ë°ì´í„°ì…‹ ìƒì„± - Trainerì™€ í˜¸í™˜ë˜ëŠ” í˜•ì‹ìœ¼ë¡œ ìƒì„±\n    \n    Args:\n        data: ì›ë³¸ ë°ì´í„°\n        max_samples: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (Noneì´ë©´ ì „ì²´ ì‚¬ìš©)\n        \n    Returns:\n        Hugging Face Dataset\n    \"\"\"\n    print(f\"ğŸ”„ í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...\")\n    \n    # ìƒ˜í”Œ ìˆ˜ ì œí•œ\n    if max_samples is not None and len(data) > max_samples:\n        data = data[:max_samples]\n        print(f\"  ìƒ˜í”Œ ìˆ˜ ì œí•œ: {max_samples}ê°œ\")\n    \n    texts = []\n    \n    for item in tqdm(data, desc=\"ë°ì´í„° ë³€í™˜\"):\n        if \"messages\" not in item:\n            continue\n            \n        # ì±„íŒ… í…œí”Œë¦¿ ì ìš©\n        formatted_text = format_chat_template(item[\"messages\"])\n        texts.append(formatted_text)\n    \n    print(f\"âœ… í…ìŠ¤íŠ¸ ë³€í™˜ ì™„ë£Œ: {len(texts)}ê°œ\")\n    \n    # í† í¬ë‚˜ì´ì§•ì„ ì—¬ê¸°ì„œ ìˆ˜í–‰í•˜ì§€ ì•Šê³ , í…ìŠ¤íŠ¸ë§Œ ì €ì¥\n    # Trainerê°€ data_collatorë¥¼ í†µí•´ ë°°ì¹˜ë³„ë¡œ í† í¬ë‚˜ì´ì§• ìˆ˜í–‰\n    dataset_dict = {\n        \"text\": texts\n    }\n    \n    dataset = Dataset.from_dict(dataset_dict)\n    print(f\"âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(dataset)}ê°œ ìƒ˜í”Œ\")\n    \n    return dataset\n\n# ë°ì´í„° ë¡œë“œ\ntrain_data, valid_data, metadata = load_training_data()\n\nif train_data is not None:\n    # Colab ë¬´ë£Œ í™˜ê²½ ê³ ë ¤í•˜ì—¬ ìƒ˜í”Œ ìˆ˜ ì œí•œ (í•„ìš”ì‹œ ì¡°ì •)\n    MAX_TRAIN_SAMPLES = 400  # í•™ìŠµ ì‹œê°„ê³¼ ë©”ëª¨ë¦¬ë¥¼ ê³ ë ¤í•œ ì œí•œ\n    MAX_VALID_SAMPLES = 100\n    \n    # ë°ì´í„°ì…‹ ìƒì„±\n    train_dataset = create_training_dataset(train_data, MAX_TRAIN_SAMPLES)\n    valid_dataset = create_training_dataset(valid_data, MAX_VALID_SAMPLES)\n    \n    # ë°ì´í„°ì…‹ ì •ë³´ ì¶œë ¥\n    print(f\"\\nğŸ“Š ìµœì¢… ë°ì´í„°ì…‹ ì •ë³´:\")\n    print(f\"  Train ìƒ˜í”Œ: {len(train_dataset)}ê°œ\")\n    print(f\"  Valid ìƒ˜í”Œ: {len(valid_dataset)}ê°œ\")\n    \n    # ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n    print(f\"\\nğŸ“‹ ìƒ˜í”Œ ë°ì´í„°:\")\n    if len(train_dataset) > 0:\n        sample_text = train_dataset[0][\"text\"]\n        print(f\"  í…ìŠ¤íŠ¸ ê¸¸ì´: {len(sample_text)} ë¬¸ì\")\n        print(f\"  í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {sample_text[:200]}...\")\n        \n        # í† í° ê¸¸ì´ í™•ì¸\n        tokens = tokenizer.encode(sample_text)\n        print(f\"  í† í° ê¸¸ì´: {len(tokens)} í† í°\")\n\nelse:\n    print(\"âŒ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ì–´ í•™ìŠµì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì½œë°±\n",
    "\n",
    "### ğŸ” ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ì˜ ì¤‘ìš”ì„±\n",
    "- Colab í™˜ê²½ì—ì„œ GPU ë©”ëª¨ë¦¬ëŠ” ì œí•œì  (15GB)\n",
    "- OOM(Out of Memory) ì˜¤ë¥˜ë¥¼ ì‚¬ì „ì— ë°©ì§€\n",
    "- í•™ìŠµ ì¤‘ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì‹¤ì‹œê°„ ì¶”ì "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUMemoryCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì½œë°± í´ë˜ìŠ¤\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, log_every_n_steps: int = 10, memory_threshold: float = 0.9):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            log_every_n_steps: ë¡œê·¸ ì¶œë ¥ ì£¼ê¸°\n",
    "            memory_threshold: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê²½ê³  ì„ê³„ì¹˜ (90%)\n",
    "        \"\"\"\n",
    "        self.log_every_n_steps = log_every_n_steps\n",
    "        self.memory_threshold = memory_threshold\n",
    "        self.memory_history = []\n",
    "        \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"\n",
    "        ê° ìŠ¤í… ì¢…ë£Œ í›„ í˜¸ì¶œë˜ëŠ” ë©”ì„œë“œ\n",
    "        \"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "            \n",
    "        if state.global_step % self.log_every_n_steps == 0:\n",
    "            # ë©”ëª¨ë¦¬ ì •ë³´ ìˆ˜ì§‘\n",
    "            memory_info = self._collect_memory_info()\n",
    "            self.memory_history.append({\n",
    "                \"step\": state.global_step,\n",
    "                \"memory_info\": memory_info\n",
    "            })\n",
    "            \n",
    "            # ë¡œê·¸ ì¶œë ¥\n",
    "            self._log_memory_info(state.global_step, memory_info)\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ ê²½ê³ \n",
    "            for i, info in enumerate(memory_info):\n",
    "                if info[\"usage_ratio\"] > self.memory_threshold:\n",
    "                    print(f\"âš ï¸ GPU {i} ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê²½ê³ : {info['usage_ratio']:.1%}\")\n",
    "    \n",
    "    def _collect_memory_info(self) -> List[Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        GPU ë©”ëª¨ë¦¬ ì •ë³´ ìˆ˜ì§‘\n",
    "        \"\"\"\n",
    "        memory_info = []\n",
    "        \n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "            total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "            \n",
    "            memory_info.append({\n",
    "                \"gpu_id\": i,\n",
    "                \"allocated_gb\": allocated,\n",
    "                \"reserved_gb\": reserved,\n",
    "                \"total_gb\": total,\n",
    "                \"usage_ratio\": reserved / total if total > 0 else 0\n",
    "            })\n",
    "        \n",
    "        return memory_info\n",
    "    \n",
    "    def _log_memory_info(self, step: int, memory_info: List[Dict[str, float]]):\n",
    "        \"\"\"\n",
    "        ë©”ëª¨ë¦¬ ì •ë³´ ë¡œê·¸ ì¶œë ¥\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ“Š Step {step} - GPU ë©”ëª¨ë¦¬ ìƒíƒœ:\")\n",
    "        for info in memory_info:\n",
    "            gpu_id = info[\"gpu_id\"]\n",
    "            allocated = info[\"allocated_gb\"]\n",
    "            reserved = info[\"reserved_gb\"]\n",
    "            total = info[\"total_gb\"]\n",
    "            usage = info[\"usage_ratio\"]\n",
    "            \n",
    "            print(f\"  GPU {gpu_id}: {reserved:.1f}GB/{total:.1f}GB ({usage:.1%}) \"\n",
    "                  f\"[í• ë‹¹ë¨: {allocated:.1f}GB]\")\n",
    "    \n",
    "    def get_memory_history(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        ë©”ëª¨ë¦¬ ì‚¬ìš© íˆìŠ¤í† ë¦¬ ë°˜í™˜\n",
    "        \"\"\"\n",
    "        return self.memory_history\n",
    "\n",
    "# ì½œë°± ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "memory_callback = GPUMemoryCallback(log_every_n_steps=5, memory_threshold=0.85)\n",
    "\n",
    "print(\"ğŸ” GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì½œë°± ìƒì„± ì™„ë£Œ\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  ëª¨ë‹ˆí„°ë§ ì£¼ê¸°: 5 ìŠ¤í…ë§ˆë‹¤\")\n",
    "    print(f\"  ê²½ê³  ì„ê³„ì¹˜: 85%\")\n",
    "else:\n",
    "    print(\"  CPU ëª¨ë“œì—ì„œëŠ” ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. í•™ìŠµ ì„¤ì •\n",
    "\n",
    "### ğŸ¯ Colab ë¬´ë£Œ í™˜ê²½ ìµœì í™” ì„¤ì •\n",
    "- **ë°°ì¹˜ í¬ê¸°**: 1 (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "- **Gradient Accumulation**: 32 (ì‹¤ì§ˆì  ë°°ì¹˜ í¬ê¸° = 32)\n",
    "- **Learning Rate**: 1e-4 (LoRAì— ì í•©í•œ ê°’)\n",
    "- **FP16**: ë©”ëª¨ë¦¬ì™€ ì†ë„ ìµœì í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# í•™ìŠµ ì„¤ì •\ndef create_training_arguments() -> TrainingArguments:\n    \"\"\"\n    í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •\n    \"\"\"\n    \n    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    output_dir = f\"./fine_tuned_model_{timestamp}\"\n    \n    return TrainingArguments(\n        # ğŸ“ ë””ë ‰í† ë¦¬ ì„¤ì •\n        output_dir=output_dir,\n        logging_dir=f\"{output_dir}/logs\",\n        \n        # ğŸ‹ï¸ í•™ìŠµ ì„¤ì •\n        num_train_epochs=1,                    # ì—í¬í¬ ìˆ˜ (ì‹œê°„ ê³ ë ¤í•˜ì—¬ 1)\n        per_device_train_batch_size=1,         # GPUë‹¹ ë°°ì¹˜ í¬ê¸°\n        per_device_eval_batch_size=1,          # í‰ê°€ ë°°ì¹˜ í¬ê¸°\n        gradient_accumulation_steps=32,        # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  (ì‹¤ì§ˆ ë°°ì¹˜=32)\n        \n        # ğŸ“ˆ ìµœì í™” ì„¤ì •\n        learning_rate=1e-4,                   # í•™ìŠµë¥ \n        warmup_steps=50,                      # ì›œì—… ìŠ¤í…\n        optim=\"paged_adamw_8bit\",            # 8bit AdamW ì˜µí‹°ë§ˆì´ì €\n        \n        # ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™”\n        fp16=True,                           # FP16 ì •ë°€ë„\n        dataloader_pin_memory=False,         # ë©”ëª¨ë¦¬ í•€ ë¹„í™œì„±í™”\n        gradient_checkpointing=True,         # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…\n        \n        # ğŸ“Š ë¡œê¹… ë° í‰ê°€\n        logging_steps=5,                     # ë¡œê·¸ ì¶œë ¥ ì£¼ê¸°\n        eval_steps=50,                       # í‰ê°€ ì£¼ê¸°\n        eval_strategy=\"steps\",               # ìŠ¤í… ê¸°ë°˜ í‰ê°€ (evaluation_strategy â†’ eval_strategy)\n        \n        # ğŸ’¾ ì €ì¥ ì„¤ì •\n        save_steps=100,                      # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°\n        save_total_limit=2,                  # ìµœëŒ€ ì²´í¬í¬ì¸íŠ¸ ê°œìˆ˜\n        save_strategy=\"steps\",               # ìŠ¤í… ê¸°ë°˜ ì €ì¥\n        \n        # ğŸ¯ ê¸°íƒ€ ì„¤ì •\n        remove_unused_columns=False,         # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì»¬ëŸ¼ ì œê±° ì•ˆí•¨\n        report_to=\"none\",                   # wandb ë“± ë¦¬í¬íŠ¸ ë¹„í™œì„±í™”\n        load_best_model_at_end=True,         # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ\n        metric_for_best_model=\"eval_loss\",   # ìµœê³  ëª¨ë¸ ì„ íƒ ê¸°ì¤€\n        greater_is_better=False,             # LossëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ\n        \n        # ğŸ”„ ì¬í˜„ì„±\n        seed=42,\n        data_seed=42,\n    )\n\n# í•™ìŠµ ì„¤ì • ìƒì„±\ntraining_args = create_training_arguments()\n\nprint(\"âš™ï¸ í•™ìŠµ ì„¤ì • ì™„ë£Œ:\")\nprint(f\"  ì¶œë ¥ ë””ë ‰í† ë¦¬: {training_args.output_dir}\")\nprint(f\"  ì—í¬í¬ ìˆ˜: {training_args.num_train_epochs}\")\nprint(f\"  ë°°ì¹˜ í¬ê¸°: {training_args.per_device_train_batch_size}\")\nprint(f\"  ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì : {training_args.gradient_accumulation_steps}\")\nprint(f\"  ì‹¤ì§ˆì  ë°°ì¹˜ í¬ê¸°: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  í•™ìŠµë¥ : {training_args.learning_rate}\")\nprint(f\"  FP16: {training_args.fp16}\")\nprint(f\"  ì˜µí‹°ë§ˆì´ì €: {training_args.optim}\")\n\n# ì˜ˆìƒ í•™ìŠµ ì‹œê°„ ê³„ì‚°\nif train_dataset is not None:\n    total_steps = len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs\n    estimated_time_minutes = total_steps * 0.5  # ìŠ¤í…ë‹¹ ì•½ 30ì´ˆ ì¶”ì •\n    \n    print(f\"\\nâ±ï¸ ì˜ˆìƒ í•™ìŠµ ì‹œê°„:\")\n    print(f\"  ì´ ìŠ¤í…: {total_steps}\")\n    print(f\"  ì˜ˆìƒ ì‹œê°„: {estimated_time_minutes:.0f}ë¶„ ({estimated_time_minutes/60:.1f}ì‹œê°„)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ë°ì´í„° ì½œë ˆì´í„° ì„¤ì •\n",
    "\n",
    "### ğŸ“¦ Dynamic Padding\n",
    "- ë°°ì¹˜ë§ˆë‹¤ ìµœì  ê¸¸ì´ë¡œ íŒ¨ë”© (ë©”ëª¨ë¦¬ íš¨ìœ¨)\n",
    "- Language Modelingìš© ë¼ë²¨ ìë™ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ë°ì´í„° ì½œë ˆì´í„° ì„¤ì • - í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì§•í•˜ê³  ë°°ì¹˜ ì²˜ë¦¬\ndef tokenize_function(examples):\n    \"\"\"\n    í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì§•í•˜ëŠ” í•¨ìˆ˜\n    \"\"\"\n    # í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì§•\n    tokenized = tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        padding=False,  # ë°°ì¹˜ ì‹œ ë™ì  íŒ¨ë”©\n        max_length=MAX_LENGTH,\n        return_tensors=\"pt\" if len(examples[\"text\"]) == 1 else None\n    )\n    \n    # labelsëŠ” input_idsì™€ ë™ì¼ (language modeling)\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    \n    return tokenized\n\n# ë°ì´í„°ì…‹ì— í† í¬ë‚˜ì´ì§• ì ìš©\nif 'train_dataset' in locals() and train_dataset is not None:\n    print(\"ğŸ”„ ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§• ì¤‘...\")\n    \n    # í† í¬ë‚˜ì´ì§• ì ìš©\n    train_dataset = train_dataset.map(\n        tokenize_function,\n        batched=True,\n        remove_columns=[\"text\"],  # ì›ë³¸ í…ìŠ¤íŠ¸ ì œê±°\n        desc=\"í† í¬ë‚˜ì´ì§• ì§„í–‰\"\n    )\n    \n    valid_dataset = valid_dataset.map(\n        tokenize_function,\n        batched=True,\n        remove_columns=[\"text\"],  # ì›ë³¸ í…ìŠ¤íŠ¸ ì œê±°\n        desc=\"í† í¬ë‚˜ì´ì§• ì§„í–‰\"\n    )\n    \n    print(\"âœ… í† í¬ë‚˜ì´ì§• ì™„ë£Œ\")\n\n# ë°ì´í„° ì½œë ˆì´í„° ì„¤ì •\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,  # Causal Language Modeling (MLM ì•„ë‹˜)\n    pad_to_multiple_of=8,  # í…ì„œ ì—°ì‚° ìµœì í™”ë¥¼ ìœ„í•´ 8ì˜ ë°°ìˆ˜ë¡œ íŒ¨ë”©\n)\n\nprint(\"ğŸ“¦ ë°ì´í„° ì½œë ˆì´í„° ì„¤ì • ì™„ë£Œ:\")\nprint(f\"  MLM: {data_collator.mlm}\")\nprint(f\"  íŒ¨ë”© ë‹¨ìœ„: {data_collator.pad_to_multiple_of}\")\nprint(f\"  íŒ¨ë”© í† í° ID: {tokenizer.pad_token_id}\")\n\n# ìƒ˜í”Œ ë°ì´í„° í™•ì¸\nif 'train_dataset' in locals() and len(train_dataset) > 0:\n    print(f\"\\nğŸ“‹ í† í¬ë‚˜ì´ì§•ëœ ìƒ˜í”Œ í™•ì¸:\")\n    sample = train_dataset[0]\n    print(f\"  Input IDs ê¸¸ì´: {len(sample['input_ids'])}\")\n    print(f\"  Labels ê¸¸ì´: {len(sample['labels'])}\")\n    print(f\"  Attention Mask ê¸¸ì´: {len(sample['attention_mask'])}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Trainer ì„¤ì • ë° í•™ìŠµ ì‹œì‘\n",
    "\n",
    "### ğŸš€ íŒŒì¸íŠœë‹ ì‹¤í–‰\n",
    "- ì‹¤ì‹œê°„ GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§\n",
    "- ìë™ ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
    "- í‰ê°€ ë©”íŠ¸ë¦­ ì¶”ì "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì „ ì‚¬ì „ ì ê²€\n",
    "def pre_training_check():\n",
    "    \"\"\"\n",
    "    í•™ìŠµ ì‹œì‘ ì „ í™˜ê²½ ì ê²€\n",
    "    \"\"\"\n",
    "    print(\"ğŸ” í•™ìŠµ ì „ í™˜ê²½ ì ê²€:\")\n",
    "    \n",
    "    # GPU ë©”ëª¨ë¦¬ í™•ì¸\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            print(f\"  GPU {i}: {allocated:.1f}GB/{total_memory:.1f}GB ì‚¬ìš© ì¤‘\")\n",
    "            \n",
    "            if allocated / total_memory > 0.8:\n",
    "                print(f\"  âš ï¸ GPU {i} ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤. í•™ìŠµ ì¤‘ OOM ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ëª¨ë¸ ìƒíƒœ í™•ì¸\n",
    "    if model.training:\n",
    "        print(\"  âœ… ëª¨ë¸ì´ í•™ìŠµ ëª¨ë“œì…ë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(\"  âš ï¸ ëª¨ë¸ì´ í‰ê°€ ëª¨ë“œì…ë‹ˆë‹¤. í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜ë©ë‹ˆë‹¤.\")\n",
    "        model.train()\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ í¬ê¸° í™•ì¸\n",
    "    print(f\"  ë°ì´í„°: Train {len(train_dataset)}ê°œ, Valid {len(valid_dataset)}ê°œ\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Trainer ìƒì„±\n",
    "if train_dataset is not None and len(train_dataset) > 0:\n",
    "    print(\"ğŸ”„ Trainer ìƒì„± ì¤‘...\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset if len(valid_dataset) > 0 else None,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[memory_callback],  # GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì½œë°±\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Trainer ìƒì„± ì™„ë£Œ\")\n",
    "    \n",
    "    # í•™ìŠµ ì „ ì ê²€\n",
    "    if pre_training_check():\n",
    "        print(\"\\nğŸš€ íŒŒì¸íŠœë‹ ì‹œì‘!\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # í•™ìŠµ ì‹œì‘ ì‹œê°„ ê¸°ë¡\n",
    "        training_start_time = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            # ì‹¤ì œ í•™ìŠµ ì‹¤í–‰\n",
    "            training_result = trainer.train()\n",
    "            \n",
    "            # í•™ìŠµ ì™„ë£Œ ì‹œê°„ ê¸°ë¡\n",
    "            training_end_time = datetime.now()\n",
    "            training_duration = training_end_time - training_start_time\n",
    "            \n",
    "            print(\"\\nğŸ‰ íŒŒì¸íŠœë‹ ì™„ë£Œ!\")\n",
    "            print(\"=\" * 50)\n",
    "            print(f\"í•™ìŠµ ì‹œê°„: {training_duration}\")\n",
    "            print(f\"ìµœì¢… Loss: {training_result.training_loss:.4f}\")\n",
    "            \n",
    "            # í•™ìŠµ ê²°ê³¼ ì €ì¥\n",
    "            training_summary = {\n",
    "                \"model_name\": MODEL_NAME,\n",
    "                \"training_start\": training_start_time.isoformat(),\n",
    "                \"training_end\": training_end_time.isoformat(),\n",
    "                \"training_duration\": str(training_duration),\n",
    "                \"final_loss\": training_result.training_loss,\n",
    "                \"total_steps\": training_result.global_step,\n",
    "                \"trainable_parameters\": trainable_params,\n",
    "                \"total_parameters\": total_params,\n",
    "                \"training_samples\": len(train_dataset),\n",
    "                \"validation_samples\": len(valid_dataset)\n",
    "            }\n",
    "            \n",
    "            # ê²°ê³¼ ì €ì¥\n",
    "            with open(f\"{training_args.output_dir}/training_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(training_summary, f, ensure_ascii=False, indent=2, default=str)\n",
    "            \n",
    "            print(f\"\\nğŸ’¾ í•™ìŠµ ê²°ê³¼ ì €ì¥: {training_args.output_dir}/training_summary.json\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                print(\"\\nâŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "                print(\"ğŸ’¡ í•´ê²° ë°©ë²•:\")\n",
    "                print(\"   1. ë°°ì¹˜ í¬ê¸°ë¥¼ ë” ì¤„ì—¬ë³´ì„¸ìš” (per_device_train_batch_size=1)\")\n",
    "                print(\"   2. ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë‹¨ê³„ë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš” (gradient_accumulation_steps=64)\")\n",
    "                print(\"   3. ìµœëŒ€ í† í° ê¸¸ì´ë¥¼ ì¤„ì—¬ë³´ì„¸ìš” (MAX_LENGTH=2048)\")\n",
    "                print(\"   4. í•™ìŠµ ìƒ˜í”Œ ìˆ˜ë¥¼ ì¤„ì—¬ë³´ì„¸ìš” (MAX_TRAIN_SAMPLES=200)\")\n",
    "            else:\n",
    "                print(f\"\\nâŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            \n",
    "else:\n",
    "    print(\"âŒ í•™ìŠµ ë°ì´í„°ê°€ ì—†ì–´ íŒŒì¸íŠœë‹ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ğŸ’¡ ë¨¼ì € 01_data_preprocessing_and_validation.ipynbë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. í•™ìŠµ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_training_results(trainer: Trainer, memory_callback: GPUMemoryCallback):\n",
    "    \"\"\"\n",
    "    í•™ìŠµ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š í•™ìŠµ ê²°ê³¼ ë¶„ì„ ì¤‘...\")\n",
    "    \n",
    "    # 1. í•™ìŠµ ë¡œê·¸ ë¶„ì„\n",
    "    log_history = trainer.state.log_history\n",
    "    \n",
    "    if not log_history:\n",
    "        print(\"âš ï¸ í•™ìŠµ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    # í•™ìŠµ ë° í‰ê°€ ë¡œê·¸ ë¶„ë¦¬\n",
    "    train_logs = [log for log in log_history if 'loss' in log and 'eval_loss' not in log]\n",
    "    eval_logs = [log for log in log_history if 'eval_loss' in log]\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ í•™ìŠµ ì§„í–‰ ìƒí™©:\")\n",
    "    print(f\"  ì´ í•™ìŠµ ìŠ¤í…: {len(train_logs)}\")\n",
    "    print(f\"  í‰ê°€ íšŸìˆ˜: {len(eval_logs)}\")\n",
    "    \n",
    "    if train_logs:\n",
    "        initial_loss = train_logs[0].get('loss', 0)\n",
    "        final_loss = train_logs[-1].get('loss', 0)\n",
    "        loss_improvement = initial_loss - final_loss\n",
    "        \n",
    "        print(f\"  ì´ˆê¸° Loss: {initial_loss:.4f}\")\n",
    "        print(f\"  ìµœì¢… Loss: {final_loss:.4f}\")\n",
    "        print(f\"  Loss ê°œì„ : {loss_improvement:.4f} ({loss_improvement/initial_loss:.1%})\")\n",
    "    \n",
    "    # 2. ì‹œê°í™”\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 2-1. Training Loss ê³¡ì„ \n",
    "    if train_logs:\n",
    "        steps = [log.get('step', 0) for log in train_logs]\n",
    "        losses = [log.get('loss', 0) for log in train_logs]\n",
    "        \n",
    "        axes[0, 0].plot(steps, losses, 'b-', label='Training Loss', linewidth=2)\n",
    "        axes[0, 0].set_xlabel('Steps')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].set_title('Training Loss Curve')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].legend()\n",
    "    \n",
    "    # 2-2. Evaluation Loss ê³¡ì„ \n",
    "    if eval_logs:\n",
    "        eval_steps = [log.get('step', 0) for log in eval_logs]\n",
    "        eval_losses = [log.get('eval_loss', 0) for log in eval_logs]\n",
    "        \n",
    "        axes[0, 1].plot(eval_steps, eval_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "        axes[0, 1].set_xlabel('Steps')\n",
    "        axes[0, 1].set_ylabel('Loss')\n",
    "        axes[0, 1].set_title('Validation Loss Curve')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].legend()\n",
    "    \n",
    "    # 2-3. GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰\n",
    "    memory_history = memory_callback.get_memory_history()\n",
    "    if memory_history:\n",
    "        memory_steps = [item['step'] for item in memory_history]\n",
    "        memory_usage = [item['memory_info'][0]['usage_ratio'] * 100 \n",
    "                       for item in memory_history if item['memory_info']]\n",
    "        \n",
    "        if memory_usage:\n",
    "            axes[1, 0].plot(memory_steps, memory_usage, 'g-', label='GPU Memory Usage', linewidth=2)\n",
    "            axes[1, 0].axhline(y=85, color='orange', linestyle='--', label='Warning (85%)')\n",
    "            axes[1, 0].axhline(y=95, color='red', linestyle='--', label='Critical (95%)')\n",
    "            axes[1, 0].set_xlabel('Steps')\n",
    "            axes[1, 0].set_ylabel('Memory Usage (%)')\n",
    "            axes[1, 0].set_title('GPU Memory Usage')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "            axes[1, 0].legend()\n",
    "    \n",
    "    # 2-4. Learning Rate ìŠ¤ì¼€ì¤„\n",
    "    lr_logs = [log.get('learning_rate', 0) for log in train_logs if 'learning_rate' in log]\n",
    "    if lr_logs:\n",
    "        lr_steps = [log.get('step', 0) for log in train_logs if 'learning_rate' in log]\n",
    "        \n",
    "        axes[1, 1].plot(lr_steps, lr_logs, 'm-', label='Learning Rate', linewidth=2)\n",
    "        axes[1, 1].set_xlabel('Steps')\n",
    "        axes[1, 1].set_ylabel('Learning Rate')\n",
    "        axes[1, 1].set_title('Learning Rate Schedule')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ê·¸ë˜í”„ ì €ì¥\n",
    "    output_dir = trainer.args.output_dir\n",
    "    plt.savefig(f\"{output_dir}/training_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ í•™ìŠµ ë¶„ì„ ê·¸ë˜í”„ ì €ì¥: {output_dir}/training_analysis.png\")\n",
    "    \n",
    "    # 3. ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±\n",
    "    analysis_report = {\n",
    "        \"training_summary\": {\n",
    "            \"total_steps\": len(train_logs),\n",
    "            \"evaluation_count\": len(eval_logs),\n",
    "            \"initial_loss\": train_logs[0].get('loss', 0) if train_logs else 0,\n",
    "            \"final_loss\": train_logs[-1].get('loss', 0) if train_logs else 0,\n",
    "            \"loss_improvement\": loss_improvement if train_logs else 0,\n",
    "            \"improvement_percentage\": (loss_improvement/initial_loss*100) if train_logs and initial_loss > 0 else 0\n",
    "        },\n",
    "        \"memory_analysis\": {\n",
    "            \"max_memory_usage\": max(memory_usage) if memory_usage else 0,\n",
    "            \"avg_memory_usage\": np.mean(memory_usage) if memory_usage else 0,\n",
    "            \"memory_warnings\": sum(1 for usage in memory_usage if usage > 85) if memory_usage else 0\n",
    "        },\n",
    "        \"recommendations\": []\n",
    "    }\n",
    "    \n",
    "    # ê¶Œì¥ì‚¬í•­ ìƒì„±\n",
    "    if analysis_report[\"training_summary\"][\"improvement_percentage\"] < 5:\n",
    "        analysis_report[\"recommendations\"].append(\"Loss ê°œì„ ì´ ë¯¸ë¯¸í•©ë‹ˆë‹¤. ë” ë§ì€ ì—í¬í¬ë‚˜ ë‹¤ë¥¸ í•™ìŠµë¥ ì„ ì‹œë„í•´ë³´ì„¸ìš”.\")\n",
    "    \n",
    "    if analysis_report[\"memory_analysis\"][\"max_memory_usage\"] > 90:\n",
    "        analysis_report[\"recommendations\"].append(\"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”.\")\n",
    "    \n",
    "    if not analysis_report[\"recommendations\"]:\n",
    "        analysis_report[\"recommendations\"].append(\"í•™ìŠµì´ ì•ˆì •ì ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "    \n",
    "    # ë¦¬í¬íŠ¸ ì €ì¥\n",
    "    with open(f\"{output_dir}/analysis_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(analysis_report, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"ğŸ’¾ ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥: {output_dir}/analysis_report.json\")\n",
    "    \n",
    "    return analysis_report\n",
    "\n",
    "# í•™ìŠµ ì™„ë£Œ í›„ ê²°ê³¼ ë¶„ì„ ì‹¤í–‰\n",
    "if 'trainer' in locals() and 'training_result' in locals():\n",
    "    analysis_report = analyze_training_results(trainer, memory_callback)\n",
    "    \n",
    "    # ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n",
    "    print(\"\\nğŸ“‹ í•™ìŠµ ê²°ê³¼ ìš”ì•½:\")\n",
    "    print(f\"  Loss ê°œì„ : {analysis_report['training_summary']['improvement_percentage']:.1f}%\")\n",
    "    print(f\"  ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©: {analysis_report['memory_analysis']['max_memory_usage']:.1f}%\")\n",
    "    print(f\"\\nğŸ’¡ ê¶Œì¥ì‚¬í•­:\")\n",
    "    for rec in analysis_report['recommendations']:\n",
    "        print(f\"   - {rec}\")\n",
    "else:\n",
    "    print(\"âš ï¸ í•™ìŠµì´ ì™„ë£Œë˜ì§€ ì•Šì•„ ê²°ê³¼ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ëª¨ë¸ ì €ì¥ ë° í…ŒìŠ¤íŠ¸\n",
    "\n",
    "### ğŸ’¾ í•™ìŠµëœ ëª¨ë¸ ì €ì¥\n",
    "- LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥ (ìš©ëŸ‰ íš¨ìœ¨ì )\n",
    "- ì¶”í›„ ì‰¬ìš´ ë¡œë“œë¥¼ ìœ„í•œ ì„¤ì • íŒŒì¼ í¬í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def save_fine_tuned_model(trainer: Trainer, output_path: str = None) -> str:\n    \"\"\"\n    íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì €ì¥\n    \n    Args:\n        trainer: í•™ìŠµëœ Trainer ê°ì²´\n        output_path: ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)\n        \n    Returns:\n        ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œ\n    \"\"\"\n    if output_path is None:\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        output_path = f\"./exaone_raft_lora_{timestamp}\"\n    \n    print(f\"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {output_path}\")\n    \n    # ëª¨ë¸ ì €ì¥ (LoRA ì–´ëŒ‘í„°ë§Œ)\n    trainer.model.save_pretrained(output_path)\n    \n    # í† í¬ë‚˜ì´ì €ë„ í•¨ê»˜ ì €ì¥\n    tokenizer.save_pretrained(output_path)\n    \n    # ëª¨ë¸ ì •ë³´ ì €ì¥ (JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜)\n    model_info = {\n        \"base_model\": MODEL_NAME,\n        \"model_type\": \"QLoRA\",\n        \"task_type\": \"RAG Fine-tuning\",\n        \"save_timestamp\": datetime.now().isoformat(),\n        \"lora_config\": {\n            \"r\": lora_config.r,\n            \"lora_alpha\": lora_config.lora_alpha,\n            \"lora_dropout\": lora_config.lora_dropout,\n            \"target_modules\": list(lora_config.target_modules)  # setì„ listë¡œ ë³€í™˜\n        },\n        \"training_info\": {\n            \"trainable_parameters\": trainable_params,\n            \"total_parameters\": total_params,\n            \"training_samples\": len(train_dataset) if train_dataset else 0\n        }\n    }\n    \n    with open(f\"{output_path}/model_info.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_info, f, ensure_ascii=False, indent=2)\n    \n    print(f\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_path}\")\n    print(f\"   - LoRA ì–´ëŒ‘í„°: adapter_model.safetensors\")\n    print(f\"   - ì„¤ì • íŒŒì¼: adapter_config.json\")\n    print(f\"   - í† í¬ë‚˜ì´ì €: tokenizer.json, tokenizer_config.json\")\n    print(f\"   - ëª¨ë¸ ì •ë³´: model_info.json\")\n    \n    return output_path\n\ndef upload_to_huggingface(model_path: str, repo_name: str = \"ryanu/my-exaone-raft-model\", private: bool = False):\n    \"\"\"\n    íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ í—ˆê¹…í˜ì´ìŠ¤ Hubì— ì—…ë¡œë“œ\n    \n    Args:\n        model_path: ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ\n        repo_name: í—ˆê¹…í˜ì´ìŠ¤ ë¦¬í¬ì§€í† ë¦¬ ì´ë¦„ (ê¸°ë³¸: ryanu/my-exaone-raft-model)\n        private: í”„ë¼ì´ë¹— ë¦¬í¬ì§€í† ë¦¬ ì—¬ë¶€\n    \"\"\"\n    \n    print(\"\\nğŸš€ í—ˆê¹…í˜ì´ìŠ¤ Hub ì—…ë¡œë“œ ì¤€ë¹„ ì¤‘...\")\n    \n    # í—ˆê¹…í˜ì´ìŠ¤ ë¡œê·¸ì¸ í™•ì¸\n    try:\n        api = HfApi()\n        user_info = api.whoami()\n        username = user_info[\"name\"]\n        print(f\"âœ… í—ˆê¹…í˜ì´ìŠ¤ ë¡œê·¸ì¸ í™•ì¸: {username}\")\n        \n    except Exception as e:\n        print(\"âŒ í—ˆê¹…í˜ì´ìŠ¤ ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤!\")\n        print(\"ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¼ì£¼ì„¸ìš”:\")\n        print(\"1. https://huggingface.co/settings/tokens ì—ì„œ í† í° ìƒì„±\")\n        print(\"2. ì•„ë˜ ì½”ë“œ ì‹¤í–‰:\")\n        print(\"   from huggingface_hub import login\")\n        print(\"   login()  # í† í° ì…ë ¥\")\n        print(\"3. ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ ì„¤ì •:\")\n        print(\"   import os\")\n        print(\"   os.environ['HF_TOKEN'] = 'your_token_here'\")\n        return None\n    \n    try:\n        print(f\"ğŸ“¦ ë¦¬í¬ì§€í† ë¦¬ ìƒì„± ì¤‘: {repo_name}\")\n        \n        # ë¦¬í¬ì§€í† ë¦¬ ìƒì„± (ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¬´ì‹œ)\n        try:\n            create_repo(\n                repo_id=repo_name,\n                private=private,\n                exist_ok=True,\n                repo_type=\"model\"\n            )\n            print(f\"âœ… ë¦¬í¬ì§€í† ë¦¬ ìƒì„± ì™„ë£Œ: {repo_name}\")\n        except Exception as e:\n            if \"already exists\" in str(e):\n                print(f\"âœ… ê¸°ì¡´ ë¦¬í¬ì§€í† ë¦¬ ì‚¬ìš©: {repo_name}\")\n            else:\n                raise e\n        \n        # ëª¨ë¸ ì¹´ë“œ ìƒì„±\n        model_card_content = f\"\"\"---\nlicense: apache-2.0\nbase_model: {MODEL_NAME}\ntags:\n- peft\n- lora\n- korean\n- rag\n- exaone\nlanguage:\n- ko\n---\n\n# EXAONE RAG Fine-tuned Model with LoRA\n\nì´ ëª¨ë¸ì€ EXAONE-3.5-2.4B-Instructë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œêµ­ì–´ RAG ë°ì´í„°ì…‹ìœ¼ë¡œ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì…ë‹ˆë‹¤.\n\n## Model Details\n\n- **Base Model**: {MODEL_NAME}\n- **Fine-tuning Method**: QLoRA (4-bit quantization + LoRA)\n- **Task**: Retrieval-Augmented Generation (RAG)\n- **Language**: Korean\n- **Training Data**: RAFT methodology based Korean RAG dataset\n\n## Usage\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# ë² ì´ìŠ¤ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\nbase_model = AutoModelForCausalLM.from_pretrained(\"{MODEL_NAME}\")\ntokenizer = AutoTokenizer.from_pretrained(\"{MODEL_NAME}\")\n\n# LoRA ì–´ëŒ‘í„° ì ìš©\nmodel = PeftModel.from_pretrained(base_model, \"{repo_name}\")\n\n# ì¶”ë¡  ì˜ˆì‹œ\nmessages = [\n    {{\"role\": \"system\", \"content\": \"ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\"}},\n    {{\"role\": \"user\", \"content\": \"ì»¨í…ìŠ¤íŠ¸: í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤. ì§ˆë¬¸: í•œêµ­ì˜ ìˆ˜ë„ëŠ”?\"\"}}\n]\n\ninput_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model.generate(inputs, max_new_tokens=100, temperature=0.7)\n    \nresponse = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\nprint(response)\n```\n\n## Training Details\n\n- **Training Framework**: Hugging Face Transformers + PEFT\n- **Optimization**: 8-bit AdamW\n- **Learning Rate**: 1e-4\n- **Batch Size**: 32 (with gradient accumulation)\n- **Precision**: FP16\n\n## Performance\n\nì´ ëª¨ë¸ì€ ë² ì´ìŠ¤ë¼ì¸ EXAONE ëª¨ë¸ ëŒ€ë¹„ í•œêµ­ì–´ RAG íƒœìŠ¤í¬ì—ì„œ í–¥ìƒëœ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\nìì„¸í•œ í‰ê°€ ê²°ê³¼ëŠ” í•™ìŠµ ë¦¬í¬ì§€í† ë¦¬ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.\n\"\"\"\n        \n        # ëª¨ë¸ ì¹´ë“œ ì €ì¥\n        with open(f\"{model_path}/README.md\", \"w\", encoding=\"utf-8\") as f:\n            f.write(model_card_content)\n        \n        print(\"ğŸ“„ ëª¨ë¸ ì¹´ë“œ ìƒì„± ì™„ë£Œ\")\n        \n        # íŒŒì¼ ì—…ë¡œë“œ\n        print(\"ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\n        \n        # ì—…ë¡œë“œí•  íŒŒì¼ë“¤\n        files_to_upload = [\n            \"adapter_config.json\",\n            \"adapter_model.safetensors\", \n            \"tokenizer.json\",\n            \"tokenizer_config.json\",\n            \"model_info.json\",\n            \"README.md\"\n        ]\n        \n        for file_name in files_to_upload:\n            file_path = os.path.join(model_path, file_name)\n            if os.path.exists(file_path):\n                api.upload_file(\n                    path_or_fileobj=file_path,\n                    path_in_repo=file_name,\n                    repo_id=repo_name,\n                    repo_type=\"model\"\n                )\n                print(f\"   âœ… {file_name} ì—…ë¡œë“œ ì™„ë£Œ\")\n            else:\n                print(f\"   âš ï¸ {file_name} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ\")\n        \n        print(f\"\\nğŸ‰ í—ˆê¹…í˜ì´ìŠ¤ ì—…ë¡œë“œ ì™„ë£Œ!\")\n        print(f\"ğŸ”— ëª¨ë¸ URL: https://huggingface.co/{repo_name}\")\n        print(f\"ğŸ“Š ë¦¬í¬ì§€í† ë¦¬ ì„¤ì •:\")\n        print(f\"   - ì´ë¦„: {repo_name}\")\n        print(f\"   - ê³µê°œ ì—¬ë¶€: {'Private' if private else 'Public'}\")\n        print(f\"   - ëª¨ë¸ íƒ€ì…: LoRA Adapter\")\n        \n        return repo_name\n        \n    except Exception as e:\n        print(f\"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}\")\n        print(\"ğŸ’¡ ë¬¸ì œ í•´ê²° ë°©ë²•:\")\n        print(\"1. í† í° ê¶Œí•œ í™•ì¸ (write ê¶Œí•œ í•„ìš”)\")\n        print(\"2. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸\")\n        print(\"3. ë¦¬í¬ì§€í† ë¦¬ ì´ë¦„ ì¤‘ë³µ í™•ì¸\")\n        return None\n\ndef test_fine_tuned_model(model, tokenizer, test_prompts: List[str]):\n    \"\"\"\n    íŒŒì¸íŠœë‹ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸\n    \"\"\"\n    print(\"ğŸ§ª íŒŒì¸íŠœë‹ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...\")\n    \n    model.eval()\n    \n    for i, prompt in enumerate(test_prompts, 1):\n        print(f\"\\nğŸ“ í…ŒìŠ¤íŠ¸ {i}:\")\n        print(f\"ì…ë ¥: {prompt[:100]}{'...' if len(prompt) > 100 else ''}\")\n        \n        # ë©”ì‹œì§€ í˜•íƒœë¡œ ë³€í™˜\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": prompt\n            }\n        ]\n        \n        # ì…ë ¥ í† í°í™”\n        input_text = tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n        \n        inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n        if torch.cuda.is_available():\n            inputs = inputs.cuda()\n        \n        # ìƒì„±\n        with torch.no_grad():\n            outputs = model.generate(\n                inputs,\n                max_new_tokens=150,\n                temperature=0.7,\n                do_sample=True,\n                pad_token_id=tokenizer.eos_token_id,\n                eos_token_id=tokenizer.eos_token_id\n            )\n        \n        # ì‘ë‹µ ë””ì½”ë”© (ì…ë ¥ ë¶€ë¶„ ì œê±°)\n        generated_text = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n        \n        print(f\"ì¶œë ¥: {generated_text.strip()}\")\n        print(\"-\" * 80)\n\n# ëª¨ë¸ ì €ì¥ ë° í…ŒìŠ¤íŠ¸ ì‹¤í–‰ - ê°œì„ ëœ ì¡°ê±´ í™•ì¸\ndef check_training_completion():\n    \"\"\"\n    í•™ìŠµ ì™„ë£Œ ìƒíƒœë¥¼ í™•ì¸í•˜ëŠ” í•¨ìˆ˜\n    \"\"\"\n    # ë³€ìˆ˜ ì¡´ì¬ í™•ì¸\n    trainer_exists = 'trainer' in locals() or 'trainer' in globals()\n    training_result_exists = 'training_result' in locals() or 'training_result' in globals()\n    \n    print(f\"ğŸ” í•™ìŠµ ì™„ë£Œ ìƒíƒœ í™•ì¸:\")\n    print(f\"  Trainer ì¡´ì¬: {trainer_exists}\")\n    print(f\"  Training Result ì¡´ì¬: {training_result_exists}\")\n    \n    if trainer_exists:\n        # trainer ê°ì²´ê°€ ì¡´ì¬í•˜ê³  í•™ìŠµì´ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸\n        try:\n            # ê¸€ë¡œë²Œ ìŠ¤ì½”í”„ì—ì„œ trainer í™•ì¸\n            current_trainer = globals().get('trainer') or locals().get('trainer')\n            if current_trainer and hasattr(current_trainer, 'state'):\n                print(f\"  í•™ìŠµëœ ìŠ¤í… ìˆ˜: {current_trainer.state.global_step}\")\n                return current_trainer.state.global_step > 0, current_trainer\n        except:\n            pass\n    \n    return False, None\n\n# í•™ìŠµ ì™„ë£Œ í™•ì¸\ntraining_completed, current_trainer = check_training_completion()\n\nif training_completed and current_trainer:\n    print(\"\\nâœ… í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.\")\n    \n    # ëª¨ë¸ ì €ì¥\n    saved_model_path = save_fine_tuned_model(current_trainer)\n    \n    # ì €ì¥ëœ íŒŒì¼ë“¤ì˜ ì¡´ì¬ í™•ì¸\n    import os\n    essential_files = [\n        f\"{saved_model_path}/adapter_config.json\",\n        f\"{saved_model_path}/adapter_model.safetensors\"\n    ]\n    \n    print(f\"\\nğŸ” ì €ì¥ëœ íŒŒì¼ í™•ì¸:\")\n    all_files_exist = True\n    for file_path in essential_files:\n        if os.path.exists(file_path):\n            file_size = os.path.getsize(file_path)\n            print(f\"  âœ… {os.path.basename(file_path)}: {file_size:,} bytes\")\n        else:\n            print(f\"  âŒ {os.path.basename(file_path)}: íŒŒì¼ ì—†ìŒ\")\n            all_files_exist = False\n    \n    if all_files_exist:\n        print(f\"\\nğŸ‰ ëª¨ë¸ ì €ì¥ ì„±ê³µ! 04ë²ˆ ë…¸íŠ¸ë¶ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\")\n        \n        # ìë™ìœ¼ë¡œ Hugging Faceì— ì—…ë¡œë“œ\n        print(f\"\\nğŸš€ Hugging Faceì— ì—…ë¡œë“œ ì¤‘...\")\n        try:\n            # í—ˆê¹…í˜ì´ìŠ¤ì— ì—…ë¡œë“œ (ë¡œê·¸ì¸ì´ ë˜ì–´ìˆë‹¤ë©´)\n            repo_name = upload_to_huggingface(\n                model_path=saved_model_path,\n                repo_name=\"ryanu/my-exaone-raft-model\",\n                private=False\n            )\n            \n            if repo_name:\n                print(f\"\\nğŸ‰ ì—…ë¡œë“œ ì™„ë£Œ!\")\n                print(f\"ğŸ”— ëª¨ë¸ ì£¼ì†Œ: https://huggingface.co/{repo_name}\")\n                print(f\"ğŸ’¡ Day 2-3 ì‹¤ìŠµì—ì„œ ì´ ì£¼ì†Œë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\")\n                \n                # Day 2-3ì—ì„œ ì‚¬ìš©í•  ëª¨ë¸ ì£¼ì†Œë¥¼ íŒŒì¼ë¡œ ì €ì¥\n                model_config = {\n                    \"model_name\": repo_name,\n                    \"base_model\": MODEL_NAME,\n                    \"upload_timestamp\": datetime.now().isoformat(),\n                    \"usage_instructions\": {\n                        \"load_command\": f'PeftModel.from_pretrained(base_model, \"{repo_name}\")',\n                        \"description\": \"Day 1ì—ì„œ íŒŒì¸íŠœë‹í•œ EXAONE RAG ëª¨ë¸\"\n                    }\n                }\n                \n                with open(\"../day2-RAG/finetuned_model_info.json\", \"w\", encoding=\"utf-8\") as f:\n                    json.dump(model_config, f, ensure_ascii=False, indent=2)\n                \n                print(f\"ğŸ“ ëª¨ë¸ ì •ë³´ ì €ì¥: ../day2-RAG/finetuned_model_info.json\")\n                \n        except Exception as e:\n            print(f\"âš ï¸ ìë™ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}\")\n            print(f\"ğŸ’¡ ìˆ˜ë™ìœ¼ë¡œ ì—…ë¡œë“œí•˜ë ¤ë©´ 25ë²ˆ ì…€ì„ ì‚¬ìš©í•˜ì„¸ìš”.\")\n        \n        # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„\n        test_prompts = [\n            \"ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\\n\\n=== ì»¨í…ìŠ¤íŠ¸ ===\\nì»¨í…ìŠ¤íŠ¸ 1: í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.\\nì»¨í…ìŠ¤íŠ¸ 2: ì„œìš¸ì€ í•œê°•ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë°œë‹¬í–ˆìŠµë‹ˆë‹¤.\\n\\n=== ì§ˆë¬¸ ===\\ní•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\",\n            \"ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\\n\\n=== ì»¨í…ìŠ¤íŠ¸ ===\\nì»¨í…ìŠ¤íŠ¸ 1: ê¹€ì¹˜ëŠ” í•œêµ­ì˜ ì „í†µ ë°œíš¨ì‹í’ˆì…ë‹ˆë‹¤.\\nì»¨í…ìŠ¤íŠ¸ 2: íŒŒìŠ¤íƒ€ëŠ” ì´íƒˆë¦¬ì•„ì˜ ëŒ€í‘œ ìŒì‹ì…ë‹ˆë‹¤.\\n\\n=== ì§ˆë¬¸ ===\\nê¹€ì¹˜ëŠ” ì–´ë–¤ ìŒì‹ì¸ê°€ìš”?\",\n            \"ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\\n\\n=== ì»¨í…ìŠ¤íŠ¸ ===\\nì»¨í…ìŠ¤íŠ¸ 1: íƒœì–‘ì€ ë³„ì…ë‹ˆë‹¤.\\nì»¨í…ìŠ¤íŠ¸ 2: ì§€êµ¬ëŠ” í–‰ì„±ì…ë‹ˆë‹¤.\\n\\n=== ì§ˆë¬¸ ===\\në°”ë‹¤ì˜ ìƒ‰ê¹”ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n        ]\n        \n        # ëª¨ë¸ í…ŒìŠ¤íŠ¸\n        test_fine_tuned_model(current_trainer.model, tokenizer, test_prompts)\n        \n        print(f\"\\nğŸ‰ Day 1 ì‹¤ìŠµ 3 ì™„ë£Œ!\")\n        print(f\"âœ… ì €ì¥ëœ ëª¨ë¸: {saved_model_path}\")\n        print(f\"âœ… Hugging Face ì£¼ì†Œ: https://huggingface.co/ryanu/my-exaone-raft-model\")\n        print(f\"ğŸ”„ ë‹¤ìŒ ë‹¨ê³„: 04_evaluation_and_comparison.ipynbì—ì„œ ì„±ëŠ¥ í‰ê°€ë¥¼ ì§„í–‰í•˜ì„¸ìš”!\")\n    else:\n        print(f\"\\nâš ï¸ ì¼ë¶€ íŒŒì¼ì´ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì €ì¥ì„ ì‹œë„í•˜ì„¸ìš”.\")\n        \nelse:\n    print(\"\\nâš ï¸ í•™ìŠµì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n    print(\"ğŸ’¡ í•™ìŠµì„ ë¨¼ì € ì™„ë£Œí•œ í›„ ì´ ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.\")\n    print(\"\\nğŸ“ í•™ìŠµ ìƒíƒœ í™•ì¸:\")\n    if 'trainer' not in locals() and 'trainer' not in globals():\n        print(\"  - Trainer ê°ì²´ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n        print(\"  - 19ë²ˆ ì…€(í•™ìŠµ ì‹¤í–‰)ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n    else:\n        print(\"  - TrainerëŠ” ì¡´ì¬í•˜ì§€ë§Œ í•™ìŠµì´ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 13. í—ˆê¹…í˜ì´ìŠ¤ ì—…ë¡œë“œ ê°€ì´ë“œ\n\n### ğŸŒ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì—…ë¡œë“œí•˜ê¸°\n\níŒŒì¸íŠœë‹ ì™„ë£Œ í›„ ëª¨ë¸ì„ í—ˆê¹…í˜ì´ìŠ¤ì— ì—…ë¡œë“œí•˜ì—¬ ì €ì¥í•˜ê³  ê³µìœ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n### ğŸ“ ì¤€ë¹„ì‚¬í•­\n1. í—ˆê¹…í˜ì´ìŠ¤ ê³„ì •: https://huggingface.co/join\n2. ì•¡ì„¸ìŠ¤ í† í° (Write ê¶Œí•œ): https://huggingface.co/settings/tokens\n\n### ğŸ” ë¡œê·¸ì¸ ë°©ë²•\n```python\nfrom huggingface_hub import login\nlogin()  # í† í° ì…ë ¥ í”„ë¡¬í”„íŠ¸\n```\n\n### ğŸš€ ì—…ë¡œë“œ ì‹¤í–‰\nì•„ë˜ ì…€ì—ì„œ ì‹¤ì œ ì—…ë¡œë“œë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "source": "# í—ˆê¹…í˜ì´ìŠ¤ ì—…ë¡œë“œ ì‹¤í–‰\n# ë¨¼ì € ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤\n\n# 1. ë¡œê·¸ì¸ (ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ì‹¤í–‰)\n# from huggingface_hub import login\n# login()\n\n# 2. í™˜ê²½ë³€ìˆ˜ë¡œ í† í° ì„¤ì • (ì„ íƒì‚¬í•­)\n# import os\n# os.environ['HF_TOKEN'] = 'your_token_here'\n\n# 3. ëª¨ë¸ ì—…ë¡œë“œ ì‹¤í–‰ (ë¡œê·¸ì¸ í›„ ì£¼ì„ í•´ì œ)\nif 'saved_model_path' in locals():\n    print(f\"ğŸ“¦ ì—…ë¡œë“œ ì¤€ë¹„ëœ ëª¨ë¸: {saved_model_path}\")\n    \n    # ì—…ë¡œë“œ ì‹¤í–‰ (ì£¼ì„ í•´ì œ í›„ ì‚¬ìš©)\n    \"\"\"\n    repo_name = upload_to_huggingface(\n        model_path=saved_model_path,\n        repo_name=\"my-exaone-raft-model\",  # ì›í•˜ëŠ” ì´ë¦„ìœ¼ë¡œ ë³€ê²½\n        private=True  # True: ë¹„ê³µê°œ, False: ê³µê°œ\n    )\n    print(f\"âœ… ì—…ë¡œë“œ ì™„ë£Œ: https://huggingface.co/{repo_name}\")\n    \"\"\"\n    \n    print(\"\\nğŸ’¡ ì—…ë¡œë“œí•˜ë ¤ë©´:\")\n    print(\"1. ìœ„ì˜ login() ì£¼ì„ í•´ì œí•˜ê³  ì‹¤í–‰\")\n    print(\"2. í† í° ì…ë ¥\")\n    print(\"3. upload_to_huggingface() ì£¼ì„ í•´ì œí•˜ê³  ì‹¤í–‰\")\n    \nelse:\n    print(\"âŒ ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € íŒŒì¸íŠœë‹ì„ ì™„ë£Œí•˜ì„¸ìš”.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ìš”ì•½ ì¶œë ¥\n",
    "print(\"ğŸ¯ Day 1 ì‹¤ìŠµ 3: QLoRA íŒŒì¸íŠœë‹ ì™„ë£Œ!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'training_result' in locals():\n",
    "    # ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œëœ ê²½ìš°\n",
    "    summary = {\n",
    "        \"âœ… ëª¨ë¸\": MODEL_NAME,\n",
    "        \"ğŸ“Š í•™ìŠµ ìƒ˜í”Œ\": len(train_dataset) if train_dataset else 0,\n",
    "        \"ğŸ¯ í•™ìŠµ íŒŒë¼ë¯¸í„°\": f\"{trainable_params:,} ({100*trainable_params/total_params:.3f}%)\",\n",
    "        \"â±ï¸ í•™ìŠµ ìŠ¤í…\": training_result.global_step if 'training_result' in locals() else \"N/A\",\n",
    "        \"ğŸ“‰ ìµœì¢… Loss\": f\"{training_result.training_loss:.4f}\" if 'training_result' in locals() else \"N/A\",\n",
    "        \"ğŸ’¾ ì €ì¥ ìœ„ì¹˜\": saved_model_path if 'saved_model_path' in locals() else \"N/A\"\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ“‹ í•™ìŠµ ê²°ê³¼ ìš”ì•½:\")\n",
    "    for key, value in summary.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ íŒŒì¸íŠœë‹ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "    print(f\"\\nğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:\")\n",
    "    if 'saved_model_path' in locals():\n",
    "        print(f\"  - {saved_model_path}/adapter_model.safetensors (LoRA ì–´ëŒ‘í„°)\")\n",
    "        print(f\"  - {saved_model_path}/adapter_config.json (LoRA ì„¤ì •)\")\n",
    "        print(f\"  - {saved_model_path}/tokenizer.json (í† í¬ë‚˜ì´ì €)\")\n",
    "        print(f\"  - {saved_model_path}/model_info.json (ëª¨ë¸ ì •ë³´)\")\n",
    "    \n",
    "    if 'trainer' in locals():\n",
    "        print(f\"  - {trainer.args.output_dir}/training_analysis.png (í•™ìŠµ ë¶„ì„ ê·¸ë˜í”„)\")\n",
    "        print(f\"  - {trainer.args.output_dir}/training_summary.json (í•™ìŠµ ìš”ì•½)\")\n",
    "        print(f\"  - {trainer.args.output_dir}/analysis_report.json (ë¶„ì„ ë¦¬í¬íŠ¸)\")\n",
    "    \n",
    "    print(f\"\\nğŸ”„ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "    print(f\"  1. 04_evaluation_and_comparison.ipynbì—ì„œ ì„±ëŠ¥ í‰ê°€\")\n",
    "    print(f\"  2. íŒŒì¸íŠœë‹ ì „í›„ ëª¨ë¸ ë¹„êµ\")\n",
    "    print(f\"  3. ROUGE, BLEU, ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì¸¡ì •\")\n",
    "    print(f\"  4. ì‹¤ì œ RAG ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ í…ŒìŠ¤íŠ¸\")\n",
    "    \n",
    "    # ëª¨ë¸ ì‚¬ìš© ê°€ì´ë“œ\n",
    "    print(f\"\\nğŸ“– ëª¨ë¸ ë¡œë“œ ë°©ë²•:\")\n",
    "    print(f\"```python\")\n",
    "    print(f\"from peft import PeftModel\")\n",
    "    print(f\"from transformers import AutoModelForCausalLM, AutoTokenizer\")\n",
    "    print(f\"\")\n",
    "    print(f\"# ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ\")\n",
    "    print(f\"base_model = AutoModelForCausalLM.from_pretrained('{MODEL_NAME}')\")\n",
    "    print(f\"tokenizer = AutoTokenizer.from_pretrained('{MODEL_NAME}')\")\n",
    "    print(f\"\")\n",
    "    print(f\"# LoRA ì–´ëŒ‘í„° ì ìš©\")\n",
    "    print(f\"model = PeftModel.from_pretrained(base_model, '{saved_model_path if 'saved_model_path' in locals() else './your_model_path'}')\")\n",
    "    print(f\"```\")\n",
    "\n",
    "else:\n",
    "    # í•™ìŠµì´ ì™„ë£Œë˜ì§€ ì•Šì€ ê²½ìš°\n",
    "    print(\"âš ï¸ íŒŒì¸íŠœë‹ì´ ì™„ì „íˆ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"\\nğŸ’¡ í™•ì¸ì‚¬í•­:\")\n",
    "    print(f\"  1. ë°ì´í„°ê°€ ì˜¬ë°”ë¥´ê²Œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸\")\n",
    "    print(f\"  2. GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•œì§€ í™•ì¸\")\n",
    "    print(f\"  3. í•™ìŠµ ì„¤ì •ì´ ì ì ˆí•œì§€ í™•ì¸\")\n",
    "    print(f\"\\nğŸ”„ ë‹¤ì‹œ ì‹œë„í•˜ë ¤ë©´:\")\n",
    "    print(f\"  1. ëŸ°íƒ€ì„ ì¬ì‹œì‘\")\n",
    "    print(f\"  2. ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸° (per_device_train_batch_size=1)\")\n",
    "    print(f\"  3. ìƒ˜í”Œ ìˆ˜ ì¤„ì´ê¸° (MAX_TRAIN_SAMPLES=200)\")\n",
    "\n",
    "print(f\"\\nğŸš€ QLoRAë¥¼ í™œìš©í•œ íš¨ìœ¨ì  íŒŒì¸íŠœë‹ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(f\"ğŸ“ í•œêµ­ì–´ RAGì— íŠ¹í™”ëœ EXAONE ëª¨ë¸ì„ ì–»ì—ˆìŠµë‹ˆë‹¤!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}