{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Day 1 실습 3: EXAONE 모델 QLoRA 파인튜닝\n",
    "\n",
    "## 학습 목표\n",
    "- EXAONE-3.5-2.4B-Instruct 모델을 활용한 경량 파인튜닝\n",
    "- QLoRA (4-bit 양자화 + LoRA) 기법 적용\n",
    "- Colab 무료 환경에서의 효율적 학습\n",
    "- GPU 메모리 최적화 및 모니터링\n",
    "- 체크포인트 저장 및 관리\n",
    "\n",
    "## 시간: 14:40–16:10 (90분)\n",
    "\n",
    "### 💡 QLoRA란?\n",
    "- **Q**: Quantization (4-bit 양자화로 메모리 사용량 75% 감소)\n",
    "- **LoRA**: Low-Rank Adaptation (파라미터의 0.3%만 학습)\n",
    "- **결과**: 기존 대비 1/4 메모리로 비슷한 성능 달성!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 필요한 라이브러리 설치 및 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Day 1 실습 3: QLoRA 파인튜닝을 위한 라이브러리 설치 및 확인\nprint(\"🚀 Day 1 실습 3: EXAONE 모델 QLoRA 파인튜닝\")\nprint(\"🔍 파인튜닝에 필요한 라이브러리 설치 및 확인 중...\")\n\n# 01, 02번에서 설치되지 않은 파인튜닝 전용 라이브러리들만 설치\nprint(\"📦 파인튜닝 전용 라이브러리 설치...\")\n\n# bitsandbytes 및 PEFT 관련 라이브러리들 (레거시 검증된 방법 사용)\n!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n\n# 허깅페이스 Hub 라이브러리 추가 (모델 업로드용)\n!pip install -q -U huggingface_hub\n\n# 기타 필요한 라이브러리들 (이미 설치된 경우 스킵됨)\n!pip install -q datasets jsonlines tqdm\n\nprint(\"✅ 파인튜닝 라이브러리 설치 완료!\")\n\n# 설치 검증\nprint(\"🧪 bitsandbytes 검증 중...\")\ntry:\n    import bitsandbytes as bnb\n    import torch\n    print(f\"✅ bitsandbytes 버전: {bnb.__version__}\")\n    \n    if torch.cuda.is_available():\n        print(f\"✅ CUDA 사용 가능: {torch.cuda.get_device_name(0)}\")\n        # 간단한 CUDA 테스트\n        test_tensor = torch.randn(10, 10).cuda()\n        print(\"✅ CUDA 텐서 생성 성공\")\n    else:\n        print(\"⚠️ CUDA를 사용할 수 없습니다\")\n\n    # 허깅페이스 Hub 확인\n    import huggingface_hub\n    print(f\"✅ huggingface_hub 버전: {huggingface_hub.__version__}\")\n        \n    print(\"🎉 파인튜닝 환경 준비 완료!\")\n    \nexcept Exception as e:\n    print(f\"❌ 검증 실패: {e}\")\n    print(\"💡 해결 방법:\")\n    print(\"1. 런타임 > 런타임 다시 시작\")\n    print(\"2. 셀을 다시 실행해보세요\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"📝 참고: 01, 02번 노트북에서 이미 설치된 라이브러리는 재사용됩니다\")\nprint(\"🔄 다음 셀부터 파인튜닝 작업을 시작합니다\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport jsonlines\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom typing import Dict, List, Any, Optional\nimport warnings\nfrom tqdm import tqdm\n\n# Transformers 관련\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer,\n    TrainerCallback,\n    DataCollatorForLanguageModeling\n)\n\n# PEFT 관련\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n    TaskType,\n    PeftModel\n)\n\n# Dataset\nfrom datasets import Dataset\n\n# 허깅페이스 Hub\nfrom huggingface_hub import HfApi, login, create_repo\n\nwarnings.filterwarnings('ignore')\n\n# 한글 폰트 설정 (matplotlib) - 학습 결과 분석 차트에서 한글이 깨지지 않도록 설정\n# 파인튜닝 과정의 Loss 곡선, GPU 메모리 사용량 등의 차트에서 한글 표시를 위해 필요\nprint(\"🔧 한글 폰트 설정 중...\")\n!apt-get update -qq\n!apt-get install fonts-nanum -qq > /dev/null\n\nimport matplotlib.font_manager as fm\n\n# 나눔바른고딕 폰트 경로 설정\nfontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n# 폰트 매니저에 폰트 추가 - 학습 분석 그래프에서 한글 표시를 위해 필요\nfm.fontManager.addfont(fontpath)\n\n# matplotlib 설정 업데이트 - 학습 과정 시각화에서 한글이 정상적으로 표시됨\nplt.rcParams.update({\n    'font.family': 'NanumBarunGothic',  # 기본 폰트를 나눔바른고딕으로 설정\n    'axes.unicode_minus': False         # 음수 기호 표시 문제 해결 (Loss 값 표시에서 중요)\n})\n\nprint(\"✅ 한글 폰트 설정 완료 - 학습 결과 차트에서 한글이 정상 표시됩니다\")\n\n# GPU 정보 확인 - 파인튜닝에 필요한 하드웨어 환경 점검\nprint(f\"🔧 파인튜닝 환경 정보:\")\nprint(f\"PyTorch 버전: {torch.__version__}\")\nprint(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU 개수: {torch.cuda.device_count()}\")\n    for i in range(torch.cuda.device_count()):\n        gpu_name = torch.cuda.get_device_name(i)\n        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n        print(f\"GPU {i}: {gpu_name}\")\n        print(f\"  전체 메모리: {gpu_memory:.1f} GB\")\n        \n        # 현재 메모리 사용량도 표시\n        allocated = torch.cuda.memory_allocated(i) / 1024**3\n        print(f\"  현재 사용 중: {allocated:.1f} GB\")\nelse:\n    print(\"⚠️ CUDA를 사용할 수 없습니다. CPU에서 실행됩니다 (매우 느림).\")\n    print(\"   GPU 환경에서 실행하는 것을 권장합니다.\")\n\nprint(\"\\n✅ 라이브러리 import 및 환경 설정 완료!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 및 토크나이저 로드\n",
    "\n",
    "### 🤖 EXAONE-3.5-2.4B-Instruct 모델\n",
    "- **개발사**: LG AI Research\n",
    "- **크기**: 2.4B 파라미터\n",
    "- **특징**: 한국어 특화, Instruction Following 최적화\n",
    "- **장점**: Colab 무료 환경에서 실행 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설정\n",
    "MODEL_NAME = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n",
    "MAX_LENGTH = 4096  # 최대 토큰 길이\n",
    "\n",
    "print(f\"🔄 모델 로드 중: {MODEL_NAME}\")\n",
    "\n",
    "# 토크나이저 로드\n",
    "print(\"📝 토크나이저 로드...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# 패딩 토큰 설정 (없는 경우)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"✅ 패딩 토큰을 EOS 토큰으로 설정\")\n",
    "\n",
    "print(f\"✅ 토크나이저 로드 완료\")\n",
    "print(f\"  어휘 크기: {tokenizer.vocab_size:,}\")\n",
    "print(f\"  패딩 토큰: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"  EOS 토큰: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. QLoRA 설정\n",
    "\n",
    "### ⚙️ 4-bit 양자화 설정\n",
    "- **nf4**: 4-bit NormalFloat 데이터 타입 사용\n",
    "- **double_quant**: 양자화 상수도 양자화 (더 많은 메모리 절약)\n",
    "- **compute_dtype**: 연산 시 사용할 데이터 타입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 4-bit 양자화 설정\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,                    # 4-bit 양자화 활성화\n    bnb_4bit_use_double_quant=True,       # 양자화 상수도 양자화 (더 많은 메모리 절약)\n    bnb_4bit_quant_type=\"nf4\",           # NormalFloat4 양자화 방식\n    bnb_4bit_compute_dtype=torch.bfloat16, # 연산용 데이터 타입\n)\n\nprint(\"⚙️ 4-bit 양자화 설정 완료:\")\nprint(f\"  양자화 방식: NF4\")\nprint(f\"  Double quantization: 활성화\")\nprint(f\"  연산 데이터 타입: bfloat16\")\n\n# LoRA 설정\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,         # 언어 모델링 태스크\n    r=8,                                  # LoRA rank (낮을수록 파라미터 적음)\n    lora_alpha=16,                        # LoRA scaling factor (보통 r의 2배)\n    lora_dropout=0.05,                    # LoRA 드롭아웃\n    target_modules=[                      # LoRA를 적용할 모듈들\n        \"q_proj\",    # Query projection\n        \"k_proj\",    # Key projection  \n        \"v_proj\",    # Value projection\n        \"out_proj\"   # Output projection\n    ],\n    bias=\"none\",                         # 바이어스는 학습하지 않음\n)\n\nprint(\"\\n🎯 LoRA 설정 완료:\")\nprint(f\"  Rank (r): {lora_config.r}\")\nprint(f\"  Alpha: {lora_config.lora_alpha}\")\nprint(f\"  Dropout: {lora_config.lora_dropout}\")\nprint(f\"  대상 모듈: {lora_config.target_modules}\")\n\n# 학습 가능한 파라미터 비율 추정\ntotal_params_estimate = 2.4e9  # 2.4B 파라미터\nlora_params_estimate = len(lora_config.target_modules) * lora_config.r * 2560 * 2  # 대략적 추정\nlora_ratio = lora_params_estimate / total_params_estimate\n\nprint(f\"\\n📊 예상 학습 파라미터 비율:\")\nprint(f\"  전체 파라미터: ~{total_params_estimate/1e9:.1f}B\")\nprint(f\"  LoRA 파라미터: ~{lora_params_estimate/1e6:.1f}M\")\nprint(f\"  학습 비율: ~{lora_ratio:.3%}\")\nprint(f\"  메모리 절약: ~75% (4-bit 양자화)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 로드 및 PEFT 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_model_with_qlora(model_name: str, bnb_config: BitsAndBytesConfig, \n                         lora_config: LoraConfig) -> tuple:\n    \"\"\"\n    QLoRA가 적용된 모델을 로드하는 함수\n    \n    Returns:\n        (model, trainable_params, total_params)\n    \"\"\"\n    print(\"🔄 모델 로드 중... (시간이 걸릴 수 있습니다)\")\n    \n    # 모델 로드 (4-bit 양자화 적용)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        device_map=\"auto\",              # 자동으로 GPU에 배치\n        trust_remote_code=True,\n        torch_dtype=torch.bfloat16,\n        # flash_attention_2는 일부 환경에서 문제가 될 수 있으므로 제거\n        # attn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else \"eager\"\n    )\n    \n    print(\"✅ 기본 모델 로드 완료\")\n    \n    # Gradient checkpointing 활성화 (메모리 절약)\n    model.gradient_checkpointing_enable()\n    \n    # k-bit training 준비\n    model = prepare_model_for_kbit_training(model)\n    \n    # LoRA 적용\n    print(\"🔄 LoRA 적용 중...\")\n    model = get_peft_model(model, lora_config)\n    \n    # 학습 가능한 파라미터 수 계산\n    trainable_params = 0\n    total_params = 0\n    \n    for name, param in model.named_parameters():\n        total_params += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    \n    print(\"✅ LoRA 적용 완료\")\n    \n    return model, trainable_params, total_params\n\n# 모델 설정 및 LoRA 설정은 이전 셀에서 정의된 변수(MODEL_NAME, bnb_config, lora_config)를 사용합니다.\nif 'bnb_config' in globals() and 'lora_config' in globals():\n    model, trainable_params, total_params = load_model_with_qlora(\n        MODEL_NAME, bnb_config, lora_config\n    )\n\n    # 파라미터 정보 출력\n    print(f\"\\n📊 모델 파라미터 정보:\")\n    print(f\"  총 파라미터: {total_params:,} ({total_params/1e9:.2f}B)\")\n    print(f\"  학습 가능한 파라미터: {trainable_params:,} ({trainable_params/1e6:.2f}M)\")\n    print(f\"  학습 비율: {100 * trainable_params / total_params:.3f}%\")\n\n    # GPU 메모리 사용량 확인\n    if torch.cuda.is_available():\n        print(f\"\\n💾 GPU 메모리 사용량:\")\n        for i in range(torch.cuda.device_count()):\n            allocated = torch.cuda.memory_allocated(i) / 1024**3\n            cached = torch.cuda.memory_reserved(i) / 1024**3\n            total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n            print(f\"  GPU {i}: {allocated:.1f}GB / {total_memory:.1f}GB (할당됨)\")\n            print(f\"          {cached:.1f}GB / {total_memory:.1f}GB (예약됨)\")\nelse:\n    print(\"❌ bnb_config 또는 lora_config가 정의되지 않았습니다.\")\n    print(\"💡 이전 셀들을 먼저 실행해주세요.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 데이터셋 로드 및 전처리\n",
    "\n",
    "### 📁 전처리된 RAFT 데이터셋 활용\n",
    "- 이전 실습에서 생성한 한국어 RAFT 데이터셋 사용\n",
    "- Chat template 적용된 메시지 형태\n",
    "- Train/Valid 분할 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_training_data() -> tuple:\n    \"\"\"\n    전처리된 학습 데이터를 로드하는 함수\n    \n    Returns:\n        (train_dataset, valid_dataset, metadata)\n    \"\"\"\n    print(\"📂 학습 데이터 로드 중...\")\n    \n    try:\n        # Train 데이터 로드\n        train_data = []\n        with jsonlines.open(\"processed_data/train_raft_ko.jsonl\", \"r\") as reader:\n            train_data = list(reader)\n        \n        # Valid 데이터 로드\n        valid_data = []\n        with jsonlines.open(\"processed_data/valid_raft_ko.jsonl\", \"r\") as reader:\n            valid_data = list(reader)\n        \n        # 메타데이터 로드\n        with open(\"processed_data/metadata.json\", \"r\", encoding=\"utf-8\") as f:\n            metadata = json.load(f)\n        \n        print(f\"✅ 데이터 로드 완료:\")\n        print(f\"  Train: {len(train_data)}개 샘플\")\n        print(f\"  Valid: {len(valid_data)}개 샘플\")\n        \n        return train_data, valid_data, metadata\n        \n    except FileNotFoundError as e:\n        print(f\"❌ 파일을 찾을 수 없습니다: {e}\")\n        print(\"💡 먼저 01_data_preprocessing_and_validation.ipynb를 실행하세요.\")\n        return None, None, None\n\ndef format_chat_template(messages: List[Dict[str, str]]) -> str:\n    \"\"\"\n    메시지를 EXAONE 채팅 템플릿 형태로 변환\n    \n    Args:\n        messages: 채팅 메시지 리스트\n        \n    Returns:\n        포맷팅된 텍스트\n    \"\"\"\n    return tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=False  # Assistant 응답까지 포함\n    )\n\ndef create_training_dataset(data: List[Dict], max_samples: Optional[int] = None) -> Dataset:\n    \"\"\"\n    학습용 데이터셋 생성 - Trainer와 호환되는 형식으로 생성\n    \n    Args:\n        data: 원본 데이터\n        max_samples: 최대 샘플 수 (None이면 전체 사용)\n        \n    Returns:\n        Hugging Face Dataset\n    \"\"\"\n    print(f\"🔄 학습 데이터셋 생성 중...\")\n    \n    # 샘플 수 제한\n    if max_samples is not None and len(data) > max_samples:\n        data = data[:max_samples]\n        print(f\"  샘플 수 제한: {max_samples}개\")\n    \n    texts = []\n    \n    for item in tqdm(data, desc=\"데이터 변환\"):\n        if \"messages\" not in item:\n            continue\n            \n        # 채팅 템플릿 적용\n        formatted_text = format_chat_template(item[\"messages\"])\n        texts.append(formatted_text)\n    \n    print(f\"✅ 텍스트 변환 완료: {len(texts)}개\")\n    \n    # 토크나이징을 여기서 수행하지 않고, 텍스트만 저장\n    # Trainer가 data_collator를 통해 배치별로 토크나이징 수행\n    dataset_dict = {\n        \"text\": texts\n    }\n    \n    dataset = Dataset.from_dict(dataset_dict)\n    print(f\"✅ 데이터셋 생성 완료: {len(dataset)}개 샘플\")\n    \n    return dataset\n\n# 데이터 로드\ntrain_data, valid_data, metadata = load_training_data()\n\nif train_data is not None:\n    # Colab 무료 환경 고려하여 샘플 수 제한 (필요시 조정)\n    MAX_TRAIN_SAMPLES = 400  # 학습 시간과 메모리를 고려한 제한\n    MAX_VALID_SAMPLES = 100\n    \n    # 데이터셋 생성\n    train_dataset = create_training_dataset(train_data, MAX_TRAIN_SAMPLES)\n    valid_dataset = create_training_dataset(valid_data, MAX_VALID_SAMPLES)\n    \n    # 데이터셋 정보 출력\n    print(f\"\\n📊 최종 데이터셋 정보:\")\n    print(f\"  Train 샘플: {len(train_dataset)}개\")\n    print(f\"  Valid 샘플: {len(valid_dataset)}개\")\n    \n    # 샘플 데이터 미리보기\n    print(f\"\\n📋 샘플 데이터:\")\n    if len(train_dataset) > 0:\n        sample_text = train_dataset[0][\"text\"]\n        print(f\"  텍스트 길이: {len(sample_text)} 문자\")\n        print(f\"  텍스트 미리보기: {sample_text[:200]}...\")\n        \n        # 토큰 길이 확인\n        tokens = tokenizer.encode(sample_text)\n        print(f\"  토큰 길이: {len(tokens)} 토큰\")\n\nelse:\n    print(\"❌ 데이터를 로드할 수 없어 학습을 진행할 수 없습니다.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. GPU 메모리 모니터링 콜백\n",
    "\n",
    "### 🔍 메모리 모니터링의 중요성\n",
    "- Colab 환경에서 GPU 메모리는 제한적 (15GB)\n",
    "- OOM(Out of Memory) 오류를 사전에 방지\n",
    "- 학습 중 메모리 사용량 실시간 추적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUMemoryCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    GPU 메모리 사용량을 모니터링하는 콜백 클래스\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, log_every_n_steps: int = 10, memory_threshold: float = 0.9):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            log_every_n_steps: 로그 출력 주기\n",
    "            memory_threshold: 메모리 사용량 경고 임계치 (90%)\n",
    "        \"\"\"\n",
    "        self.log_every_n_steps = log_every_n_steps\n",
    "        self.memory_threshold = memory_threshold\n",
    "        self.memory_history = []\n",
    "        \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"\n",
    "        각 스텝 종료 후 호출되는 메서드\n",
    "        \"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "            \n",
    "        if state.global_step % self.log_every_n_steps == 0:\n",
    "            # 메모리 정보 수집\n",
    "            memory_info = self._collect_memory_info()\n",
    "            self.memory_history.append({\n",
    "                \"step\": state.global_step,\n",
    "                \"memory_info\": memory_info\n",
    "            })\n",
    "            \n",
    "            # 로그 출력\n",
    "            self._log_memory_info(state.global_step, memory_info)\n",
    "            \n",
    "            # 메모리 경고\n",
    "            for i, info in enumerate(memory_info):\n",
    "                if info[\"usage_ratio\"] > self.memory_threshold:\n",
    "                    print(f\"⚠️ GPU {i} 메모리 사용량 경고: {info['usage_ratio']:.1%}\")\n",
    "    \n",
    "    def _collect_memory_info(self) -> List[Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        GPU 메모리 정보 수집\n",
    "        \"\"\"\n",
    "        memory_info = []\n",
    "        \n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "            total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "            \n",
    "            memory_info.append({\n",
    "                \"gpu_id\": i,\n",
    "                \"allocated_gb\": allocated,\n",
    "                \"reserved_gb\": reserved,\n",
    "                \"total_gb\": total,\n",
    "                \"usage_ratio\": reserved / total if total > 0 else 0\n",
    "            })\n",
    "        \n",
    "        return memory_info\n",
    "    \n",
    "    def _log_memory_info(self, step: int, memory_info: List[Dict[str, float]]):\n",
    "        \"\"\"\n",
    "        메모리 정보 로그 출력\n",
    "        \"\"\"\n",
    "        print(f\"\\n📊 Step {step} - GPU 메모리 상태:\")\n",
    "        for info in memory_info:\n",
    "            gpu_id = info[\"gpu_id\"]\n",
    "            allocated = info[\"allocated_gb\"]\n",
    "            reserved = info[\"reserved_gb\"]\n",
    "            total = info[\"total_gb\"]\n",
    "            usage = info[\"usage_ratio\"]\n",
    "            \n",
    "            print(f\"  GPU {gpu_id}: {reserved:.1f}GB/{total:.1f}GB ({usage:.1%}) \"\n",
    "                  f\"[할당됨: {allocated:.1f}GB]\")\n",
    "    \n",
    "    def get_memory_history(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        메모리 사용 히스토리 반환\n",
    "        \"\"\"\n",
    "        return self.memory_history\n",
    "\n",
    "# 콜백 인스턴스 생성\n",
    "memory_callback = GPUMemoryCallback(log_every_n_steps=5, memory_threshold=0.85)\n",
    "\n",
    "print(\"🔍 GPU 메모리 모니터링 콜백 생성 완료\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  모니터링 주기: 5 스텝마다\")\n",
    "    print(f\"  경고 임계치: 85%\")\n",
    "else:\n",
    "    print(\"  CPU 모드에서는 메모리 모니터링이 비활성화됩니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 학습 설정\n",
    "\n",
    "### 🎯 Colab 무료 환경 최적화 설정\n",
    "- **배치 크기**: 1 (메모리 절약)\n",
    "- **Gradient Accumulation**: 32 (실질적 배치 크기 = 32)\n",
    "- **Learning Rate**: 1e-4 (LoRA에 적합한 값)\n",
    "- **FP16**: 메모리와 속도 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 학습 설정\ndef create_training_arguments() -> TrainingArguments:\n    \"\"\"\n    학습 파라미터 설정\n    \"\"\"\n    \n    # 출력 디렉토리 설정\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    output_dir = f\"./fine_tuned_model_{timestamp}\"\n    \n    return TrainingArguments(\n        # 📁 디렉토리 설정\n        output_dir=output_dir,\n        logging_dir=f\"{output_dir}/logs\",\n        \n        # 🏋️ 학습 설정\n        num_train_epochs=1,                    # 에포크 수 (시간 고려하여 1)\n        per_device_train_batch_size=1,         # GPU당 배치 크기\n        per_device_eval_batch_size=1,          # 평가 배치 크기\n        gradient_accumulation_steps=32,        # 그래디언트 누적 (실질 배치=32)\n        \n        # 📈 최적화 설정\n        learning_rate=1e-4,                   # 학습률\n        warmup_steps=50,                      # 웜업 스텝\n        optim=\"paged_adamw_8bit\",            # 8bit AdamW 옵티마이저\n        \n        # 💾 메모리 최적화\n        fp16=True,                           # FP16 정밀도\n        dataloader_pin_memory=False,         # 메모리 핀 비활성화\n        gradient_checkpointing=True,         # 그래디언트 체크포인팅\n        \n        # 📊 로깅 및 평가\n        logging_steps=5,                     # 로그 출력 주기\n        eval_steps=50,                       # 평가 주기\n        eval_strategy=\"steps\",               # 스텝 기반 평가 (evaluation_strategy → eval_strategy)\n        \n        # 💾 저장 설정\n        save_steps=100,                      # 체크포인트 저장 주기\n        save_total_limit=2,                  # 최대 체크포인트 개수\n        save_strategy=\"steps\",               # 스텝 기반 저장\n        \n        # 🎯 기타 설정\n        remove_unused_columns=False,         # 사용하지 않는 컬럼 제거 안함\n        report_to=\"none\",                   # wandb 등 리포트 비활성화\n        load_best_model_at_end=True,         # 최고 성능 모델 로드\n        metric_for_best_model=\"eval_loss\",   # 최고 모델 선택 기준\n        greater_is_better=False,             # Loss는 낮을수록 좋음\n        \n        # 🔄 재현성\n        seed=42,\n        data_seed=42,\n    )\n\n# 학습 설정 생성\ntraining_args = create_training_arguments()\n\nprint(\"⚙️ 학습 설정 완료:\")\nprint(f\"  출력 디렉토리: {training_args.output_dir}\")\nprint(f\"  에포크 수: {training_args.num_train_epochs}\")\nprint(f\"  배치 크기: {training_args.per_device_train_batch_size}\")\nprint(f\"  그래디언트 누적: {training_args.gradient_accumulation_steps}\")\nprint(f\"  실질적 배치 크기: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  학습률: {training_args.learning_rate}\")\nprint(f\"  FP16: {training_args.fp16}\")\nprint(f\"  옵티마이저: {training_args.optim}\")\n\n# 예상 학습 시간 계산\nif train_dataset is not None:\n    total_steps = len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs\n    estimated_time_minutes = total_steps * 0.5  # 스텝당 약 30초 추정\n    \n    print(f\"\\n⏱️ 예상 학습 시간:\")\n    print(f\"  총 스텝: {total_steps}\")\n    print(f\"  예상 시간: {estimated_time_minutes:.0f}분 ({estimated_time_minutes/60:.1f}시간)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 데이터 콜레이터 설정\n",
    "\n",
    "### 📦 Dynamic Padding\n",
    "- 배치마다 최적 길이로 패딩 (메모리 효율)\n",
    "- Language Modeling용 라벨 자동 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 데이터 콜레이터 설정 - 텍스트를 토크나이징하고 배치 처리\ndef tokenize_function(examples):\n    \"\"\"\n    텍스트를 토크나이징하는 함수\n    \"\"\"\n    # 텍스트를 토크나이징\n    tokenized = tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        padding=False,  # 배치 시 동적 패딩\n        max_length=MAX_LENGTH,\n        return_tensors=\"pt\" if len(examples[\"text\"]) == 1 else None\n    )\n    \n    # labels는 input_ids와 동일 (language modeling)\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    \n    return tokenized\n\n# 데이터셋에 토크나이징 적용\nif 'train_dataset' in locals() and train_dataset is not None:\n    print(\"🔄 데이터셋 토크나이징 중...\")\n    \n    # 토크나이징 적용\n    train_dataset = train_dataset.map(\n        tokenize_function,\n        batched=True,\n        remove_columns=[\"text\"],  # 원본 텍스트 제거\n        desc=\"토크나이징 진행\"\n    )\n    \n    valid_dataset = valid_dataset.map(\n        tokenize_function,\n        batched=True,\n        remove_columns=[\"text\"],  # 원본 텍스트 제거\n        desc=\"토크나이징 진행\"\n    )\n    \n    print(\"✅ 토크나이징 완료\")\n\n# 데이터 콜레이터 설정\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,  # Causal Language Modeling (MLM 아님)\n    pad_to_multiple_of=8,  # 텐서 연산 최적화를 위해 8의 배수로 패딩\n)\n\nprint(\"📦 데이터 콜레이터 설정 완료:\")\nprint(f\"  MLM: {data_collator.mlm}\")\nprint(f\"  패딩 단위: {data_collator.pad_to_multiple_of}\")\nprint(f\"  패딩 토큰 ID: {tokenizer.pad_token_id}\")\n\n# 샘플 데이터 확인\nif 'train_dataset' in locals() and len(train_dataset) > 0:\n    print(f\"\\n📋 토크나이징된 샘플 확인:\")\n    sample = train_dataset[0]\n    print(f\"  Input IDs 길이: {len(sample['input_ids'])}\")\n    print(f\"  Labels 길이: {len(sample['labels'])}\")\n    print(f\"  Attention Mask 길이: {len(sample['attention_mask'])}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Trainer 설정 및 학습 시작\n",
    "\n",
    "### 🚀 파인튜닝 실행\n",
    "- 실시간 GPU 메모리 모니터링\n",
    "- 자동 체크포인트 저장\n",
    "- 평가 메트릭 추적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 전 사전 점검\n",
    "def pre_training_check():\n",
    "    \"\"\"\n",
    "    학습 시작 전 환경 점검\n",
    "    \"\"\"\n",
    "    print(\"🔍 학습 전 환경 점검:\")\n",
    "    \n",
    "    # GPU 메모리 확인\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            print(f\"  GPU {i}: {allocated:.1f}GB/{total_memory:.1f}GB 사용 중\")\n",
    "            \n",
    "            if allocated / total_memory > 0.8:\n",
    "                print(f\"  ⚠️ GPU {i} 메모리 사용량이 높습니다. 학습 중 OOM 위험이 있습니다.\")\n",
    "    \n",
    "    # 모델 상태 확인\n",
    "    if model.training:\n",
    "        print(\"  ✅ 모델이 학습 모드입니다.\")\n",
    "    else:\n",
    "        print(\"  ⚠️ 모델이 평가 모드입니다. 학습 모드로 전환됩니다.\")\n",
    "        model.train()\n",
    "    \n",
    "    # 데이터셋 크기 확인\n",
    "    print(f\"  데이터: Train {len(train_dataset)}개, Valid {len(valid_dataset)}개\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Trainer 생성\n",
    "if train_dataset is not None and len(train_dataset) > 0:\n",
    "    print(\"🔄 Trainer 생성 중...\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset if len(valid_dataset) > 0 else None,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[memory_callback],  # GPU 메모리 모니터링 콜백\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Trainer 생성 완료\")\n",
    "    \n",
    "    # 학습 전 점검\n",
    "    if pre_training_check():\n",
    "        print(\"\\n🚀 파인튜닝 시작!\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 학습 시작 시간 기록\n",
    "        training_start_time = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            # 실제 학습 실행\n",
    "            training_result = trainer.train()\n",
    "            \n",
    "            # 학습 완료 시간 기록\n",
    "            training_end_time = datetime.now()\n",
    "            training_duration = training_end_time - training_start_time\n",
    "            \n",
    "            print(\"\\n🎉 파인튜닝 완료!\")\n",
    "            print(\"=\" * 50)\n",
    "            print(f\"학습 시간: {training_duration}\")\n",
    "            print(f\"최종 Loss: {training_result.training_loss:.4f}\")\n",
    "            \n",
    "            # 학습 결과 저장\n",
    "            training_summary = {\n",
    "                \"model_name\": MODEL_NAME,\n",
    "                \"training_start\": training_start_time.isoformat(),\n",
    "                \"training_end\": training_end_time.isoformat(),\n",
    "                \"training_duration\": str(training_duration),\n",
    "                \"final_loss\": training_result.training_loss,\n",
    "                \"total_steps\": training_result.global_step,\n",
    "                \"trainable_parameters\": trainable_params,\n",
    "                \"total_parameters\": total_params,\n",
    "                \"training_samples\": len(train_dataset),\n",
    "                \"validation_samples\": len(valid_dataset)\n",
    "            }\n",
    "            \n",
    "            # 결과 저장\n",
    "            with open(f\"{training_args.output_dir}/training_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(training_summary, f, ensure_ascii=False, indent=2, default=str)\n",
    "            \n",
    "            print(f\"\\n💾 학습 결과 저장: {training_args.output_dir}/training_summary.json\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n⏹️ 사용자에 의해 학습이 중단되었습니다.\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                print(\"\\n❌ GPU 메모리 부족으로 학습이 중단되었습니다.\")\n",
    "                print(\"💡 해결 방법:\")\n",
    "                print(\"   1. 배치 크기를 더 줄여보세요 (per_device_train_batch_size=1)\")\n",
    "                print(\"   2. 그래디언트 누적 단계를 늘려보세요 (gradient_accumulation_steps=64)\")\n",
    "                print(\"   3. 최대 토큰 길이를 줄여보세요 (MAX_LENGTH=2048)\")\n",
    "                print(\"   4. 학습 샘플 수를 줄여보세요 (MAX_TRAIN_SAMPLES=200)\")\n",
    "            else:\n",
    "                print(f\"\\n❌ 학습 중 오류 발생: {e}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ 예상치 못한 오류 발생: {e}\")\n",
    "            \n",
    "else:\n",
    "    print(\"❌ 학습 데이터가 없어 파인튜닝을 진행할 수 없습니다.\")\n",
    "    print(\"💡 먼저 01_data_preprocessing_and_validation.ipynb를 실행하세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 학습 결과 분석 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_training_results(trainer: Trainer, memory_callback: GPUMemoryCallback):\n",
    "    \"\"\"\n",
    "    학습 결과 분석 및 시각화\n",
    "    \"\"\"\n",
    "    print(\"📊 학습 결과 분석 중...\")\n",
    "    \n",
    "    # 1. 학습 로그 분석\n",
    "    log_history = trainer.state.log_history\n",
    "    \n",
    "    if not log_history:\n",
    "        print(\"⚠️ 학습 로그가 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    # 학습 및 평가 로그 분리\n",
    "    train_logs = [log for log in log_history if 'loss' in log and 'eval_loss' not in log]\n",
    "    eval_logs = [log for log in log_history if 'eval_loss' in log]\n",
    "    \n",
    "    print(f\"\\n📈 학습 진행 상황:\")\n",
    "    print(f\"  총 학습 스텝: {len(train_logs)}\")\n",
    "    print(f\"  평가 횟수: {len(eval_logs)}\")\n",
    "    \n",
    "    if train_logs:\n",
    "        initial_loss = train_logs[0].get('loss', 0)\n",
    "        final_loss = train_logs[-1].get('loss', 0)\n",
    "        loss_improvement = initial_loss - final_loss\n",
    "        \n",
    "        print(f\"  초기 Loss: {initial_loss:.4f}\")\n",
    "        print(f\"  최종 Loss: {final_loss:.4f}\")\n",
    "        print(f\"  Loss 개선: {loss_improvement:.4f} ({loss_improvement/initial_loss:.1%})\")\n",
    "    \n",
    "    # 2. 시각화\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 2-1. Training Loss 곡선\n",
    "    if train_logs:\n",
    "        steps = [log.get('step', 0) for log in train_logs]\n",
    "        losses = [log.get('loss', 0) for log in train_logs]\n",
    "        \n",
    "        axes[0, 0].plot(steps, losses, 'b-', label='Training Loss', linewidth=2)\n",
    "        axes[0, 0].set_xlabel('Steps')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].set_title('Training Loss Curve')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].legend()\n",
    "    \n",
    "    # 2-2. Evaluation Loss 곡선\n",
    "    if eval_logs:\n",
    "        eval_steps = [log.get('step', 0) for log in eval_logs]\n",
    "        eval_losses = [log.get('eval_loss', 0) for log in eval_logs]\n",
    "        \n",
    "        axes[0, 1].plot(eval_steps, eval_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "        axes[0, 1].set_xlabel('Steps')\n",
    "        axes[0, 1].set_ylabel('Loss')\n",
    "        axes[0, 1].set_title('Validation Loss Curve')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].legend()\n",
    "    \n",
    "    # 2-3. GPU 메모리 사용량\n",
    "    memory_history = memory_callback.get_memory_history()\n",
    "    if memory_history:\n",
    "        memory_steps = [item['step'] for item in memory_history]\n",
    "        memory_usage = [item['memory_info'][0]['usage_ratio'] * 100 \n",
    "                       for item in memory_history if item['memory_info']]\n",
    "        \n",
    "        if memory_usage:\n",
    "            axes[1, 0].plot(memory_steps, memory_usage, 'g-', label='GPU Memory Usage', linewidth=2)\n",
    "            axes[1, 0].axhline(y=85, color='orange', linestyle='--', label='Warning (85%)')\n",
    "            axes[1, 0].axhline(y=95, color='red', linestyle='--', label='Critical (95%)')\n",
    "            axes[1, 0].set_xlabel('Steps')\n",
    "            axes[1, 0].set_ylabel('Memory Usage (%)')\n",
    "            axes[1, 0].set_title('GPU Memory Usage')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "            axes[1, 0].legend()\n",
    "    \n",
    "    # 2-4. Learning Rate 스케줄\n",
    "    lr_logs = [log.get('learning_rate', 0) for log in train_logs if 'learning_rate' in log]\n",
    "    if lr_logs:\n",
    "        lr_steps = [log.get('step', 0) for log in train_logs if 'learning_rate' in log]\n",
    "        \n",
    "        axes[1, 1].plot(lr_steps, lr_logs, 'm-', label='Learning Rate', linewidth=2)\n",
    "        axes[1, 1].set_xlabel('Steps')\n",
    "        axes[1, 1].set_ylabel('Learning Rate')\n",
    "        axes[1, 1].set_title('Learning Rate Schedule')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 그래프 저장\n",
    "    output_dir = trainer.args.output_dir\n",
    "    plt.savefig(f\"{output_dir}/training_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n💾 학습 분석 그래프 저장: {output_dir}/training_analysis.png\")\n",
    "    \n",
    "    # 3. 상세 분석 리포트 생성\n",
    "    analysis_report = {\n",
    "        \"training_summary\": {\n",
    "            \"total_steps\": len(train_logs),\n",
    "            \"evaluation_count\": len(eval_logs),\n",
    "            \"initial_loss\": train_logs[0].get('loss', 0) if train_logs else 0,\n",
    "            \"final_loss\": train_logs[-1].get('loss', 0) if train_logs else 0,\n",
    "            \"loss_improvement\": loss_improvement if train_logs else 0,\n",
    "            \"improvement_percentage\": (loss_improvement/initial_loss*100) if train_logs and initial_loss > 0 else 0\n",
    "        },\n",
    "        \"memory_analysis\": {\n",
    "            \"max_memory_usage\": max(memory_usage) if memory_usage else 0,\n",
    "            \"avg_memory_usage\": np.mean(memory_usage) if memory_usage else 0,\n",
    "            \"memory_warnings\": sum(1 for usage in memory_usage if usage > 85) if memory_usage else 0\n",
    "        },\n",
    "        \"recommendations\": []\n",
    "    }\n",
    "    \n",
    "    # 권장사항 생성\n",
    "    if analysis_report[\"training_summary\"][\"improvement_percentage\"] < 5:\n",
    "        analysis_report[\"recommendations\"].append(\"Loss 개선이 미미합니다. 더 많은 에포크나 다른 학습률을 시도해보세요.\")\n",
    "    \n",
    "    if analysis_report[\"memory_analysis\"][\"max_memory_usage\"] > 90:\n",
    "        analysis_report[\"recommendations\"].append(\"GPU 메모리 사용량이 높습니다. 배치 크기를 줄이는 것을 고려하세요.\")\n",
    "    \n",
    "    if not analysis_report[\"recommendations\"]:\n",
    "        analysis_report[\"recommendations\"].append(\"학습이 안정적으로 진행되었습니다!\")\n",
    "    \n",
    "    # 리포트 저장\n",
    "    with open(f\"{output_dir}/analysis_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(analysis_report, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"💾 분석 리포트 저장: {output_dir}/analysis_report.json\")\n",
    "    \n",
    "    return analysis_report\n",
    "\n",
    "# 학습 완료 후 결과 분석 실행\n",
    "if 'trainer' in locals() and 'training_result' in locals():\n",
    "    analysis_report = analyze_training_results(trainer, memory_callback)\n",
    "    \n",
    "    # 분석 결과 요약 출력\n",
    "    print(\"\\n📋 학습 결과 요약:\")\n",
    "    print(f\"  Loss 개선: {analysis_report['training_summary']['improvement_percentage']:.1f}%\")\n",
    "    print(f\"  최대 메모리 사용: {analysis_report['memory_analysis']['max_memory_usage']:.1f}%\")\n",
    "    print(f\"\\n💡 권장사항:\")\n",
    "    for rec in analysis_report['recommendations']:\n",
    "        print(f\"   - {rec}\")\n",
    "else:\n",
    "    print(\"⚠️ 학습이 완료되지 않아 결과 분석을 수행할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 모델 저장 및 테스트\n",
    "\n",
    "### 💾 학습된 모델 저장\n",
    "- LoRA 어댑터만 저장 (용량 효율적)\n",
    "- 추후 쉬운 로드를 위한 설정 파일 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def save_fine_tuned_model(trainer: Trainer, output_path: str = None) -> str:\n    \"\"\"\n    파인튜닝된 모델 저장\n    \n    Args:\n        trainer: 학습된 Trainer 객체\n        output_path: 저장 경로 (None이면 자동 생성)\n        \n    Returns:\n        저장된 모델 경로\n    \"\"\"\n    if output_path is None:\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        output_path = f\"./exaone_raft_lora_{timestamp}\"\n    \n    print(f\"💾 모델 저장 중: {output_path}\")\n    \n    # 모델 저장 (LoRA 어댑터만)\n    trainer.model.save_pretrained(output_path)\n    \n    # 토크나이저도 함께 저장\n    tokenizer.save_pretrained(output_path)\n    \n    # 모델 정보 저장 (JSON 직렬화 가능한 형태로 변환)\n    model_info = {\n        \"base_model\": MODEL_NAME,\n        \"model_type\": \"QLoRA\",\n        \"task_type\": \"RAG Fine-tuning\",\n        \"save_timestamp\": datetime.now().isoformat(),\n        \"lora_config\": {\n            \"r\": lora_config.r,\n            \"lora_alpha\": lora_config.lora_alpha,\n            \"lora_dropout\": lora_config.lora_dropout,\n            \"target_modules\": list(lora_config.target_modules)  # set을 list로 변환\n        },\n        \"training_info\": {\n            \"trainable_parameters\": trainable_params,\n            \"total_parameters\": total_params,\n            \"training_samples\": len(train_dataset) if train_dataset else 0\n        }\n    }\n    \n    with open(f\"{output_path}/model_info.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(model_info, f, ensure_ascii=False, indent=2)\n    \n    print(f\"✅ 모델 저장 완료: {output_path}\")\n    print(f\"   - LoRA 어댑터: adapter_model.safetensors\")\n    print(f\"   - 설정 파일: adapter_config.json\")\n    print(f\"   - 토크나이저: tokenizer.json, tokenizer_config.json\")\n    print(f\"   - 모델 정보: model_info.json\")\n    \n    return output_path\n\ndef upload_to_huggingface(model_path: str, repo_name: str = \"ryanu/my-exaone-raft-model\", private: bool = False):\n    \"\"\"\n    파인튜닝된 모델을 허깅페이스 Hub에 업로드\n    \n    Args:\n        model_path: 로컬 모델 경로\n        repo_name: 허깅페이스 리포지토리 이름 (기본: ryanu/my-exaone-raft-model)\n        private: 프라이빗 리포지토리 여부\n    \"\"\"\n    \n    print(\"\\n🚀 허깅페이스 Hub 업로드 준비 중...\")\n    \n    # 허깅페이스 로그인 확인\n    try:\n        api = HfApi()\n        user_info = api.whoami()\n        username = user_info[\"name\"]\n        print(f\"✅ 허깅페이스 로그인 확인: {username}\")\n        \n    except Exception as e:\n        print(\"❌ 허깅페이스 로그인이 필요합니다!\")\n        print(\"💡 다음 단계를 따라주세요:\")\n        print(\"1. https://huggingface.co/settings/tokens 에서 토큰 생성\")\n        print(\"2. 아래 코드 실행:\")\n        print(\"   from huggingface_hub import login\")\n        print(\"   login()  # 토큰 입력\")\n        print(\"3. 또는 환경변수 설정:\")\n        print(\"   import os\")\n        print(\"   os.environ['HF_TOKEN'] = 'your_token_here'\")\n        return None\n    \n    try:\n        print(f\"📦 리포지토리 생성 중: {repo_name}\")\n        \n        # 리포지토리 생성 (이미 존재하면 무시)\n        try:\n            create_repo(\n                repo_id=repo_name,\n                private=private,\n                exist_ok=True,\n                repo_type=\"model\"\n            )\n            print(f\"✅ 리포지토리 생성 완료: {repo_name}\")\n        except Exception as e:\n            if \"already exists\" in str(e):\n                print(f\"✅ 기존 리포지토리 사용: {repo_name}\")\n            else:\n                raise e\n        \n        # 모델 카드 생성\n        model_card_content = f\"\"\"---\nlicense: apache-2.0\nbase_model: {MODEL_NAME}\ntags:\n- peft\n- lora\n- korean\n- rag\n- exaone\nlanguage:\n- ko\n---\n\n# EXAONE RAG Fine-tuned Model with LoRA\n\n이 모델은 EXAONE-3.5-2.4B-Instruct를 기반으로 한국어 RAG 데이터셋으로 파인튜닝된 모델입니다.\n\n## Model Details\n\n- **Base Model**: {MODEL_NAME}\n- **Fine-tuning Method**: QLoRA (4-bit quantization + LoRA)\n- **Task**: Retrieval-Augmented Generation (RAG)\n- **Language**: Korean\n- **Training Data**: RAFT methodology based Korean RAG dataset\n\n## Usage\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n# 베이스 모델과 토크나이저 로드\nbase_model = AutoModelForCausalLM.from_pretrained(\"{MODEL_NAME}\")\ntokenizer = AutoTokenizer.from_pretrained(\"{MODEL_NAME}\")\n\n# LoRA 어댑터 적용\nmodel = PeftModel.from_pretrained(base_model, \"{repo_name}\")\n\n# 추론 예시\nmessages = [\n    {{\"role\": \"system\", \"content\": \"주어진 컨텍스트를 바탕으로 질문에 답변하세요.\"}},\n    {{\"role\": \"user\", \"content\": \"컨텍스트: 한국의 수도는 서울입니다. 질문: 한국의 수도는?\"\"}}\n]\n\ninput_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model.generate(inputs, max_new_tokens=100, temperature=0.7)\n    \nresponse = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\nprint(response)\n```\n\n## Training Details\n\n- **Training Framework**: Hugging Face Transformers + PEFT\n- **Optimization**: 8-bit AdamW\n- **Learning Rate**: 1e-4\n- **Batch Size**: 32 (with gradient accumulation)\n- **Precision**: FP16\n\n## Performance\n\n이 모델은 베이스라인 EXAONE 모델 대비 한국어 RAG 태스크에서 향상된 성능을 보입니다.\n자세한 평가 결과는 학습 리포지토리를 참고하세요.\n\"\"\"\n        \n        # 모델 카드 저장\n        with open(f\"{model_path}/README.md\", \"w\", encoding=\"utf-8\") as f:\n            f.write(model_card_content)\n        \n        print(\"📄 모델 카드 생성 완료\")\n        \n        # 파일 업로드\n        print(\"📤 파일 업로드 중... (시간이 걸릴 수 있습니다)\")\n        \n        # 업로드할 파일들\n        files_to_upload = [\n            \"adapter_config.json\",\n            \"adapter_model.safetensors\", \n            \"tokenizer.json\",\n            \"tokenizer_config.json\",\n            \"model_info.json\",\n            \"README.md\"\n        ]\n        \n        for file_name in files_to_upload:\n            file_path = os.path.join(model_path, file_name)\n            if os.path.exists(file_path):\n                api.upload_file(\n                    path_or_fileobj=file_path,\n                    path_in_repo=file_name,\n                    repo_id=repo_name,\n                    repo_type=\"model\"\n                )\n                print(f\"   ✅ {file_name} 업로드 완료\")\n            else:\n                print(f\"   ⚠️ {file_name} 파일을 찾을 수 없음\")\n        \n        print(f\"\\n🎉 허깅페이스 업로드 완료!\")\n        print(f\"🔗 모델 URL: https://huggingface.co/{repo_name}\")\n        print(f\"📊 리포지토리 설정:\")\n        print(f\"   - 이름: {repo_name}\")\n        print(f\"   - 공개 여부: {'Private' if private else 'Public'}\")\n        print(f\"   - 모델 타입: LoRA Adapter\")\n        \n        return repo_name\n        \n    except Exception as e:\n        print(f\"❌ 업로드 실패: {e}\")\n        print(\"💡 문제 해결 방법:\")\n        print(\"1. 토큰 권한 확인 (write 권한 필요)\")\n        print(\"2. 네트워크 연결 확인\")\n        print(\"3. 리포지토리 이름 중복 확인\")\n        return None\n\ndef test_fine_tuned_model(model, tokenizer, test_prompts: List[str]):\n    \"\"\"\n    파인튜닝된 모델 테스트\n    \"\"\"\n    print(\"🧪 파인튜닝된 모델 테스트 중...\")\n    \n    model.eval()\n    \n    for i, prompt in enumerate(test_prompts, 1):\n        print(f\"\\n📝 테스트 {i}:\")\n        print(f\"입력: {prompt[:100]}{'...' if len(prompt) > 100 else ''}\")\n        \n        # 메시지 형태로 변환\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"당신은 주어진 컨텍스트를 바탕으로 질문에 정확하고 도움이 되는 답변을 제공하는 AI 어시스턴트입니다.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": prompt\n            }\n        ]\n        \n        # 입력 토큰화\n        input_text = tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n        \n        inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n        if torch.cuda.is_available():\n            inputs = inputs.cuda()\n        \n        # 생성\n        with torch.no_grad():\n            outputs = model.generate(\n                inputs,\n                max_new_tokens=150,\n                temperature=0.7,\n                do_sample=True,\n                pad_token_id=tokenizer.eos_token_id,\n                eos_token_id=tokenizer.eos_token_id\n            )\n        \n        # 응답 디코딩 (입력 부분 제거)\n        generated_text = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n        \n        print(f\"출력: {generated_text.strip()}\")\n        print(\"-\" * 80)\n\n# 모델 저장 및 테스트 실행 - 개선된 조건 확인\ndef check_training_completion():\n    \"\"\"\n    학습 완료 상태를 확인하는 함수\n    \"\"\"\n    # 변수 존재 확인\n    trainer_exists = 'trainer' in locals() or 'trainer' in globals()\n    training_result_exists = 'training_result' in locals() or 'training_result' in globals()\n    \n    print(f\"🔍 학습 완료 상태 확인:\")\n    print(f\"  Trainer 존재: {trainer_exists}\")\n    print(f\"  Training Result 존재: {training_result_exists}\")\n    \n    if trainer_exists:\n        # trainer 객체가 존재하고 학습이 실행되었는지 확인\n        try:\n            # 글로벌 스코프에서 trainer 확인\n            current_trainer = globals().get('trainer') or locals().get('trainer')\n            if current_trainer and hasattr(current_trainer, 'state'):\n                print(f\"  학습된 스텝 수: {current_trainer.state.global_step}\")\n                return current_trainer.state.global_step > 0, current_trainer\n        except:\n            pass\n    \n    return False, None\n\n# 학습 완료 확인\ntraining_completed, current_trainer = check_training_completion()\n\nif training_completed and current_trainer:\n    print(\"\\n✅ 학습이 완료되었습니다. 모델을 저장합니다.\")\n    \n    # 모델 저장\n    saved_model_path = save_fine_tuned_model(current_trainer)\n    \n    # 저장된 파일들의 존재 확인\n    import os\n    essential_files = [\n        f\"{saved_model_path}/adapter_config.json\",\n        f\"{saved_model_path}/adapter_model.safetensors\"\n    ]\n    \n    print(f\"\\n🔍 저장된 파일 확인:\")\n    all_files_exist = True\n    for file_path in essential_files:\n        if os.path.exists(file_path):\n            file_size = os.path.getsize(file_path)\n            print(f\"  ✅ {os.path.basename(file_path)}: {file_size:,} bytes\")\n        else:\n            print(f\"  ❌ {os.path.basename(file_path)}: 파일 없음\")\n            all_files_exist = False\n    \n    if all_files_exist:\n        print(f\"\\n🎉 모델 저장 성공! 04번 노트북에서 사용 가능합니다.\")\n        \n        # 자동으로 Hugging Face에 업로드\n        print(f\"\\n🚀 Hugging Face에 업로드 중...\")\n        try:\n            # 허깅페이스에 업로드 (로그인이 되어있다면)\n            repo_name = upload_to_huggingface(\n                model_path=saved_model_path,\n                repo_name=\"ryanu/my-exaone-raft-model\",\n                private=False\n            )\n            \n            if repo_name:\n                print(f\"\\n🎉 업로드 완료!\")\n                print(f\"🔗 모델 주소: https://huggingface.co/{repo_name}\")\n                print(f\"💡 Day 2-3 실습에서 이 주소를 사용하여 모델을 불러올 수 있습니다!\")\n                \n                # Day 2-3에서 사용할 모델 주소를 파일로 저장\n                model_config = {\n                    \"model_name\": repo_name,\n                    \"base_model\": MODEL_NAME,\n                    \"upload_timestamp\": datetime.now().isoformat(),\n                    \"usage_instructions\": {\n                        \"load_command\": f'PeftModel.from_pretrained(base_model, \"{repo_name}\")',\n                        \"description\": \"Day 1에서 파인튜닝한 EXAONE RAG 모델\"\n                    }\n                }\n                \n                with open(\"../day2-RAG/finetuned_model_info.json\", \"w\", encoding=\"utf-8\") as f:\n                    json.dump(model_config, f, ensure_ascii=False, indent=2)\n                \n                print(f\"📝 모델 정보 저장: ../day2-RAG/finetuned_model_info.json\")\n                \n        except Exception as e:\n            print(f\"⚠️ 자동 업로드 실패: {e}\")\n            print(f\"💡 수동으로 업로드하려면 25번 셀을 사용하세요.\")\n        \n        # 테스트 프롬프트 준비\n        test_prompts = [\n            \"다음 컨텍스트들을 참고하여 질문에 답변해주세요.\\n\\n=== 컨텍스트 ===\\n컨텍스트 1: 한국의 수도는 서울입니다.\\n컨텍스트 2: 서울은 한강을 중심으로 발달했습니다.\\n\\n=== 질문 ===\\n한국의 수도는 어디인가요?\",\n            \"다음 컨텍스트들을 참고하여 질문에 답변해주세요.\\n\\n=== 컨텍스트 ===\\n컨텍스트 1: 김치는 한국의 전통 발효식품입니다.\\n컨텍스트 2: 파스타는 이탈리아의 대표 음식입니다.\\n\\n=== 질문 ===\\n김치는 어떤 음식인가요?\",\n            \"다음 컨텍스트들을 참고하여 질문에 답변해주세요.\\n\\n=== 컨텍스트 ===\\n컨텍스트 1: 태양은 별입니다.\\n컨텍스트 2: 지구는 행성입니다.\\n\\n=== 질문 ===\\n바다의 색깔은 무엇인가요?\"\n        ]\n        \n        # 모델 테스트\n        test_fine_tuned_model(current_trainer.model, tokenizer, test_prompts)\n        \n        print(f\"\\n🎉 Day 1 실습 3 완료!\")\n        print(f\"✅ 저장된 모델: {saved_model_path}\")\n        print(f\"✅ Hugging Face 주소: https://huggingface.co/ryanu/my-exaone-raft-model\")\n        print(f\"🔄 다음 단계: 04_evaluation_and_comparison.ipynb에서 성능 평가를 진행하세요!\")\n    else:\n        print(f\"\\n⚠️ 일부 파일이 저장되지 않았습니다. 다시 저장을 시도하세요.\")\n        \nelse:\n    print(\"\\n⚠️ 학습이 완료되지 않았습니다.\")\n    print(\"💡 학습을 먼저 완료한 후 이 셀을 다시 실행하세요.\")\n    print(\"\\n📝 학습 상태 확인:\")\n    if 'trainer' not in locals() and 'trainer' not in globals():\n        print(\"  - Trainer 객체가 생성되지 않았습니다.\")\n        print(\"  - 19번 셀(학습 실행)을 먼저 실행하세요.\")\n    else:\n        print(\"  - Trainer는 존재하지만 학습이 실행되지 않았을 수 있습니다.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 13. 허깅페이스 업로드 가이드\n\n### 🌐 파인튜닝된 모델 업로드하기\n\n파인튜닝 완료 후 모델을 허깅페이스에 업로드하여 저장하고 공유할 수 있습니다.\n\n### 📝 준비사항\n1. 허깅페이스 계정: https://huggingface.co/join\n2. 액세스 토큰 (Write 권한): https://huggingface.co/settings/tokens\n\n### 🔐 로그인 방법\n```python\nfrom huggingface_hub import login\nlogin()  # 토큰 입력 프롬프트\n```\n\n### 🚀 업로드 실행\n아래 셀에서 실제 업로드를 수행할 수 있습니다."
  },
  {
   "cell_type": "code",
   "source": "# 허깅페이스 업로드 실행\n# 먼저 로그인이 필요합니다\n\n# 1. 로그인 (아래 주석을 해제하고 실행)\n# from huggingface_hub import login\n# login()\n\n# 2. 환경변수로 토큰 설정 (선택사항)\n# import os\n# os.environ['HF_TOKEN'] = 'your_token_here'\n\n# 3. 모델 업로드 실행 (로그인 후 주석 해제)\nif 'saved_model_path' in locals():\n    print(f\"📦 업로드 준비된 모델: {saved_model_path}\")\n    \n    # 업로드 실행 (주석 해제 후 사용)\n    \"\"\"\n    repo_name = upload_to_huggingface(\n        model_path=saved_model_path,\n        repo_name=\"my-exaone-raft-model\",  # 원하는 이름으로 변경\n        private=True  # True: 비공개, False: 공개\n    )\n    print(f\"✅ 업로드 완료: https://huggingface.co/{repo_name}\")\n    \"\"\"\n    \n    print(\"\\n💡 업로드하려면:\")\n    print(\"1. 위의 login() 주석 해제하고 실행\")\n    print(\"2. 토큰 입력\")\n    print(\"3. upload_to_huggingface() 주석 해제하고 실행\")\n    \nelse:\n    print(\"❌ 저장된 모델이 없습니다. 먼저 파인튜닝을 완료하세요.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 요약 출력\n",
    "print(\"🎯 Day 1 실습 3: QLoRA 파인튜닝 완료!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'training_result' in locals():\n",
    "    # 성공적으로 완료된 경우\n",
    "    summary = {\n",
    "        \"✅ 모델\": MODEL_NAME,\n",
    "        \"📊 학습 샘플\": len(train_dataset) if train_dataset else 0,\n",
    "        \"🎯 학습 파라미터\": f\"{trainable_params:,} ({100*trainable_params/total_params:.3f}%)\",\n",
    "        \"⏱️ 학습 스텝\": training_result.global_step if 'training_result' in locals() else \"N/A\",\n",
    "        \"📉 최종 Loss\": f\"{training_result.training_loss:.4f}\" if 'training_result' in locals() else \"N/A\",\n",
    "        \"💾 저장 위치\": saved_model_path if 'saved_model_path' in locals() else \"N/A\"\n",
    "    }\n",
    "    \n",
    "    print(\"📋 학습 결과 요약:\")\n",
    "    for key, value in summary.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\n🎉 파인튜닝이 성공적으로 완료되었습니다!\")\n",
    "    print(f\"\\n📁 생성된 파일들:\")\n",
    "    if 'saved_model_path' in locals():\n",
    "        print(f\"  - {saved_model_path}/adapter_model.safetensors (LoRA 어댑터)\")\n",
    "        print(f\"  - {saved_model_path}/adapter_config.json (LoRA 설정)\")\n",
    "        print(f\"  - {saved_model_path}/tokenizer.json (토크나이저)\")\n",
    "        print(f\"  - {saved_model_path}/model_info.json (모델 정보)\")\n",
    "    \n",
    "    if 'trainer' in locals():\n",
    "        print(f\"  - {trainer.args.output_dir}/training_analysis.png (학습 분석 그래프)\")\n",
    "        print(f\"  - {trainer.args.output_dir}/training_summary.json (학습 요약)\")\n",
    "        print(f\"  - {trainer.args.output_dir}/analysis_report.json (분석 리포트)\")\n",
    "    \n",
    "    print(f\"\\n🔄 다음 단계:\")\n",
    "    print(f\"  1. 04_evaluation_and_comparison.ipynb에서 성능 평가\")\n",
    "    print(f\"  2. 파인튜닝 전후 모델 비교\")\n",
    "    print(f\"  3. ROUGE, BLEU, 코사인 유사도 측정\")\n",
    "    print(f\"  4. 실제 RAG 시나리오에서 테스트\")\n",
    "    \n",
    "    # 모델 사용 가이드\n",
    "    print(f\"\\n📖 모델 로드 방법:\")\n",
    "    print(f\"```python\")\n",
    "    print(f\"from peft import PeftModel\")\n",
    "    print(f\"from transformers import AutoModelForCausalLM, AutoTokenizer\")\n",
    "    print(f\"\")\n",
    "    print(f\"# 베이스 모델 로드\")\n",
    "    print(f\"base_model = AutoModelForCausalLM.from_pretrained('{MODEL_NAME}')\")\n",
    "    print(f\"tokenizer = AutoTokenizer.from_pretrained('{MODEL_NAME}')\")\n",
    "    print(f\"\")\n",
    "    print(f\"# LoRA 어댑터 적용\")\n",
    "    print(f\"model = PeftModel.from_pretrained(base_model, '{saved_model_path if 'saved_model_path' in locals() else './your_model_path'}')\")\n",
    "    print(f\"```\")\n",
    "\n",
    "else:\n",
    "    # 학습이 완료되지 않은 경우\n",
    "    print(\"⚠️ 파인튜닝이 완전히 완료되지 않았습니다.\")\n",
    "    print(f\"\\n💡 확인사항:\")\n",
    "    print(f\"  1. 데이터가 올바르게 로드되었는지 확인\")\n",
    "    print(f\"  2. GPU 메모리가 충분한지 확인\")\n",
    "    print(f\"  3. 학습 설정이 적절한지 확인\")\n",
    "    print(f\"\\n🔄 다시 시도하려면:\")\n",
    "    print(f\"  1. 런타임 재시작\")\n",
    "    print(f\"  2. 배치 크기 줄이기 (per_device_train_batch_size=1)\")\n",
    "    print(f\"  3. 샘플 수 줄이기 (MAX_TRAIN_SAMPLES=200)\")\n",
    "\n",
    "print(f\"\\n🚀 QLoRA를 활용한 효율적 파인튜닝이 완료되었습니다!\")\n",
    "print(f\"🎓 한국어 RAG에 특화된 EXAONE 모델을 얻었습니다!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}