{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 ì‹¤ìŠµ 3: EXAONE QLoRA íŒŒì¸íŠœë‹\n",
    "\n",
    "## ğŸ¯ í•™ìŠµ ëª©í‘œ\n",
    "\n",
    "1. **RAFT ì „ì²˜ë¦¬** ì´í•´í•˜ê¸° (ì´ë¡ )\n",
    "2. **í•œêµ­ì–´ ìš”ì•½** ë°ì´í„°ë¡œ ì‹¤ì „ íŒŒì¸íŠœë‹\n",
    "3. QLoRA (4bit + LoRA) ì ìš©\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š êµ¬ì„±\n",
    "\n",
    "- **Part 1**: RAFT ì „ì²˜ë¦¬ (ê°„ë‹¨ ì„¤ëª…)\n",
    "- **Part 2**: í•œêµ­ì–´ ìš”ì•½ íŒŒì¸íŠœë‹ (ë©”ì¸ ì‹¤ìŠµ!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: RAFT ì „ì²˜ë¦¬ ê°œë… (ê°„ë‹¨ ì„¤ëª…)\n",
    "\n",
    "### RAFT (Retrieval Augmented Fine-Tuning)ë€?\n",
    "\n",
    "- **ëª©ì **: RAG ì‹œìŠ¤í…œì—ì„œ ë” ë‚˜ì€ ë‹µë³€ ìƒì„±\n",
    "- **ë°©ë²•**: ì§ˆë¬¸ + ì •ë‹µ ë¬¸ì„œ + ì˜¤ë‹µ ë¬¸ì„œ(distractor)ë¥¼ í•¨ê»˜ í•™ìŠµ\n",
    "\n",
    "```\n",
    "ì…ë ¥: ì§ˆë¬¸: What is...ë¬¸ì„œ: The answer is...ì˜¤ë‹µ ë¬¸ì„œ: Wrong info...ë‹µë³€:\n",
    "ì¶œë ¥: The answer is...\n",
    "```\n",
    "\n",
    "### ì™œ ë³µì¡í• ê¹Œ?\n",
    "\n",
    "- ê¸´ í”„ë¡¬í”„íŠ¸ (1000+ tokens)\n",
    "- GPU ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼\n",
    "- í‰ê°€ ë³µì¡í•¨\n",
    "\n",
    "â†’ **ì‹¤ìŠµì—ì„œëŠ” ë” ê°„ë‹¨í•œ ìš”ì•½ íƒœìŠ¤í¬ ì‚¬ìš©!**\n",
    "\n",
    "---\n",
    "\n",
    "RAFT ì „ì²˜ë¦¬ëŠ” 01,02ë²ˆ ë…¸íŠ¸ë¶ì—ì„œ ì´ë¯¸ ë°°ì› ìŠµë‹ˆë‹¤.\n",
    "ì—¬ê¸°ì„œëŠ” **ì‹¤ì „ íŒŒì¸íŠœë‹**ì— ì§‘ì¤‘í•˜ê² ìŠµë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: í•œêµ­ì–´ ìš”ì•½ íŒŒì¸íŠœë‹ (ë©”ì¸ ì‹¤ìŠµ)\n",
    "\n",
    "### ğŸ’¡ ì™œ ìš”ì•½?\n",
    "\n",
    "1. **ê°„ë‹¨**: ê¸°ì‚¬ ë³¸ë¬¸ â†’ ì œëª© (ì§§ìŒ!)\n",
    "2. **ë¹ ë¦„**: 10ë¶„ ë‚´ í•™ìŠµ ê°€ëŠ¥\n",
    "3. **ëª…í™•**: ê²°ê³¼ë¥¼ ë°”ë¡œ í™•ì¸ ê°€ëŠ¥\n",
    "4. **í•œêµ­ì–´**: ì‹¤ì œ í™•ì¸ ê°€ëŠ¥\n",
    "\n",
    "### ğŸ“¦ ë°ì´í„°ì…‹\n",
    "\n",
    "**daekeun-ml/naver-news-summarization-ko**\n",
    "- ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ â†’ ì œëª©\n",
    "- Train: 22K, Validation: 2.4K\n",
    "- ì˜ˆì‹œ:\n",
    "  ```\n",
    "  ê¸°ì‚¬: \"ì‚¼ì„±ì „ìê°€ ì˜¤ëŠ˜ ì‹ ì œí’ˆ ë°œí‘œíšŒë¥¼ ì—´ê³ ...\"\n",
    "  ì œëª©: \"ì‚¼ì„± ì‹ ì œí’ˆ ê³µê°œ\"\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "\n",
    "Colab/Kaggleì—ì„œ ì‹¤í–‰ ì‹œ ì•„ë˜ë¥¼ ì‹¤í–‰í•˜ê³  **ëŸ°íƒ€ì„ ì¬ì‹œì‘**í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes accelerate peft datasets transformers\n",
    "\n",
    "# ëŸ°íƒ€ì„ ì¬ì‹œì‘ í•„ìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ import"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# í•œêµ­ì–´ ë‰´ìŠ¤ ìš”ì•½ ë°ì´í„°ì…‹\n",
    "print(\"ğŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "train_dataset = load_dataset(\"daekeun-ml/naver-news-summarization-ko\", split=\"train\")\n",
    "val_dataset = load_dataset(\"daekeun-ml/naver-news-summarization-ko\", split=\"validation\")\n",
    "\n",
    "print(f\"âœ… í•™ìŠµ ë°ì´í„°: {len(train_dataset):,}ê°œ\")\n",
    "print(f\"âœ… ê²€ì¦ ë°ì´í„°: {len(val_dataset):,}ê°œ\")\n",
    "\n",
    "# ìƒ˜í”Œ í™•ì¸\n",
    "sample = train_dataset[0]\n",
    "print(\"\\nğŸ“ ìƒ˜í”Œ:\")\n",
    "print(f\"  ê¸°ì‚¬: {sample['document'][:100]}...\")\n",
    "print(f\"  ìš”ì•½: {sample['summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë°ì´í„° ì„¤ì •\n",
    "\n",
    "ë¹ ë¥¸ ì‹¤ìŠµì„ ìœ„í•´ ìƒ˜í”Œ ìˆ˜ë¥¼ ì œí•œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸: 100ê°œ (5ë¶„) | ë³´í†µ: 1000ê°œ (20ë¶„) | ì „ì²´: 22K (2ì‹œê°„+)\n",
    "MAX_TRAIN_SAMPLES = 1000\n",
    "MAX_EVAL_SAMPLES = 100\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# í† í° ê¸¸ì´ë¡œ í•„í„°ë§ (ì§§ì€ ìƒ˜í”Œë§Œ ì„ íƒ)\n",
    "def is_valid_sample(example):\n",
    "    \"\"\"ê¸°ì‚¬ + ìš”ì•½ì´ MAX_LENGTH ì•ˆì— ë“¤ì–´ê°€ëŠ”ì§€ ì²´í¬\"\"\"\n",
    "    article = str(example.get(\"document\", \"\"))\n",
    "    summary = str(example.get(\"summary\", \"\"))\n",
    "    \n",
    "    # ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì • (í•œê¸€: 1.5ìë‹¹ 1í† í°, ì˜ì–´: 0.7ë‹¨ì–´ë‹¹ 1í† í°)\n",
    "    estimated_tokens = len(article) / 1.5 + len(summary) / 1.5\n",
    "    \n",
    "    # ì—¬ìœ  ìˆê²Œ MAX_LENGTHì˜ 80% ì´í•˜ë§Œ ì„ íƒ\n",
    "    return estimated_tokens < MAX_LENGTH * 0.8\n",
    "\n",
    "print(f\"ğŸ“Š ì›ë³¸ ë°ì´í„°: train={len(train_dataset):,}, val={len(val_dataset):,}\")\n",
    "\n",
    "# í•„í„°ë§\n",
    "train_dataset = train_dataset.filter(is_valid_sample)\n",
    "val_dataset = val_dataset.filter(is_valid_sample)\n",
    "\n",
    "print(f\"âœ‚ï¸ í•„í„°ë§ í›„: train={len(train_dataset):,}, val={len(val_dataset):,}\")\n",
    "\n",
    "# ìƒ˜í”Œ ì„ íƒ\n",
    "train_dataset = train_dataset.select(range(min(MAX_TRAIN_SAMPLES, len(train_dataset))))\n",
    "val_dataset = val_dataset.select(range(min(MAX_EVAL_SAMPLES, len(val_dataset))))\n",
    "\n",
    "print(f\"âœ‚ï¸ ìµœì¢… ìƒ˜í”Œ: train={len(train_dataset):,}, val={len(val_dataset):,}\")\n",
    "\n",
    "# ìƒ˜í”Œ í™•ì¸\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nğŸ“ ìƒ˜í”Œ:\")\n",
    "print(f\"  ê¸°ì‚¬: {sample['document'][:100]}...\")\n",
    "print(f\"  ìš”ì•½: {sample['summary'][:100]}...\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ (4bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ LoRA íŒŒë¼ë¯¸í„° ì„¤ëª…\n",
    "\n",
    "#### r (rank): LoRA ì°¨ì›\n",
    "- ë‚®ì„ìˆ˜ë¡: ë¹ ë¦„, ì„±ëŠ¥ ë‚®ìŒ\n",
    "- ë†’ì„ìˆ˜ë¡: ëŠë¦¼, ì„±ëŠ¥ ë†’ìŒ\n",
    "- ì¼ë°˜ì : r=8 ë˜ëŠ” r=16\n",
    "\n",
    "#### alpha: ê°€ì¤‘ì¹˜ ìŠ¤ì¼€ì¼\n",
    "- ë³´í†µ `alpha = r Ã— 2`\n",
    "- r=8 â†’ alpha=16\n",
    "\n",
    "#### dropout: ê³¼ì í•© ë°©ì§€\n",
    "- 0.05 ~ 0.1 ì‚¬ìš©\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¥ QLoRA = 4bit + LoRA\n",
    "\n",
    "- **4bit ì–‘ìí™”**: ë©”ëª¨ë¦¬ 1/4 ê°ì†Œ\n",
    "- **LoRA**: ì „ì²´ íŒŒë¼ë¯¸í„°ì˜ 0.5~2%ë§Œ í•™ìŠµ\n",
    "- **ê²°ê³¼**: GPU ì ˆì•½ + ë¹ ë¥¸ í•™ìŠµ\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "MODEL_NAME = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n",
    "\n",
    "print(\"ğŸ“¥ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\",\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 4bit ì–‘ìí™” ì„¤ì •\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16,\n",
    ")\n",
    "\n",
    "print(\"ğŸ“¥ ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16,\n",
    "    quantization_config=quant_config,\n",
    ")\n",
    "\n",
    "print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (4bit)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LoRA ì„¤ì • ë° ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# target_modules ìë™ ì¶”ì¶œ\n",
    "print(\"ğŸ” ì²« ë²ˆì§¸ ë ˆì´ì–´ íŒŒë¼ë¯¸í„°:\")\n",
    "target_modules = []\n",
    "\n",
    "for name, _ in model.named_parameters():\n",
    "    if 'layers.0' in name or 'layer.0' in name or 'h.0' in name:\n",
    "        if 'attn' in name.lower() and 'weight' in name:\n",
    "            print(f\"  {name}\")\n",
    "            parts = name.split('.')\n",
    "            for part in parts:\n",
    "                if part.endswith('_proj'):\n",
    "                    if part not in target_modules:\n",
    "                        target_modules.append(part)\n",
    "\n",
    "print(f\"\\nâœ… target_modules: {target_modules}\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=target_modules,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\nâœ… LoRA ì ìš©: {trainable_params:,} / {total_params:,} ({trainable_params/total_params:.2%})\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“š Label Masking ì›ë¦¬\n",
    "\n",
    "**í”„ë¡¬í”„íŠ¸ëŠ” í•™ìŠµ ì•ˆ í•˜ê³ , ë‹µë³€ë§Œ í•™ìŠµí•©ë‹ˆë‹¤.**\n",
    "\n",
    "```python\n",
    "ì…ë ¥: \"ì‚¼ì„±ì „ìê°€ ì‹ ì œí’ˆì„...ìš”ì•½: ì‚¼ì„± ì‹ ì œí’ˆ ê³µê°œ\"\n",
    "     â””â”€â”€â”€â”€â”€ í”„ë¡¬í”„íŠ¸ â”€â”€â”€â”€â”˜â””â”€â”€â”€ ë‹µë³€ â”€â”€â”€â”˜\n",
    "\n",
    "labels: [-100, -100, ..., 201, 202, 203]\n",
    "        â”” Loss ê³„ì‚° ì•ˆ í•¨ â”˜â”” Loss ê³„ì‚° â”˜\n",
    "```\n",
    "\n",
    "| í•­ëª© | ì„¤ëª… |\n",
    "|------|------|\n",
    "| **í”„ë¡¬í”„íŠ¸ (ì§ˆë¬¸)** | ëª¨ë¸ì´ \"ì½ëŠ” ë¶€ë¶„\". Attentionì—ëŠ” í¬í•¨ë˜ì§€ë§Œ, loss ê³„ì‚°ì€ ì•ˆ í•¨ |\n",
    "| **ë‹µë³€ (ì •ë‹µ)** | ëª¨ë¸ì´ \"ìƒì„±í•˜ëŠ” ë¶€ë¶„\". Loss ê³„ì‚°ì— í¬í•¨ë¨ |\n",
    "| **ì´ìœ ** | ëª¨ë¸ì´ ì…ë ¥ ë³µì‚¬ê°€ ì•„ë‹ˆë¼ \"ì¡°ê±´ë¶€ ìƒì„± ëŠ¥ë ¥\"ì„ ë°°ìš°ê²Œ í•˜ê¸° ìœ„í•¨ |\n",
    "\n",
    "**ì£¼ì˜**: ëª¨ë¸ì€ í”„ë¡¬í”„íŠ¸ë¥¼ **ë´…ë‹ˆë‹¤!** (Causal LM)\n",
    "- ë‹¨ì§€ Loss ê³„ì‚°ë§Œ ì•ˆ í•¨\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ë°ì´í„° í† í¬ë‚˜ì´ì§•\n",
    "\n",
    "ìš”ì•½ í˜•ì‹:\n",
    "```\n",
    "ì…ë ¥: ê¸°ì‚¬ ë³¸ë¬¸ìš”ì•½:\n",
    "ì¶œë ¥: ì œëª©\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "PROMPT_SUFFIX = \"ìš”ì•½:\"\n",
    "\n",
    "def prepare_summarization_example(example):\n",
    "    \"\"\"ìš”ì•½ ë°ì´í„°ë¥¼ í† í¬ë‚˜ì´ì§•\"\"\"\n",
    "    # ì…ë ¥: ê¸°ì‚¬ ë³¸ë¬¸\n",
    "    article = str(example.get(\"document\", \"\"))\n",
    "    summary_text = str(example.get(\"summary\", \"\"))\n",
    "    \n",
    "    # í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "    prompt_text = article + PROMPT_SUFFIX\n",
    "    \n",
    "    # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°\n",
    "    prompt_ids = tokenizer(prompt_text, add_special_tokens=False)[\"input_ids\"]\n",
    "    if len(prompt_ids) >= MAX_LENGTH - 64:\n",
    "        prompt_ids = prompt_ids[:MAX_LENGTH - 64]\n",
    "        prompt_text = tokenizer.decode(prompt_ids, skip_special_tokens=True) + PROMPT_SUFFIX\n",
    "    \n",
    "    # ì „ì²´ í…ìŠ¤íŠ¸ (í”„ë¡¬í”„íŠ¸ + ìš”ì•½)\n",
    "    full_text = prompt_text + \" \" + summary_text + tokenizer.eos_token\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì§•\n",
    "    model_inputs = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    # ë¼ë²¨ ìƒì„± (í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ë§ˆìŠ¤í‚¹)\n",
    "    prompt_len = len(tokenizer(prompt_text, add_special_tokens=False)[\"input_ids\"])\n",
    "    labels = model_inputs[\"input_ids\"].copy()\n",
    "    labels[:prompt_len] = [-100] * min(prompt_len, MAX_LENGTH)\n",
    "    \n",
    "    # ë¹ˆ ë¼ë²¨ì´ë©´ ì œì™¸\n",
    "    if all(label == -100 for label in labels):\n",
    "        return None\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "print(\"ğŸ”„ í† í¬ë‚˜ì´ì§• ì¤‘...\")\n",
    "train_dataset = train_dataset.map(prepare_summarization_example, remove_columns=train_dataset.column_names)\n",
    "train_dataset = train_dataset.filter(lambda x: x is not None)\n",
    "\n",
    "if val_dataset is not None:\n",
    "    val_dataset = val_dataset.map(prepare_summarization_example, remove_columns=val_dataset.column_names)\n",
    "    val_dataset = val_dataset.filter(lambda x: x is not None)\n",
    "\n",
    "print(f\"âœ… í† í¬ë‚˜ì´ì§• ì™„ë£Œ (í•™ìŠµ: {len(train_dataset)}, ê²€ì¦: {len(val_dataset) if val_dataset else 0})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. í•™ìŠµ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./exaone_summary_lora\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=[],\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(\"ğŸš€ íŒŒì¸íŠœë‹ ì‹œì‘\\n\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nâœ… íŒŒì¸íŠœë‹ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ëª¨ë¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"./exaone_summary_lora_final\"\n",
    "\n",
    "print(f\"ğŸ’¾ LoRA ì–´ëŒ‘í„° ì €ì¥: {OUTPUT_DIR}\")\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"\\nâœ… ì €ì¥ ì™„ë£Œ!\")\n",
    "print(f\"\\nğŸ“Š í•™ìŠµ ê²°ê³¼:\")\n",
    "for key, value in train_result.metrics.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. (ì„ íƒ) Hugging Face ì—…ë¡œë“œ\n",
    "\n",
    "í•™ìŠµí•œ LoRA ì–´ëŒ‘í„°ë¥¼ Hugging Face Hubì— ì—…ë¡œë“œí•˜ë©´ ì–´ë””ì„œë“  ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hugging Face ë¡œê·¸ì¸\n",
    "from huggingface_hub import login\n",
    "\n",
    "# HF í† í° ì…ë ¥ (https://huggingface.co/settings/tokens)\n",
    "login(token=\"YOUR_HF_TOKEN_HERE\")\n",
    "\n",
    "# ì—…ë¡œë“œ\n",
    "repo_name = \"your-username/exaone-summary-lora\"  # ì›í•˜ëŠ” ì´ë¦„ìœ¼ë¡œ ë³€ê²½\n",
    "\n",
    "model.push_to_hub(repo_name)\n",
    "tokenizer.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"âœ… ì—…ë¡œë“œ ì™„ë£Œ: https://huggingface.co/{repo_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}