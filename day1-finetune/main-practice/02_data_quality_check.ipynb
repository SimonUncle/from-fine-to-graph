{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š Day 1 ì‹¤ìŠµ 2: ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë° ì ê²€\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- ì „ì²˜ë¦¬ëœ ë°ì´í„°ì˜ í’ˆì§ˆ ê²€ì¦\n",
    "- í† í° ê¸¸ì´ ì‚¬ì „ ì ê²€\n",
    "- RAFT ë°ì´í„° êµ¬ì¡° ê²€ì¦\n",
    "- ì¤‘ë³µ ë° ì´ìƒì¹˜ íƒì§€\n",
    "- ë°ì´í„° ë¶„í¬ ë¶„ì„ ë° ì‹œê°í™”\n",
    "\n",
    "## ì‹œê°„: 14:00â€“14:40 (40ë¶„)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸ (01ë²ˆì—ì„œ ì´ë¯¸ ì„¤ì¹˜ë˜ì—ˆìœ¼ë¯€ë¡œ í™•ì¸ë§Œ)\nimport importlib\n\ndef check_package(package_name, import_name=None):\n    \"\"\"íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì—¬ë¶€ë§Œ í™•ì¸\"\"\"\n    if import_name is None:\n        import_name = package_name.replace('-', '_')\n    \n    try:\n        importlib.import_module(import_name)\n        print(f\"âœ… {package_name} ì‚¬ìš© ê°€ëŠ¥\")\n        return True\n    except ImportError:\n        print(f\"âŒ {package_name} ì„¤ì¹˜ í•„ìš” - 01ë²ˆ ë…¸íŠ¸ë¶ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”\")\n        return False\n\nprint(\"ğŸš€ Day 1 ì‹¤ìŠµ 2: ë°ì´í„° í’ˆì§ˆ ê²€ì¦\")\nprint(\"ğŸ” í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸ ì¤‘...\")\n\n# 02ë²ˆì—ì„œ ì‚¬ìš©í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ í™•ì¸\npackages = [\n    (\"transformers\", \"transformers\"),\n    (\"torch\", \"torch\"), \n    (\"datasets\", \"datasets\"),\n    (\"jsonlines\", \"jsonlines\"),\n    (\"pandas\", \"pandas\"),\n    (\"numpy\", \"numpy\"),\n    (\"matplotlib\", \"matplotlib\"),\n    (\"seaborn\", \"seaborn\"),\n    (\"tqdm\", \"tqdm\"),\n    (\"scikit-learn\", \"sklearn\")\n]\n\nall_available = True\nprint(\"ğŸ“‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ:\")\nfor package_name, import_name in packages:\n    if not check_package(package_name, import_name):\n        all_available = False\n\nif all_available:\n    print(\"\\nğŸ‰ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤€ë¹„ ì™„ë£Œ!\")\n    print(\"ğŸ’¡ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\nelse:\n    print(\"\\nâš ï¸ ì¼ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n    print(\"ğŸ’¡ ë¨¼ì € 01_data_preprocessing_and_validation.ipynbë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport jsonlines\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom tqdm import tqdm\nimport warnings\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, TrainerCallback\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport torch\n\nwarnings.filterwarnings('ignore')\n\n# í•œê¸€ í°íŠ¸ ì„¤ì • (matplotlib) - ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì°¨íŠ¸ì—ì„œ í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šë„ë¡ ì„¤ì •\nprint(\"ğŸ”§ í•œê¸€ í°íŠ¸ ì„¤ì • ì¤‘...\")\n!apt-get update -qq\n!apt-get install fonts-nanum -qq > /dev/null\n\nimport matplotlib.font_manager as fm\n\n# ë‚˜ëˆ”ë°”ë¥¸ê³ ë”• í°íŠ¸ ê²½ë¡œ ì„¤ì •\nfontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n# í°íŠ¸ ë§¤ë‹ˆì €ì— í°íŠ¸ ì¶”ê°€ - í’ˆì§ˆ ë¶„ì„ ê·¸ë˜í”„ì—ì„œ í•œê¸€ í‘œì‹œë¥¼ ìœ„í•´ í•„ìš”\nfm.fontManager.addfont(fontpath)\n\n# matplotlib ì„¤ì • ì—…ë°ì´íŠ¸ - ëª¨ë“  í’ˆì§ˆ ë¶„ì„ ì°¨íŠ¸ì—ì„œ í•œê¸€ì´ ì •ìƒì ìœ¼ë¡œ í‘œì‹œë¨\nplt.rcParams.update({\n    'font.family': 'NanumBarunGothic',  # ê¸°ë³¸ í°íŠ¸ë¥¼ ë‚˜ëˆ”ë°”ë¥¸ê³ ë”•ìœ¼ë¡œ ì„¤ì •\n    'axes.unicode_minus': False         # ìŒìˆ˜ ê¸°í˜¸ í‘œì‹œ ë¬¸ì œ í•´ê²° (í†µê³„ ì°¨íŠ¸ì—ì„œ ì¤‘ìš”)\n})\n\n# ì‹œê°í™” ìŠ¤íƒ€ì¼ ì„¤ì • - ë” ê¹”ë”í•˜ê³  ì „ë¬¸ì ì¸ ì°¨íŠ¸ë¥¼ ìœ„í•œ ì„¤ì •\nplt.style.use('default')  # ê¸°ë³¸ ìŠ¤íƒ€ì¼ ì‚¬ìš©\nsns.set_palette(\"husl\")   # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ì„¤ì • - êµ¬ë³„í•˜ê¸° ì‰¬ìš´ ìƒ‰ìƒ ì‚¬ìš©\n\nprint(\"âœ… í•œê¸€ í°íŠ¸ ì„¤ì • ì™„ë£Œ - ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ì°¨íŠ¸ì—ì„œ í•œê¸€ì´ ì •ìƒ í‘œì‹œë©ë‹ˆë‹¤\")\nprint(\"ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ import (í’ˆì§ˆ ë¶„ì„ì— í•„ìš”í•œ íŠ¹ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤)\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom collections import Counter, defaultdict\nimport re\nfrom sentence_transformers import SentenceTransformer\nfrom wordcloud import WordCloud\n\nprint(\"ğŸ“¦ ì¶”ê°€ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data():\n",
    "    \"\"\"\n",
    "    ì „ì²˜ë¦¬ëœ ë°ì´í„°ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        train_data, valid_data, metadata\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "    \n",
    "    \n",
    "    # Train ë°ì´í„° ë¡œë“œ\n",
    "    train_data = []\n",
    "    with jsonlines.open(\"processed_data/train_raft_ko.jsonl\", \"r\") as reader:\n",
    "        train_data = list(reader)\n",
    "    \n",
    "    # Valid ë°ì´í„° ë¡œë“œ\n",
    "    valid_data = []\n",
    "    with jsonlines.open(\"processed_data/valid_raft_ko.jsonl\", \"r\") as reader:\n",
    "        valid_data = list(reader)\n",
    "    \n",
    "    # ë©”íƒ€ë°ì´í„° ë¡œë“œ\n",
    "    with open(\"processed_data/metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ:\")\n",
    "    print(f\"  - Train: {len(train_data)}ê°œ ìƒ˜í”Œ\")\n",
    "    print(f\"  - Valid: {len(valid_data)}ê°œ ìƒ˜í”Œ\")\n",
    "    \n",
    "    return train_data, valid_data, metadata\n",
    "        \n",
    "    \n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "train_data, valid_data, metadata = load_processed_data()\n",
    "\n",
    "if train_data is not None:\n",
    "    print(f\"\\nğŸ“‹ ë©”íƒ€ë°ì´í„° ì •ë³´:\")\n",
    "    for key, value in metadata.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ê¸°ë³¸ ë°ì´í„° í’ˆì§ˆ ê²€ì¦\n",
    "\n",
    "### ğŸ“‹ ê²€ì¦ í•­ëª©\n",
    "1. ë°ì´í„° ë¬´ê²°ì„± í™•ì¸\n",
    "2. í•„ìˆ˜ í•„ë“œ ì¡´ì¬ ì—¬ë¶€\n",
    "3. ë°ì´í„° íƒ€ì… ê²€ì¦\n",
    "4. ë¹ˆ ê°’ ë° ê²°ì¸¡ì¹˜ íƒì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data_integrity(data: List[Dict], data_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        data: ê²€ì¦í•  ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n",
    "        data_name: ë°ì´í„°ì…‹ ì´ë¦„\n",
    "        \n",
    "    Returns:\n",
    "        ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” {data_name} ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ì¤‘...\")\n",
    "    \n",
    "    integrity_report = {\n",
    "        \"total_samples\": len(data),\n",
    "        \"required_fields\": [\"messages\", \"type\", \"original_question\", \"original_answer\"],\n",
    "        \"missing_fields\": defaultdict(int),\n",
    "        \"empty_values\": defaultdict(int),\n",
    "        \"invalid_types\": defaultdict(int),\n",
    "        \"message_structure_errors\": 0,\n",
    "        \"type_distribution\": defaultdict(int)\n",
    "    }\n",
    "    \n",
    "    for i, item in enumerate(data):\n",
    "        # 1. í•„ìˆ˜ í•„ë“œ ì¡´ì¬ í™•ì¸\n",
    "        for field in integrity_report[\"required_fields\"]:\n",
    "            if field not in item:\n",
    "                integrity_report[\"missing_fields\"][field] += 1\n",
    "        \n",
    "        # 2. ë¹ˆ ê°’ í™•ì¸\n",
    "        for field in [\"original_question\", \"original_answer\"]:\n",
    "            if field in item and (not item[field] or item[field].strip() == \"\"):\n",
    "                integrity_report[\"empty_values\"][field] += 1\n",
    "        \n",
    "        # 3. messages êµ¬ì¡° ê²€ì¦\n",
    "        if \"messages\" in item:\n",
    "            if not isinstance(item[\"messages\"], list) or len(item[\"messages\"]) != 3:\n",
    "                integrity_report[\"message_structure_errors\"] += 1\n",
    "            else:\n",
    "                # ë©”ì‹œì§€ ì—­í•  í™•ì¸\n",
    "                expected_roles = [\"system\", \"user\", \"assistant\"]\n",
    "                actual_roles = [msg.get(\"role\", \"\") for msg in item[\"messages\"]]\n",
    "                if actual_roles != expected_roles:\n",
    "                    integrity_report[\"message_structure_errors\"] += 1\n",
    "        \n",
    "        # 4. type ë¶„í¬ í™•ì¸\n",
    "        if \"type\" in item:\n",
    "            integrity_report[\"type_distribution\"][item[\"type\"]] += 1\n",
    "    \n",
    "    return integrity_report\n",
    "\n",
    "def print_integrity_report(report: Dict[str, Any], data_name: str):\n",
    "    \"\"\"\n",
    "    ë¬´ê²°ì„± ê²€ì¦ ê²°ê³¼ ì¶œë ¥ í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“Š {data_name} ë¬´ê²°ì„± ê²€ì¦ ê²°ê³¼:\")\n",
    "    print(f\"  ì´ ìƒ˜í”Œ ìˆ˜: {report['total_samples']}ê°œ\")\n",
    "    \n",
    "    # ê²°ì¸¡ í•„ë“œ\n",
    "    if report['missing_fields']:\n",
    "        print(f\"  âŒ ê²°ì¸¡ í•„ë“œ:\")\n",
    "        for field, count in report['missing_fields'].items():\n",
    "            print(f\"    {field}: {count}ê°œ ({count/report['total_samples']:.1%})\")\n",
    "    else:\n",
    "        print(f\"  âœ… í•„ìˆ˜ í•„ë“œ ëª¨ë‘ ì¡´ì¬\")\n",
    "    \n",
    "    # ë¹ˆ ê°’\n",
    "    if report['empty_values']:\n",
    "        print(f\"  âš ï¸ ë¹ˆ ê°’:\")\n",
    "        for field, count in report['empty_values'].items():\n",
    "            print(f\"    {field}: {count}ê°œ ({count/report['total_samples']:.1%})\")\n",
    "    else:\n",
    "        print(f\"  âœ… ë¹ˆ ê°’ ì—†ìŒ\")\n",
    "    \n",
    "    # ë©”ì‹œì§€ êµ¬ì¡° ì˜¤ë¥˜\n",
    "    if report['message_structure_errors'] > 0:\n",
    "        error_rate = report['message_structure_errors'] / report['total_samples']\n",
    "        print(f\"  âŒ ë©”ì‹œì§€ êµ¬ì¡° ì˜¤ë¥˜: {report['message_structure_errors']}ê°œ ({error_rate:.1%})\")\n",
    "    else:\n",
    "        print(f\"  âœ… ë©”ì‹œì§€ êµ¬ì¡° ì •ìƒ\")\n",
    "    \n",
    "    # íƒ€ì… ë¶„í¬\n",
    "    print(f\"  ğŸ“ˆ íƒ€ì… ë¶„í¬:\")\n",
    "    for type_name, count in report['type_distribution'].items():\n",
    "        print(f\"    {type_name}: {count}ê°œ ({count/report['total_samples']:.1%})\")\n",
    "\n",
    "# Train/Valid ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦\n",
    "if train_data is not None:\n",
    "    train_report = validate_data_integrity(train_data, \"Train\")\n",
    "    valid_report = validate_data_integrity(valid_data, \"Valid\")\n",
    "    \n",
    "    print_integrity_report(train_report, \"Train\")\n",
    "    print_integrity_report(valid_report, \"Valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. í† í° ê¸¸ì´ ë¶„ì„\n",
    "\n",
    "### ğŸ” ë¶„ì„ ë‚´ìš©\n",
    "1. ì „ì²´ ëŒ€í™” í† í° ê¸¸ì´ ë¶„í¬\n",
    "2. ë©”ì‹œì§€ë³„ í† í° ê¸¸ì´ ë¶„ì„\n",
    "3. 4096 í† í° ì œí•œ ì¤€ìˆ˜ í™•ì¸\n",
    "4. Outlier íƒì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAONE í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "print(\"ğŸ”„ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\")\n",
    "print(\"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "def analyze_token_distribution(data: List[Dict], data_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    í† í° ê¸¸ì´ ë¶„í¬ ë¶„ì„ í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        data: ë¶„ì„í•  ë°ì´í„°\n",
    "        data_name: ë°ì´í„°ì…‹ ì´ë¦„\n",
    "        \n",
    "    Returns:\n",
    "        í† í° ë¶„ì„ ê²°ê³¼\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“Š {data_name} í† í° ê¸¸ì´ ë¶„ì„ ì¤‘...\")\n",
    "    \n",
    "    token_analysis = {\n",
    "        \"total_tokens\": [],\n",
    "        \"system_tokens\": [],\n",
    "        \"user_tokens\": [],\n",
    "        \"assistant_tokens\": [],\n",
    "        \"overflow_samples\": [],\n",
    "        \"type_wise_tokens\": {\"positive\": [], \"negative\": []}\n",
    "    }\n",
    "    \n",
    "    for i, item in enumerate(tqdm(data, desc=f\"{data_name} í† í° ë¶„ì„\")):\n",
    "        if \"messages\" not in item or len(item[\"messages\"]) != 3:\n",
    "            continue\n",
    "        \n",
    "        messages = item[\"messages\"]\n",
    "        \n",
    "        # ì „ì²´ ëŒ€í™” í† í° ìˆ˜\n",
    "        full_conversation = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        total_tokens = len(tokenizer.encode(full_conversation))\n",
    "        token_analysis[\"total_tokens\"].append(total_tokens)\n",
    "        \n",
    "        # ë©”ì‹œì§€ë³„ í† í° ìˆ˜\n",
    "        for j, (msg, token_list) in enumerate(zip(messages, \n",
    "                                                 [\"system_tokens\", \"user_tokens\", \"assistant_tokens\"])):\n",
    "            msg_tokens = len(tokenizer.encode(msg[\"content\"]))\n",
    "            token_analysis[token_list].append(msg_tokens)\n",
    "        \n",
    "        # 4096 í† í° ì´ˆê³¼ ìƒ˜í”Œ ê¸°ë¡\n",
    "        if total_tokens > 4096:\n",
    "            token_analysis[\"overflow_samples\"].append({\n",
    "                \"index\": i,\n",
    "                \"total_tokens\": total_tokens,\n",
    "                \"type\": item.get(\"type\", \"unknown\")\n",
    "            })\n",
    "        \n",
    "        # Typeë³„ í† í° ë¶„í¬\n",
    "        sample_type = item.get(\"type\", \"unknown\")\n",
    "        if sample_type in token_analysis[\"type_wise_tokens\"]:\n",
    "            token_analysis[\"type_wise_tokens\"][sample_type].append(total_tokens)\n",
    "    \n",
    "    return token_analysis\n",
    "\n",
    "# í† í° ë¶„ì„ ì‹¤í–‰\n",
    "if train_data is not None:\n",
    "    train_tokens = analyze_token_distribution(train_data, \"Train\")\n",
    "    valid_tokens = analyze_token_distribution(valid_data, \"Valid\")\n",
    "    \n",
    "    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n",
    "    def print_token_summary(token_analysis: Dict, data_name: str):\n",
    "        total_tokens = token_analysis[\"total_tokens\"]\n",
    "        if not total_tokens:\n",
    "            return\n",
    "            \n",
    "        print(f\"\\nğŸ“Š {data_name} í† í° ë¶„ì„ ê²°ê³¼:\")\n",
    "        print(f\"  í‰ê·  í† í° ìˆ˜: {np.mean(total_tokens):.1f}\")\n",
    "        print(f\"  ì¤‘ê°„ê°’: {np.median(total_tokens):.1f}\")\n",
    "        print(f\"  í‘œì¤€í¸ì°¨: {np.std(total_tokens):.1f}\")\n",
    "        print(f\"  ìµœì†Œ/ìµœëŒ€: {min(total_tokens)} / {max(total_tokens)}\")\n",
    "        print(f\"  4096 ì´ˆê³¼: {len(token_analysis['overflow_samples'])}ê°œ ({len(token_analysis['overflow_samples'])/len(total_tokens):.1%})\")\n",
    "        \n",
    "        # Typeë³„ í‰ê· \n",
    "        for type_name, tokens in token_analysis[\"type_wise_tokens\"].items():\n",
    "            if tokens:\n",
    "                print(f\"  {type_name} í‰ê· : {np.mean(tokens):.1f} í† í°\")\n",
    "    \n",
    "    print_token_summary(train_tokens, \"Train\")\n",
    "    print_token_summary(valid_tokens, \"Valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë°ì´í„° ë¶„í¬ ì‹œê°í™”\n",
    "\n",
    "### ğŸ“ˆ ì‹œê°í™” ì°¨íŠ¸\n",
    "1. í† í° ê¸¸ì´ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨\n",
    "2. Typeë³„ í† í° ê¸¸ì´ ë¹„êµ\n",
    "3. ë©”ì‹œì§€ ì—­í• ë³„ í† í° ë¶„í¬\n",
    "4. Train/Valid ë¶„í¬ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_comprehensive_visualization(train_tokens: Dict, valid_tokens: Dict):\n    \"\"\"\n    ì¢…í•©ì ì¸ ë°ì´í„° í’ˆì§ˆ ì‹œê°í™” í•¨ìˆ˜\n    ì´ í•¨ìˆ˜ëŠ” 6ê°œì˜ ë‹¤ë¥¸ ê´€ì ì—ì„œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.\n    ê° ì°¨íŠ¸ëŠ” ë°ì´í„°ì˜ íŠ¹ì • ì¸¡ë©´ì„ ë³´ì—¬ì£¼ë©°, ì „ì²´ì ì¸ í’ˆì§ˆì„ í‰ê°€í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.\n    \"\"\"\n    print(\"ğŸ“Š ì¢…í•© ë°ì´í„° í’ˆì§ˆ ëŒ€ì‹œë³´ë“œ ìƒì„± ì¤‘...\")\n    \n    # ëŒ€ì‹œë³´ë“œ ìŠ¤íƒ€ì¼ ì„œë¸Œí”Œë¡¯ ìƒì„± (3í–‰ 2ì—´ êµ¬ì„±)\n    # ğŸ“Š ì˜ë¯¸: 6ê°œì˜ ì°¨íŠ¸ë¡œ ë°ì´í„°ì˜ ë‹¤ì–‘í•œ ì¸¡ë©´ì„ í•œë²ˆì— ë¶„ì„\n    fig = make_subplots(\n        rows=3, cols=2,\n        subplot_titles=[\n            \"ğŸ“ í† í° ê¸¸ì´ ë¶„í¬ ë¹„êµ (Train vs Valid)\",\n            \"ğŸ¯ Typeë³„ í† í° ê¸¸ì´ íŠ¹ì„±\", \n            \"ğŸ’¬ ë©”ì‹œì§€ ì—­í• ë³„ í† í° ë¶„í¬\",\n            \"âš ï¸ 4096 í† í° ì´ˆê³¼ ìƒ˜í”Œ ë¶„ì„\",\n            \"ğŸ“¦ í† í° ê¸¸ì´ í†µê³„ (ë°•ìŠ¤í”Œë¡¯)\",\n            \"ğŸ“ˆ ëˆ„ì  ë¶„í¬ í•¨ìˆ˜ (CDF) ë¹„êµ\"\n        ],\n        specs=[\n            [{\"secondary_y\": False}, {\"secondary_y\": False}],\n            [{\"secondary_y\": False}, {\"secondary_y\": False}],\n            [{\"secondary_y\": False}, {\"secondary_y\": False}]\n        ]\n    )\n    \n    # 1. í† í° ê¸¸ì´ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ (Train vs Valid)\n    # ğŸ“Š ì˜ë¯¸: Trainê³¼ Valid ë°ì´í„°ì˜ í† í° ê¸¸ì´ ë¶„í¬ê°€ ìœ ì‚¬í•œì§€ í™•ì¸\n    # - ë‘ ë¶„í¬ê°€ ë¹„ìŠ·í•˜ë©´ ë°ì´í„° ë¶„í• ì´ ì˜ ë˜ì—ˆìŒì„ ì˜ë¯¸\n    # - í° ì°¨ì´ê°€ ìˆë‹¤ë©´ ë¶„í•  ì „ëµì„ ì¬ê²€í† í•´ì•¼ í•¨\n    fig.add_trace(\n        go.Histogram(x=train_tokens[\"total_tokens\"], name=\"Train ë°ì´í„°\", \n                    opacity=0.7, nbinsx=50, marker_color='lightblue'),\n        row=1, col=1\n    )\n    fig.add_trace(\n        go.Histogram(x=valid_tokens[\"total_tokens\"], name=\"Valid ë°ì´í„°\", \n                    opacity=0.7, nbinsx=50, marker_color='lightcoral'),\n        row=1, col=1\n    )\n    \n    # 2. Typeë³„ í† í° ê¸¸ì´ ë¶„í¬\n    # ğŸ“Š ì˜ë¯¸: RAFTì˜ Positive/Negative ìƒ˜í”Œì´ í† í° ê¸¸ì´ ì¸¡ë©´ì—ì„œ ê· í˜•ì¡í˜€ ìˆëŠ”ì§€ í™•ì¸\n    # - Positive ìƒ˜í”Œ: ì •ë‹µ contextê°€ í¬í•¨ë˜ì–´ ë³´í†µ ë” ê¸¸ ìˆ˜ ìˆìŒ\n    # - Negative ìƒ˜í”Œ: ì •ë‹µ ì—†ëŠ” distractorë§Œ ìˆì–´ ë¹„êµì  ì§§ì„ ìˆ˜ ìˆìŒ\n    colors = ['lightgreen', 'lightsalmon']\n    for i, (type_name, tokens) in enumerate(train_tokens[\"type_wise_tokens\"].items()):\n        if tokens:\n            fig.add_trace(\n                go.Histogram(x=tokens, name=f\"Train-{type_name.capitalize()}\", \n                           opacity=0.7, nbinsx=30, marker_color=colors[i % len(colors)]),\n                row=1, col=2\n            )\n    \n    # 3. ë©”ì‹œì§€ ì—­í• ë³„ í† í° ë¶„í¬\n    # ğŸ“Š ì˜ë¯¸: System, User, Assistant ë©”ì‹œì§€ì˜ ê¸¸ì´ ë¶„í¬ë¥¼ ë¹„êµ\n    # - System: ë³´í†µ ê³ ì •ëœ ê¸¸ì´ (ì—­í•  ì„¤ëª…)\n    # - User: Contextì™€ ì§ˆë¬¸ í¬í•¨ìœ¼ë¡œ ê°€ì¥ ê¸¸ ìˆ˜ ìˆìŒ  \n    # - Assistant: ë‹µë³€ ê¸¸ì´ë¡œ ì ì ˆí•œ ë²”ìœ„ì— ìˆì–´ì•¼ í•¨\n    roles = [\"system_tokens\", \"user_tokens\", \"assistant_tokens\"] \n    role_names = [\"System (ì—­í•  ì„¤ëª…)\", \"User (ì§ˆë¬¸+Context)\", \"Assistant (ë‹µë³€)\"]\n    role_colors = ['lightsteelblue', 'lightpink', 'lightgreen']\n    \n    for role, name, color in zip(roles, role_names, role_colors):\n        if train_tokens[role]:\n            fig.add_trace(\n                go.Box(y=train_tokens[role], name=name, \n                      marker_color=color, boxmean=True),\n                row=2, col=1\n            )\n    \n    # 4. 4096 í† í° ì´ˆê³¼ ë¶„ì„\n    # ğŸ“Š ì˜ë¯¸: ëª¨ë¸ì˜ ìµœëŒ€ ì…ë ¥ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ìƒ˜í”Œë“¤ì˜ Typeë³„ ë¶„í¬\n    # - ì´ˆê³¼ ìƒ˜í”Œì´ ë§ìœ¼ë©´ ë°ì´í„° ì „ì²˜ë¦¬ë‚˜ Context ê¸¸ì´ ì¡°ì • í•„ìš”\n    # - Typeë³„ë¡œ ì´ˆê³¼ ë¹„ìœ¨ì´ ë‹¤ë¥´ë©´ í•´ë‹¹ Typeì˜ êµ¬ì¡°ì  ë¬¸ì œ ê°€ëŠ¥ì„±\n    overflow_types = defaultdict(int)\n    for sample in train_tokens[\"overflow_samples\"]:\n        overflow_types[sample[\"type\"]] += 1\n    \n    if overflow_types:\n        fig.add_trace(\n            go.Bar(x=list(overflow_types.keys()), y=list(overflow_types.values()),\n                   name=\"í† í° ì´ˆê³¼ ìƒ˜í”Œ\", marker_color='red', opacity=0.7),\n            row=2, col=2\n        )\n        # ì´ˆê³¼ìœ¨ í‘œì‹œë¥¼ ìœ„í•œ í…ìŠ¤íŠ¸ ì¶”ê°€\n        total_samples = len(train_tokens[\"total_tokens\"])\n        for type_name, count in overflow_types.items():\n            fig.add_annotation(\n                x=type_name, y=count + 0.1,\n                text=f\"{count/total_samples:.1%}\",\n                showarrow=False, row=2, col=2\n            )\n    \n    # 5. í† í° ê¸¸ì´ ë°•ìŠ¤í”Œë¡¯ (Train vs Valid)\n    # ğŸ“Š ì˜ë¯¸: ì‚¬ë¶„ìœ„ìˆ˜ì™€ ì´ìƒê°’ì„ í†µí•œ ìƒì„¸ ë¶„í¬ ë¹„êµ\n    # - ì¤‘ì•™ê°’, ì‚¬ë¶„ìœ„ìˆ˜ë¡œ ë¶„í¬ì˜ ì¤‘ì‹¬ê³¼ í¼ì§ ì •ë„ íŒŒì•…\n    # - ì´ìƒê°’(outlier)ìœ¼ë¡œ ë¹„ì •ìƒì ìœ¼ë¡œ ê¸´ ìƒ˜í”Œ ì‹ë³„\n    fig.add_trace(\n        go.Box(y=train_tokens[\"total_tokens\"], name=\"Train ë¶„í¬\",\n               marker_color='lightblue', boxmean=True),\n        row=3, col=1\n    )\n    fig.add_trace(\n        go.Box(y=valid_tokens[\"total_tokens\"], name=\"Valid ë¶„í¬\", \n               marker_color='lightcoral', boxmean=True),\n        row=3, col=1\n    )\n    \n    # 6. ëˆ„ì  ë¶„í¬ í•¨ìˆ˜ (CDF)\n    # ğŸ“Š ì˜ë¯¸: íŠ¹ì • í† í° ê¸¸ì´ ì´í•˜ì˜ ìƒ˜í”Œ ë¹„ìœ¨ì„ ë³´ì—¬ì¤Œ\n    # - ì˜ˆ: 2000 í† í° ì´í•˜ ìƒ˜í”Œì´ ì „ì²´ì˜ ëª‡ %ì¸ì§€ í™•ì¸\n    # - ë°ì´í„°ì˜ ë¶„í¬ íŠ¹ì„±ê³¼ ê¸¸ì´ ì œí•œ ì„¤ì •ì— ë„ì›€\n    train_sorted = np.sort(train_tokens[\"total_tokens\"])\n    train_cdf = np.arange(1, len(train_sorted) + 1) / len(train_sorted)\n    \n    valid_sorted = np.sort(valid_tokens[\"total_tokens\"])  \n    valid_cdf = np.arange(1, len(valid_sorted) + 1) / len(valid_sorted)\n    \n    fig.add_trace(\n        go.Scatter(x=train_sorted, y=train_cdf, mode='lines', \n                   name=\"Train CDF\", line=dict(color='blue', width=2)),\n        row=3, col=2\n    )\n    fig.add_trace(\n        go.Scatter(x=valid_sorted, y=valid_cdf, mode='lines',\n                   name=\"Valid CDF\", line=dict(color='red', width=2)),\n        row=3, col=2\n    )\n    \n    # 4096 í† í° ì œí•œì„  ì¶”ê°€ (ì¤‘ìš”í•œ ê¸°ì¤€ì„ )\n    # ğŸ“Š ì˜ë¯¸: ëª¨ë¸ì˜ ìµœëŒ€ ì…ë ¥ ê¸¸ì´ í‘œì‹œë¡œ ë°ì´í„° ì í•©ì„± íŒë‹¨\n    fig.add_vline(x=4096, line_dash=\"dash\", line_color=\"orange\", \n                  annotation_text=\"ğŸš¨ ëª¨ë¸ ìµœëŒ€ ê¸¸ì´: 4096 í† í°\", \n                  annotation_position=\"top\", row=3, col=2)\n    \n    # í‰ê· ì„ ë„ ì¶”ê°€ (ì°¸ê³ ìš©)\n    train_mean = np.mean(train_tokens[\"total_tokens\"])\n    fig.add_vline(x=train_mean, line_dash=\"dot\", line_color=\"green\",\n                  annotation_text=f\"ğŸ“Š Train í‰ê· : {train_mean:.0f}í† í°\",\n                  annotation_position=\"bottom\", row=3, col=2)\n    \n    # ì „ì²´ ë ˆì´ì•„ì›ƒ ì—…ë°ì´íŠ¸\n    fig.update_layout(\n        height=1200,  # ì¶©ë¶„í•œ ë†’ì´ë¡œ ê° ì°¨íŠ¸ê°€ ì˜ ë³´ì´ë„ë¡\n        title_text=\"ğŸ“Š ë°ì´í„° í’ˆì§ˆ ì¢…í•© ë¶„ì„ ëŒ€ì‹œë³´ë“œ<br><sub>íŒŒì¸íŠœë‹ ì „ í•„ìˆ˜ ì ê²€ ì‚¬í•­</sub>\",\n        showlegend=True,\n        title_x=0.5,\n        title_font_size=18\n    )\n    \n    # ê° ì„œë¸Œí”Œë¡¯ë³„ ì¶• ì œëª© ì„¤ì •\n    fig.update_xaxes(title_text=\"í† í° ê¸¸ì´\", row=1, col=1)\n    fig.update_yaxes(title_text=\"ìƒ˜í”Œ ìˆ˜\", row=1, col=1)\n    \n    fig.update_xaxes(title_text=\"í† í° ê¸¸ì´\", row=1, col=2)\n    fig.update_yaxes(title_text=\"ìƒ˜í”Œ ìˆ˜\", row=1, col=2)\n    \n    fig.update_yaxes(title_text=\"í† í° ê¸¸ì´\", row=2, col=1)\n    \n    fig.update_xaxes(title_text=\"ìƒ˜í”Œ Type\", row=2, col=2)\n    fig.update_yaxes(title_text=\"ì´ˆê³¼ ìƒ˜í”Œ ìˆ˜\", row=2, col=2)\n    \n    fig.update_yaxes(title_text=\"í† í° ê¸¸ì´\", row=3, col=1)\n    \n    fig.update_xaxes(title_text=\"í† í° ê¸¸ì´\", row=3, col=2)\n    fig.update_yaxes(title_text=\"ëˆ„ì  í™•ë¥ \", row=3, col=2)\n    \n    return fig\n\n# ì‹œê°í™” ìƒì„± ë° í‘œì‹œ\nif train_data is not None and train_tokens[\"total_tokens\"]:\n    print(\"ğŸ¨ ë°ì´í„° í’ˆì§ˆ ëŒ€ì‹œë³´ë“œ ìƒì„± ì¤‘...\")\n    dashboard_fig = create_comprehensive_visualization(train_tokens, valid_tokens)\n    dashboard_fig.show()\n    \n    # HTMLë¡œë„ ì €ì¥í•˜ì—¬ ìƒí˜¸ì‘ìš© ê°€ëŠ¥í•˜ê²Œ í•¨\n    dashboard_fig.write_html(\"processed_data/interactive_dashboard.html\")\n    print(\"âœ… ìƒí˜¸ì‘ìš© ëŒ€ì‹œë³´ë“œ ì €ì¥: processed_data/interactive_dashboard.html\")\n    print(\"\\nğŸ” ëŒ€ì‹œë³´ë“œ í•´ì„ ê°€ì´ë“œ:\")\n    print(\"  ğŸ“ í† í° ê¸¸ì´ ë¶„í¬: Train/Valid ë¶„í¬ê°€ ìœ ì‚¬í•´ì•¼ ë¶„í• ì´ ì ì ˆí•¨\")\n    print(\"  ğŸ¯ Typeë³„ ë¶„í¬: Positive/Negative ìƒ˜í”Œì˜ í† í° ê¸¸ì´ ê· í˜• í™•ì¸\")\n    print(\"  ğŸ’¬ ë©”ì‹œì§€ ì—­í• ë³„: System < Assistant < User ìˆœìœ¼ë¡œ ê¸¸ì´ê°€ ì¼ë°˜ì \")\n    print(\"  âš ï¸ í† í° ì´ˆê³¼: 4096ì„ ì´ˆê³¼í•˜ëŠ” ìƒ˜í”Œì€ ì˜ë¦¬ê±°ë‚˜ ì œê±° í•„ìš”\")  \n    print(\"  ğŸ“¦ ë°•ìŠ¤í”Œë¡¯: ì´ìƒê°’(ì ë“¤)ì´ ë§ìœ¼ë©´ ë°ì´í„° ì •ì œ í•„ìš”\")\n    print(\"  ğŸ“ˆ CDF: 90% ìƒ˜í”Œì´ 4096 ì´í•˜ì— ìˆì–´ì•¼ ì´ìƒì \")\n    print(\"\\nğŸ’¡ ì´ ëŒ€ì‹œë³´ë“œë¥¼ í†µí•´ íŒŒì¸íŠœë‹ ì „ ë°ì´í„° ì í•©ì„±ì„ ì¢…í•© íŒë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ì¤‘ë³µ ë° ìœ ì‚¬ë„ ë¶„ì„\n",
    "\n",
    "### ğŸ” ë¶„ì„ ë‚´ìš©\n",
    "1. ì •í™•í•œ í…ìŠ¤íŠ¸ ì¤‘ë³µ íƒì§€\n",
    "2. ì˜ë¯¸ì  ìœ ì‚¬ë„ ë¶„ì„\n",
    "3. Question ë‹¤ì–‘ì„± ê²€ì¦\n",
    "4. Answer í’ˆì§ˆ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_duplicates_and_similarity(data: List[Dict], sample_size: int = 100) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    ì¤‘ë³µ ë° ìœ ì‚¬ë„ ë¶„ì„ í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        data: ë¶„ì„í•  ë°ì´í„°\n",
    "        sample_size: ìœ ì‚¬ë„ ë¶„ì„ì„ ìœ„í•œ ìƒ˜í”Œ í¬ê¸°\n",
    "        \n",
    "    Returns:\n",
    "        ì¤‘ë³µ/ìœ ì‚¬ë„ ë¶„ì„ ê²°ê³¼\n",
    "    \"\"\"\n",
    "    print(\"ğŸ” ì¤‘ë³µ ë° ìœ ì‚¬ë„ ë¶„ì„ ì¤‘...\")\n",
    "    \n",
    "    analysis_result = {\n",
    "        \"exact_duplicates\": {\n",
    "            \"questions\": 0,\n",
    "            \"answers\": 0,\n",
    "            \"full_conversations\": 0\n",
    "        },\n",
    "        \"question_diversity\": {},\n",
    "        \"answer_diversity\": {},\n",
    "        \"semantic_similarity\": {}\n",
    "    }\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    questions = [item.get(\"original_question\", \"\") for item in data]\n",
    "    answers = [item.get(\"original_answer\", \"\") for item in data]\n",
    "    \n",
    "    # 1. ì •í™•í•œ ì¤‘ë³µ íƒì§€\n",
    "    unique_questions = set(questions)\n",
    "    unique_answers = set(answers)\n",
    "    \n",
    "    analysis_result[\"exact_duplicates\"][\"questions\"] = len(questions) - len(unique_questions)\n",
    "    analysis_result[\"exact_duplicates\"][\"answers\"] = len(answers) - len(unique_answers)\n",
    "    \n",
    "    # 2. Question/Answer ë‹¤ì–‘ì„± ë¶„ì„\n",
    "    analysis_result[\"question_diversity\"] = {\n",
    "        \"total_questions\": len(questions),\n",
    "        \"unique_questions\": len(unique_questions),\n",
    "        \"diversity_ratio\": len(unique_questions) / len(questions) if questions else 0,\n",
    "        \"avg_length\": np.mean([len(q) for q in questions if q]),\n",
    "        \"length_std\": np.std([len(q) for q in questions if q])\n",
    "    }\n",
    "    \n",
    "    analysis_result[\"answer_diversity\"] = {\n",
    "        \"total_answers\": len(answers),\n",
    "        \"unique_answers\": len(unique_answers),\n",
    "        \"diversity_ratio\": len(unique_answers) / len(answers) if answers else 0,\n",
    "        \"avg_length\": np.mean([len(a) for a in answers if a]),\n",
    "        \"length_std\": np.std([len(a) for a in answers if a])\n",
    "    }\n",
    "    \n",
    "    # 3. ì˜ë¯¸ì  ìœ ì‚¬ë„ ë¶„ì„ (ìƒ˜í”Œë§)\n",
    "    if len(data) > sample_size:\n",
    "        sampled_indices = np.random.choice(len(data), sample_size, replace=False)\n",
    "        sampled_questions = [questions[i] for i in sampled_indices if questions[i]]\n",
    "        sampled_answers = [answers[i] for i in sampled_indices if answers[i]]\n",
    "    else:\n",
    "        sampled_questions = [q for q in questions if q]\n",
    "        sampled_answers = [a for a in answers if a]\n",
    "    \n",
    "    try:\n",
    "        print(\"ğŸ”„ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\n",
    "        model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        \n",
    "        if sampled_questions:\n",
    "            question_embeddings = model.encode(sampled_questions[:50])  # ê³„ì‚° ì‹œê°„ ë‹¨ì¶•\n",
    "            q_similarity_matrix = cosine_similarity(question_embeddings)\n",
    "            \n",
    "            # ëŒ€ê°ì„  ì œì™¸í•œ ìœ ì‚¬ë„ ì ìˆ˜\n",
    "            q_similarities = q_similarity_matrix[np.triu_indices_from(q_similarity_matrix, k=1)]\n",
    "            \n",
    "            analysis_result[\"semantic_similarity\"][\"questions\"] = {\n",
    "                \"mean_similarity\": np.mean(q_similarities),\n",
    "                \"max_similarity\": np.max(q_similarities),\n",
    "                \"high_similarity_pairs\": np.sum(q_similarities > 0.8)\n",
    "            }\n",
    "        \n",
    "        if sampled_answers:\n",
    "            answer_embeddings = model.encode(sampled_answers[:50])\n",
    "            a_similarity_matrix = cosine_similarity(answer_embeddings)\n",
    "            \n",
    "            a_similarities = a_similarity_matrix[np.triu_indices_from(a_similarity_matrix, k=1)]\n",
    "            \n",
    "            analysis_result[\"semantic_similarity\"][\"answers\"] = {\n",
    "                \"mean_similarity\": np.mean(a_similarities),\n",
    "                \"max_similarity\": np.max(a_similarities),\n",
    "                \"high_similarity_pairs\": np.sum(a_similarities > 0.8)\n",
    "            }\n",
    "            \n",
    "        print(\"âœ… ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}\")\n",
    "        analysis_result[\"semantic_similarity\"] = {\"error\": str(e)}\n",
    "    \n",
    "    return analysis_result\n",
    "\n",
    "# ì¤‘ë³µ/ìœ ì‚¬ë„ ë¶„ì„ ì‹¤í–‰\n",
    "if train_data is not None:\n",
    "    similarity_analysis = analyze_duplicates_and_similarity(train_data)\n",
    "    \n",
    "    # ê²°ê³¼ ì¶œë ¥\n",
    "    print(\"\\nğŸ“Š ì¤‘ë³µ ë° ìœ ì‚¬ë„ ë¶„ì„ ê²°ê³¼:\")\n",
    "    print(f\"\\nğŸ” ì •í™•í•œ ì¤‘ë³µ:\")\n",
    "    for key, value in similarity_analysis[\"exact_duplicates\"].items():\n",
    "        print(f\"  {key}: {value}ê°œ\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ Question ë‹¤ì–‘ì„±:\")\n",
    "    q_div = similarity_analysis[\"question_diversity\"]\n",
    "    print(f\"  ì „ì²´/ê³ ìœ : {q_div['total_questions']}/{q_div['unique_questions']}\")\n",
    "    print(f\"  ë‹¤ì–‘ì„± ë¹„ìœ¨: {q_div['diversity_ratio']:.1%}\")\n",
    "    print(f\"  í‰ê·  ê¸¸ì´: {q_div['avg_length']:.1f}ì\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¬ Answer ë‹¤ì–‘ì„±:\")\n",
    "    a_div = similarity_analysis[\"answer_diversity\"]\n",
    "    print(f\"  ì „ì²´/ê³ ìœ : {a_div['total_answers']}/{a_div['unique_answers']}\")\n",
    "    print(f\"  ë‹¤ì–‘ì„± ë¹„ìœ¨: {a_div['diversity_ratio']:.1%}\")\n",
    "    print(f\"  í‰ê·  ê¸¸ì´: {a_div['avg_length']:.1f}ì\")\n",
    "    \n",
    "    if \"error\" not in similarity_analysis[\"semantic_similarity\"]:\n",
    "        print(f\"\\nğŸ§  ì˜ë¯¸ì  ìœ ì‚¬ë„:\")\n",
    "        if \"questions\" in similarity_analysis[\"semantic_similarity\"]:\n",
    "            q_sim = similarity_analysis[\"semantic_similarity\"][\"questions\"]\n",
    "            print(f\"  Question í‰ê·  ìœ ì‚¬ë„: {q_sim['mean_similarity']:.3f}\")\n",
    "            print(f\"  Question ìµœëŒ€ ìœ ì‚¬ë„: {q_sim['max_similarity']:.3f}\")\n",
    "            print(f\"  ê³ ìœ ì‚¬ë„(>0.8) ìŒ: {q_sim['high_similarity_pairs']}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. RAFT êµ¬ì¡° ê²€ì¦\n",
    "\n",
    "### ğŸ¯ RAFT íŠ¹í™” ê²€ì¦\n",
    "1. Positive/Negative ìƒ˜í”Œ ê· í˜•\n",
    "2. Context ê°œìˆ˜ ë¶„í¬\n",
    "3. Context ê´€ë ¨ì„± ë¶„ì„\n",
    "4. Distractor í’ˆì§ˆ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def analyze_raft_structure(data: List[Dict]) -> Dict[str, Any]:\n    \"\"\"\n    RAFT ë°ì´í„° êµ¬ì¡° ë¶„ì„ í•¨ìˆ˜\n    \n    Args:\n        data: RAFT êµ¬ì¡° ë°ì´í„°\n        \n    Returns:\n        RAFT êµ¬ì¡° ë¶„ì„ ê²°ê³¼\n    \"\"\"\n    print(\"ğŸ¯ RAFT êµ¬ì¡° ë¶„ì„ ì¤‘...\")\n    \n    raft_analysis = {\n        \"type_distribution\": defaultdict(int),\n        \"context_analysis\": {\n            \"context_counts\": [],\n            \"context_lengths\": []\n        },\n        \"message_analysis\": {\n            \"user_message_lengths\": [],\n            \"system_message_lengths\": [],\n            \"assistant_message_lengths\": []\n        },\n        \"structure_issues\": []\n    }\n    \n    for i, item in enumerate(data):\n        # 1. Type ë¶„í¬\n        item_type = item.get(\"type\", \"unknown\")\n        raft_analysis[\"type_distribution\"][item_type] += 1\n        \n        # 2. Message êµ¬ì¡° ë¶„ì„\n        if \"messages\" in item and len(item[\"messages\"]) == 3:\n            messages = item[\"messages\"]\n            \n            # ê° ë©”ì‹œì§€ ê¸¸ì´ ìˆ˜ì§‘\n            raft_analysis[\"message_analysis\"][\"system_message_lengths\"].append(\n                len(messages[0].get(\"content\", \"\"))\n            )\n            raft_analysis[\"message_analysis\"][\"user_message_lengths\"].append(\n                len(messages[1].get(\"content\", \"\"))\n            )\n            raft_analysis[\"message_analysis\"][\"assistant_message_lengths\"].append(\n                len(messages[2].get(\"content\", \"\"))\n            )\n            \n            # 3. User ë©”ì‹œì§€ì—ì„œ context ì •ë³´ ì¶”ì¶œ\n            user_content = messages[1].get(\"content\", \"\")\n            \n            # Context ê°œìˆ˜ ì¶”ì • (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)\n            context_matches = re.findall(r'ì»¨í…ìŠ¤íŠ¸ \\d+:', user_content)\n            context_count = len(context_matches)\n            \n            if context_count > 0:\n                raft_analysis[\"context_analysis\"][\"context_counts\"].append(context_count)\n            \n            # Context ì„¹ì…˜ ê¸¸ì´\n            context_section_match = re.search(r'=== ì»¨í…ìŠ¤íŠ¸ ===\\n(.*?)\\n=== ì§ˆë¬¸ ===', \n                                             user_content, re.DOTALL)\n            if context_section_match:\n                context_length = len(context_section_match.group(1).strip())\n                raft_analysis[\"context_analysis\"][\"context_lengths\"].append(context_length)\n        else:\n            raft_analysis[\"structure_issues\"].append(f\"Sample {i}: Invalid message structure\")\n    \n    return raft_analysis\n\ndef visualize_raft_analysis(raft_analysis: Dict[str, Any]):\n    \"\"\"\n    RAFT ë¶„ì„ ê²°ê³¼ ì‹œê°í™”\n    \"\"\"\n    # í•œê¸€ í°íŠ¸ ì„¤ì • ì¬í™•ì¸ - RAFT ì°¨íŠ¸ì—ì„œ í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šë„ë¡ ë³´ì¥\n    plt.rcParams.update({\n        'font.family': 'NanumBarunGothic',\n        'axes.unicode_minus': False\n    })\n    \n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n    \n    # 1. Type ë¶„í¬ íŒŒì´ì°¨íŠ¸\n    type_counts = list(raft_analysis[\"type_distribution\"].values())\n    type_labels = list(raft_analysis[\"type_distribution\"].keys())\n    \n    ax1.pie(type_counts, labels=type_labels, autopct='%1.1f%%', startangle=90)\n    ax1.set_title('RAFT Type ë¶„í¬', fontsize=14, fontweight='bold')\n    \n    # 2. Context ê°œìˆ˜ ë¶„í¬\n    context_counts = raft_analysis[\"context_analysis\"][\"context_counts\"]\n    if context_counts:\n        ax2.hist(context_counts, bins=range(1, max(context_counts)+2), \n                alpha=0.7, edgecolor='black')\n        ax2.set_xlabel('Context ê°œìˆ˜')\n        ax2.set_ylabel('ìƒ˜í”Œ ìˆ˜')\n        ax2.set_title('Context ê°œìˆ˜ë³„ ë¶„í¬', fontsize=14, fontweight='bold')\n        ax2.grid(True, alpha=0.3)\n    \n    # 3. ë©”ì‹œì§€ ê¸¸ì´ ë¶„í¬\n    msg_analysis = raft_analysis[\"message_analysis\"]\n    roles = ['system_message_lengths', 'user_message_lengths', 'assistant_message_lengths']\n    role_names = ['System', 'User', 'Assistant']\n    \n    box_data = [msg_analysis[role] for role in roles if msg_analysis[role]]\n    box_labels = [name for role, name in zip(roles, role_names) if msg_analysis[role]]\n    \n    if box_data:\n        ax3.boxplot(box_data, labels=box_labels)\n        ax3.set_ylabel('ë©”ì‹œì§€ ê¸¸ì´ (ë¬¸ì)')\n        ax3.set_title('ë©”ì‹œì§€ ì—­í• ë³„ ê¸¸ì´ ë¶„í¬', fontsize=14, fontweight='bold')\n        ax3.grid(True, alpha=0.3)\n    \n    # 4. Context ê¸¸ì´ ë¶„í¬\n    context_lengths = raft_analysis[\"context_analysis\"][\"context_lengths\"]\n    if context_lengths:\n        ax4.hist(context_lengths, bins=30, alpha=0.7, color='lightblue', edgecolor='black')\n        ax4.set_xlabel('Context ì„¹ì…˜ ê¸¸ì´ (ë¬¸ì)')\n        ax4.set_ylabel('ë¹ˆë„')\n        ax4.set_title('Context ê¸¸ì´ ë¶„í¬', fontsize=14, fontweight='bold')\n        ax4.axvline(np.mean(context_lengths), color='red', linestyle='--', \n                   label=f'í‰ê· : {np.mean(context_lengths):.0f}')\n        ax4.legend()\n        ax4.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig('processed_data/raft_analysis.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    return fig\n\n# RAFT êµ¬ì¡° ë¶„ì„ ì‹¤í–‰\nif train_data is not None:\n    raft_analysis = analyze_raft_structure(train_data)\n    \n    # ë¶„ì„ ê²°ê³¼ ì¶œë ¥\n    print(\"\\nğŸ¯ RAFT êµ¬ì¡° ë¶„ì„ ê²°ê³¼:\")\n    print(f\"\\nğŸ“Š Type ë¶„í¬:\")\n    total_samples = sum(raft_analysis[\"type_distribution\"].values())\n    for type_name, count in raft_analysis[\"type_distribution\"].items():\n        percentage = (count / total_samples) * 100 if total_samples > 0 else 0\n        print(f\"  {type_name}: {count}ê°œ ({percentage:.1f}%)\")\n    \n    # Context ë¶„ì„\n    context_counts = raft_analysis[\"context_analysis\"][\"context_counts\"]\n    context_lengths = raft_analysis[\"context_analysis\"][\"context_lengths\"]\n    \n    if context_counts:\n        print(f\"\\nğŸ“‹ Context ë¶„ì„:\")\n        print(f\"  í‰ê·  Context ê°œìˆ˜: {np.mean(context_counts):.1f}ê°œ\")\n        print(f\"  Context ê°œìˆ˜ ë²”ìœ„: {min(context_counts)} ~ {max(context_counts)}ê°œ\")\n        \n        # Context ê°œìˆ˜ë³„ ë¹ˆë„\n        context_freq = Counter(context_counts)\n        print(f\"  Context ê°œìˆ˜ë³„ ë¶„í¬:\")\n        for count, freq in sorted(context_freq.items()):\n            print(f\"    {count}ê°œ: {freq}íšŒ ({freq/len(context_counts):.1%})\")\n    \n    if context_lengths:\n        print(f\"\\nğŸ“ Context ê¸¸ì´ í†µê³„:\")\n        print(f\"  í‰ê·  ê¸¸ì´: {np.mean(context_lengths):.0f}ì\")\n        print(f\"  ì¤‘ê°„ê°’: {np.median(context_lengths):.0f}ì\")\n        print(f\"  ê¸¸ì´ ë²”ìœ„: {min(context_lengths)} ~ {max(context_lengths)}ì\")\n    \n    # êµ¬ì¡° ë¬¸ì œ í™•ì¸\n    if raft_analysis[\"structure_issues\"]:\n        print(f\"\\nâš ï¸ êµ¬ì¡° ë¬¸ì œ ë°œê²¬:\")\n        for issue in raft_analysis[\"structure_issues\"][:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ\n            print(f\"  {issue}\")\n        if len(raft_analysis[\"structure_issues\"]) > 5:\n            print(f\"  ... ì´ {len(raft_analysis['structure_issues'])}ê°œ ë¬¸ì œ\")\n    else:\n        print(f\"\\nâœ… êµ¬ì¡° ë¬¸ì œ ì—†ìŒ\")\n    \n    # ì‹œê°í™”\n    raft_viz = visualize_raft_analysis(raft_analysis)\n    print(\"\\nâœ… RAFT ë¶„ì„ ì°¨íŠ¸ ì €ì¥: processed_data/raft_analysis.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ìµœì¢… í’ˆì§ˆ ì ìˆ˜ ë° ê¶Œì¥ì‚¬í•­\n",
    "\n",
    "### ğŸ“‹ ë°ì´í„° í’ˆì§ˆ ìŠ¤ì½”ì–´ì¹´ë“œ\n",
    "1. ë¬´ê²°ì„± ì ìˆ˜ (0-100)\n",
    "2. ë‹¤ì–‘ì„± ì ìˆ˜ (0-100) \n",
    "3. êµ¬ì¡° ì í•©ì„± ì ìˆ˜ (0-100)\n",
    "4. í† í° íš¨ìœ¨ì„± ì ìˆ˜ (0-100)\n",
    "5. ì¢…í•© í’ˆì§ˆ ì ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quality_score(train_report: Dict, valid_report: Dict, \n",
    "                          train_tokens: Dict, similarity_analysis: Dict,\n",
    "                          raft_analysis: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    ë°ì´í„° í’ˆì§ˆ ì¢…í•© ì ìˆ˜ ê³„ì‚°\n",
    "    \n",
    "    Returns:\n",
    "        í’ˆì§ˆ ì ìˆ˜ ë”•ì…”ë„ˆë¦¬\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š ë°ì´í„° í’ˆì§ˆ ì¢…í•© ì ìˆ˜ ê³„ì‚° ì¤‘...\")\n",
    "    \n",
    "    quality_scores = {}\n",
    "    \n",
    "    # 1. ë¬´ê²°ì„± ì ìˆ˜ (0-100)\n",
    "    integrity_issues = (\n",
    "        sum(train_report.get(\"missing_fields\", {}).values()) +\n",
    "        sum(train_report.get(\"empty_values\", {}).values()) +\n",
    "        train_report.get(\"message_structure_errors\", 0)\n",
    "    )\n",
    "    total_train_samples = train_report.get(\"total_samples\", 1)\n",
    "    integrity_score = max(0, 100 - (integrity_issues / total_train_samples) * 100)\n",
    "    quality_scores[\"integrity_score\"] = integrity_score\n",
    "    \n",
    "    # 2. ë‹¤ì–‘ì„± ì ìˆ˜ (0-100)\n",
    "    q_diversity = similarity_analysis.get(\"question_diversity\", {}).get(\"diversity_ratio\", 0)\n",
    "    a_diversity = similarity_analysis.get(\"answer_diversity\", {}).get(\"diversity_ratio\", 0)\n",
    "    diversity_score = ((q_diversity + a_diversity) / 2) * 100\n",
    "    quality_scores[\"diversity_score\"] = diversity_score\n",
    "    \n",
    "    # 3. êµ¬ì¡° ì í•©ì„± ì ìˆ˜ (0-100)\n",
    "    structure_issues = len(raft_analysis.get(\"structure_issues\", []))\n",
    "    structure_score = max(0, 100 - (structure_issues / total_train_samples) * 100)\n",
    "    quality_scores[\"structure_score\"] = structure_score\n",
    "    \n",
    "    # 4. í† í° íš¨ìœ¨ì„± ì ìˆ˜ (0-100)\n",
    "    overflow_rate = len(train_tokens.get(\"overflow_samples\", [])) / len(train_tokens.get(\"total_tokens\", [1]))\n",
    "    token_efficiency_score = max(0, 100 - (overflow_rate * 100))\n",
    "    quality_scores[\"token_efficiency_score\"] = token_efficiency_score\n",
    "    \n",
    "    # 5. RAFT ê· í˜• ì ìˆ˜ (0-100)\n",
    "    type_dist = raft_analysis.get(\"type_distribution\", {})\n",
    "    positive_ratio = type_dist.get(\"positive\", 0) / sum(type_dist.values()) if type_dist else 0\n",
    "    # ì´ìƒì ì¸ ë¹„ìœ¨(0.6)ì—ì„œ ì–¼ë§ˆë‚˜ ë²—ì–´ë‚¬ëŠ”ì§€ ê³„ì‚°\n",
    "    balance_deviation = abs(positive_ratio - 0.6)\n",
    "    raft_balance_score = max(0, 100 - (balance_deviation * 200))  # í¸ì°¨ë¥¼ ì ìˆ˜ë¡œ ë³€í™˜\n",
    "    quality_scores[\"raft_balance_score\"] = raft_balance_score\n",
    "    \n",
    "    # 6. ì¢…í•© í’ˆì§ˆ ì ìˆ˜ (ê°€ì¤‘í‰ê· )\n",
    "    weights = {\n",
    "        \"integrity_score\": 0.3,\n",
    "        \"diversity_score\": 0.25,\n",
    "        \"structure_score\": 0.2,\n",
    "        \"token_efficiency_score\": 0.15,\n",
    "        \"raft_balance_score\": 0.1\n",
    "    }\n",
    "    \n",
    "    overall_score = sum(quality_scores[key] * weight for key, weight in weights.items())\n",
    "    quality_scores[\"overall_score\"] = overall_score\n",
    "    \n",
    "    return quality_scores\n",
    "\n",
    "def generate_recommendations(quality_scores: Dict[str, float], \n",
    "                           train_tokens: Dict, \n",
    "                           similarity_analysis: Dict) -> List[str]:\n",
    "    \"\"\"\n",
    "    í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±\n",
    "    \"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    # ë¬´ê²°ì„± ê´€ë ¨\n",
    "    if quality_scores[\"integrity_score\"] < 90:\n",
    "        recommendations.append(\n",
    "            \"âš ï¸ ë°ì´í„° ë¬´ê²°ì„± ê°œì„  í•„ìš”: ê²°ì¸¡ê°’ì´ë‚˜ êµ¬ì¡°ì  ë¬¸ì œê°€ ìˆëŠ” ìƒ˜í”Œì„ ìˆ˜ì •í•˜ì„¸ìš”.\"\n",
    "        )\n",
    "    \n",
    "    # ë‹¤ì–‘ì„± ê´€ë ¨\n",
    "    if quality_scores[\"diversity_score\"] < 80:\n",
    "        recommendations.append(\n",
    "            \"ğŸ“ ë°ì´í„° ë‹¤ì–‘ì„± ê°œì„  í•„ìš”: ì¤‘ë³µëœ ì§ˆë¬¸ì´ë‚˜ ë‹µë³€ì„ ì œê±°í•˜ê³  ë” ë‹¤ì–‘í•œ ìƒ˜í”Œì„ ì¶”ê°€í•˜ì„¸ìš”.\"\n",
    "        )\n",
    "    \n",
    "    # í† í° íš¨ìœ¨ì„± ê´€ë ¨\n",
    "    if quality_scores[\"token_efficiency_score\"] < 90:\n",
    "        overflow_count = len(train_tokens.get(\"overflow_samples\", []))\n",
    "        recommendations.append(\n",
    "            f\"ğŸ“ í† í° ê¸¸ì´ ìµœì í™” í•„ìš”: {overflow_count}ê°œ ìƒ˜í”Œì´ 4096 í† í°ì„ ì´ˆê³¼í•©ë‹ˆë‹¤. \"\n",
    "            \"ê¸´ contextë¥¼ ì¤„ì´ê±°ë‚˜ ë¶„í• ì„ ê³ ë ¤í•˜ì„¸ìš”.\"\n",
    "        )\n",
    "    \n",
    "    # RAFT ê· í˜• ê´€ë ¨\n",
    "    if quality_scores[\"raft_balance_score\"] < 85:\n",
    "        recommendations.append(\n",
    "            \"âš–ï¸ RAFT ìƒ˜í”Œ ê· í˜• ì¡°ì • í•„ìš”: Positiveì™€ Negative ìƒ˜í”Œ ë¹„ìœ¨ì„ 6:4ë¡œ ë§ì¶”ì„¸ìš”.\"\n",
    "        )\n",
    "    \n",
    "    # ì˜ë¯¸ì  ìœ ì‚¬ë„ ê´€ë ¨\n",
    "    if \"semantic_similarity\" in similarity_analysis:\n",
    "        q_sim = similarity_analysis[\"semantic_similarity\"].get(\"questions\", {})\n",
    "        if q_sim.get(\"high_similarity_pairs\", 0) > 5:\n",
    "            recommendations.append(\n",
    "                \"ğŸ§  ì˜ë¯¸ì  ì¤‘ë³µ ì œê±° í•„ìš”: ìœ ì‚¬í•œ ì§ˆë¬¸ë“¤ì´ ë§ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. \"\n",
    "                \"ì˜ë¯¸ì ìœ¼ë¡œ ì¤‘ë³µë˜ëŠ” ìƒ˜í”Œì„ ì œê±°í•˜ì„¸ìš”.\"\n",
    "            )\n",
    "    \n",
    "    # ì „ì²´ ì ìˆ˜ê°€ ë‚®ì€ ê²½ìš°\n",
    "    if quality_scores[\"overall_score\"] < 75:\n",
    "        recommendations.append(\n",
    "            \"ğŸ”„ ì „ë©´ì ì¸ ë°ì´í„° ê°œì„  í•„ìš”: ì „ì²´ í’ˆì§ˆ ì ìˆ˜ê°€ ë‚®ìŠµë‹ˆë‹¤. \"\n",
    "            \"ë°ì´í„° ìˆ˜ì§‘ë¶€í„° ì „ì²˜ë¦¬ê¹Œì§€ ì „ ê³¼ì •ì„ ì¬ê²€í† í•˜ì„¸ìš”.\"\n",
    "        )\n",
    "    elif quality_scores[\"overall_score\"] >= 90:\n",
    "        recommendations.append(\n",
    "            \"âœ… ìš°ìˆ˜í•œ ë°ì´í„° í’ˆì§ˆ: í˜„ì¬ ë°ì´í„°ëŠ” íŒŒì¸íŠœë‹ì— ì í•©í•œ í’ˆì§ˆì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\"\n",
    "        )\n",
    "    \n",
    "    if not recommendations:\n",
    "        recommendations.append(\n",
    "            \"âœ… ì „ë°˜ì ìœ¼ë¡œ ì–‘í˜¸í•œ ë°ì´í„° í’ˆì§ˆì…ë‹ˆë‹¤. íŒŒì¸íŠœë‹ì„ ì§„í–‰í•´ë„ ì¢‹ìŠµë‹ˆë‹¤.\"\n",
    "        )\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "def create_quality_scorecard_viz(quality_scores: Dict[str, float]):\n",
    "    \"\"\"\n",
    "    í’ˆì§ˆ ìŠ¤ì½”ì–´ì¹´ë“œ ì‹œê°í™”\n",
    "    \"\"\"\n",
    "    # ì ìˆ˜ ë°ì´í„° ì¤€ë¹„\n",
    "    score_names = [\n",
    "        \"ë¬´ê²°ì„±\", \"ë‹¤ì–‘ì„±\", \"êµ¬ì¡° ì í•©ì„±\", \n",
    "        \"í† í° íš¨ìœ¨ì„±\", \"RAFT ê· í˜•\", \"ì¢…í•© ì ìˆ˜\"\n",
    "    ]\n",
    "    score_keys = [\n",
    "        \"integrity_score\", \"diversity_score\", \"structure_score\",\n",
    "        \"token_efficiency_score\", \"raft_balance_score\", \"overall_score\"\n",
    "    ]\n",
    "    scores = [quality_scores[key] for key in score_keys]\n",
    "    \n",
    "    # ìƒ‰ìƒ ë§¤í•‘ (ì ìˆ˜ì— ë”°ë¼)\n",
    "    def get_color(score):\n",
    "        if score >= 90:\n",
    "            return 'green'\n",
    "        elif score >= 75:\n",
    "            return 'orange'\n",
    "        else:\n",
    "            return 'red'\n",
    "    \n",
    "    colors = [get_color(score) for score in scores]\n",
    "    \n",
    "    # ë°” ì°¨íŠ¸ ìƒì„±\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    bars = ax.barh(score_names, scores, color=colors, alpha=0.7)\n",
    "    \n",
    "    # ì ìˆ˜ í‘œì‹œ\n",
    "    for i, (bar, score) in enumerate(zip(bars, scores)):\n",
    "        ax.text(score + 1, i, f'{score:.1f}', va='center', fontweight='bold')\n",
    "    \n",
    "    # ì°¨íŠ¸ ì„¤ì •\n",
    "    ax.set_xlim(0, 105)\n",
    "    ax.set_xlabel('ì ìˆ˜', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('ğŸ“Š ë°ì´í„° í’ˆì§ˆ ìŠ¤ì½”ì–´ì¹´ë“œ', fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # ì ìˆ˜ êµ¬ê°„ í‘œì‹œ\n",
    "    ax.axvline(x=75, color='orange', linestyle='--', alpha=0.5, label='ì–‘í˜¸ ê¸°ì¤€ (75ì )')\n",
    "    ax.axvline(x=90, color='green', linestyle='--', alpha=0.5, label='ìš°ìˆ˜ ê¸°ì¤€ (90ì )')\n",
    "    ax.legend()\n",
    "    \n",
    "    # ê·¸ë¦¬ë“œ\n",
    "    ax.grid(True, axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('processed_data/quality_scorecard.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ë° ê¶Œì¥ì‚¬í•­ ìƒì„±\n",
    "if train_data is not None:\n",
    "    quality_scores = calculate_quality_score(\n",
    "        train_report, valid_report, train_tokens, similarity_analysis, raft_analysis\n",
    "    )\n",
    "    \n",
    "    recommendations = generate_recommendations(\n",
    "        quality_scores, train_tokens, similarity_analysis\n",
    "    )\n",
    "    \n",
    "    # ê²°ê³¼ ì¶œë ¥\n",
    "    print(\"\\nğŸ† ë°ì´í„° í’ˆì§ˆ ì¢…í•© í‰ê°€ ê²°ê³¼:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    score_names = {\n",
    "        \"integrity_score\": \"ë¬´ê²°ì„± ì ìˆ˜\",\n",
    "        \"diversity_score\": \"ë‹¤ì–‘ì„± ì ìˆ˜\", \n",
    "        \"structure_score\": \"êµ¬ì¡° ì í•©ì„± ì ìˆ˜\",\n",
    "        \"token_efficiency_score\": \"í† í° íš¨ìœ¨ì„± ì ìˆ˜\",\n",
    "        \"raft_balance_score\": \"RAFT ê· í˜• ì ìˆ˜\",\n",
    "        \"overall_score\": \"ğŸ“Š ì¢…í•© í’ˆì§ˆ ì ìˆ˜\"\n",
    "    }\n",
    "    \n",
    "    for key, name in score_names.items():\n",
    "        score = quality_scores[key]\n",
    "        if score >= 90:\n",
    "            status = \"ğŸŸ¢ ìš°ìˆ˜\"\n",
    "        elif score >= 75:\n",
    "            status = \"ğŸŸ¡ ì–‘í˜¸\"\n",
    "        else:\n",
    "            status = \"ğŸ”´ ê°œì„  í•„ìš”\"\n",
    "        \n",
    "        print(f\"{name}: {score:.1f}ì  {status}\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ ê¶Œì¥ì‚¬í•­:\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {rec}\")\n",
    "    \n",
    "    # ìŠ¤ì½”ì–´ì¹´ë“œ ì‹œê°í™”\n",
    "    scorecard_fig = create_quality_scorecard_viz(quality_scores)\n",
    "    print(\"\\nâœ… í’ˆì§ˆ ìŠ¤ì½”ì–´ì¹´ë“œ ì €ì¥: processed_data/quality_scorecard.png\")\n",
    "    \n",
    "    # í’ˆì§ˆ ë¦¬í¬íŠ¸ JSON ì €ì¥\n",
    "    quality_report = {\n",
    "        \"quality_scores\": quality_scores,\n",
    "        \"recommendations\": recommendations,\n",
    "        \"analysis_timestamp\": pd.Timestamp.now().isoformat(),\n",
    "        \"data_summary\": {\n",
    "            \"train_samples\": len(train_data),\n",
    "            \"valid_samples\": len(valid_data),\n",
    "            \"avg_token_length\": np.mean(train_tokens[\"total_tokens\"]) if train_tokens[\"total_tokens\"] else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(\"processed_data/quality_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(quality_report, f, ensure_ascii=False, indent=2, default=str)\n",
    "    \n",
    "    print(\"âœ… í’ˆì§ˆ ë¦¬í¬íŠ¸ ì €ì¥: processed_data/quality_report.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ìµœì¢… ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### âœ… ì™„ë£Œëœ ê²€ì¦ í•­ëª©\n",
    "1. **ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦**: í•„ìˆ˜ í•„ë“œ, ë¹ˆ ê°’, êµ¬ì¡° ì˜¤ë¥˜ í™•ì¸\n",
    "2. **í† í° ê¸¸ì´ ë¶„ì„**: ë¶„í¬, ì´ˆê³¼ ìƒ˜í”Œ, íš¨ìœ¨ì„± í‰ê°€\n",
    "3. **ì¤‘ë³µ ë° ìœ ì‚¬ë„ ë¶„ì„**: ì •í™•í•œ ì¤‘ë³µ, ì˜ë¯¸ì  ìœ ì‚¬ë„ ì¸¡ì •\n",
    "4. **RAFT êµ¬ì¡° ê²€ì¦**: Type ê· í˜•, Context í’ˆì§ˆ, êµ¬ì¡° ì í•©ì„±\n",
    "5. **ì¢…í•© í’ˆì§ˆ í‰ê°€**: 5ê°œ ì˜ì—­ ì ìˆ˜ + ì „ì²´ ì ìˆ˜\n",
    "6. **ì‹œê°í™” ë° ë¦¬í¬íŠ¸**: ëŒ€ì‹œë³´ë“œ, ì°¨íŠ¸, JSON ë¦¬í¬íŠ¸\n",
    "\n",
    "### ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤\n",
    "- `processed_data/token_analysis_dashboard.png`: í† í° ë¶„ì„ ëŒ€ì‹œë³´ë“œ\n",
    "- `processed_data/raft_analysis.png`: RAFT êµ¬ì¡° ë¶„ì„ ì°¨íŠ¸\n",
    "- `processed_data/quality_scorecard.png`: í’ˆì§ˆ ìŠ¤ì½”ì–´ì¹´ë“œ\n",
    "- `processed_data/quality_report.json`: ì¢…í•© í’ˆì§ˆ ë¦¬í¬íŠ¸\n",
    "\n",
    "### ğŸ”„ ë‹¤ìŒ ë‹¨ê³„\n",
    "**03_fine_tuning_with_lora.ipynb**ì—ì„œ ì‹¤ì œ íŒŒì¸íŠœë‹ì„ ì§„í–‰í•©ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¯ Day 1 ì‹¤ìŠµ 2 ì™„ë£Œ!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if train_data is not None:\n",
    "    final_summary = {\n",
    "        \"ë°ì´í„° í˜„í™©\": {\n",
    "            \"Train ìƒ˜í”Œ\": len(train_data),\n",
    "            \"Valid ìƒ˜í”Œ\": len(valid_data),\n",
    "            \"í‰ê·  í† í° ê¸¸ì´\": f\"{np.mean(train_tokens['total_tokens']):.1f}\" if train_tokens['total_tokens'] else \"N/A\",\n",
    "            \"4096 í† í° ì´ˆê³¼ìœ¨\": f\"{len(train_tokens.get('overflow_samples', [])) / len(train_tokens.get('total_tokens', [1])):.1%}\" if train_tokens.get('total_tokens') else \"N/A\"\n",
    "        },\n",
    "        \"RAFT êµ¬ì¡°\": {\n",
    "            \"Positive ìƒ˜í”Œ\": raft_analysis.get(\"type_distribution\", {}).get(\"positive\", 0),\n",
    "            \"Negative ìƒ˜í”Œ\": raft_analysis.get(\"type_distribution\", {}).get(\"negative\", 0),\n",
    "            \"í‰ê·  Context ê°œìˆ˜\": f\"{np.mean(raft_analysis.get('context_analysis', {}).get('context_counts', [4])):.1f}ê°œ\" if raft_analysis.get('context_analysis', {}).get('context_counts') else \"4ê°œ\"\n",
    "        },\n",
    "        \"í’ˆì§ˆ ì ìˆ˜\": {\n",
    "            \"ì¢…í•© ì ìˆ˜\": f\"{quality_scores['overall_score']:.1f}ì \",\n",
    "            \"ë¬´ê²°ì„±\": f\"{quality_scores['integrity_score']:.1f}ì \",\n",
    "            \"ë‹¤ì–‘ì„±\": f\"{quality_scores['diversity_score']:.1f}ì \"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ“Š ìµœì¢… ìš”ì•½:\")\n",
    "    for category, items in final_summary.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for key, value in items.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ ìƒì„±ëœ íŒŒì¼:\")\n",
    "    files = [\n",
    "        \"token_analysis_dashboard.png\",\n",
    "        \"raft_analysis.png\", \n",
    "        \"quality_scorecard.png\",\n",
    "        \"quality_report.json\"\n",
    "    ]\n",
    "    for file in files:\n",
    "        print(f\"  - processed_data/{file}\")\n",
    "    \n",
    "    print(f\"\\nğŸš€ ë‹¤ìŒ ë‹¨ê³„: 03_fine_tuning_with_lora.ipynbì—ì„œ íŒŒì¸íŠœë‹ì„ ì‹œì‘í•˜ì„¸ìš”!\")\n",
    "    \n",
    "    # ì¢…í•© í’ˆì§ˆ ë“±ê¸‰ íŒì •\n",
    "    overall_score = quality_scores['overall_score']\n",
    "    if overall_score >= 90:\n",
    "        grade = \"A (ìš°ìˆ˜)\"\n",
    "        message = \"ë°ì´í„° í’ˆì§ˆì´ ìš°ìˆ˜í•©ë‹ˆë‹¤. íŒŒì¸íŠœë‹ì„ ì§„í–‰í•˜ì„¸ìš”! ğŸŒŸ\"\n",
    "    elif overall_score >= 75:\n",
    "        grade = \"B (ì–‘í˜¸)\"\n",
    "        message = \"ë°ì´í„° í’ˆì§ˆì´ ì–‘í˜¸í•©ë‹ˆë‹¤. íŒŒì¸íŠœë‹ì„ ì§„í–‰í•´ë„ ì¢‹ìŠµë‹ˆë‹¤. âœ…\"\n",
    "    elif overall_score >= 60:\n",
    "        grade = \"C (ë³´í†µ)\"\n",
    "        message = \"ë°ì´í„° í’ˆì§ˆì´ ë³´í†µì…ë‹ˆë‹¤. ê¶Œì¥ì‚¬í•­ì„ ê²€í†  í›„ ì§„í–‰í•˜ì„¸ìš”. âš ï¸\"\n",
    "    else:\n",
    "        grade = \"D (ê°œì„  í•„ìš”)\"\n",
    "        message = \"ë°ì´í„° í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤. ê¶Œì¥ì‚¬í•­ì„ ì ìš©í•˜ì„¸ìš”. ğŸ”„\"\n",
    "    \n",
    "    print(f\"\\nğŸ† ìµœì¢… í’ˆì§ˆ ë“±ê¸‰: {grade}\")\n",
    "    print(f\"ğŸ’¬ {message}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ì–´ ë¶„ì„ì„ ì™„ë£Œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ğŸ’¡ ë¨¼ì € 01_data_preprocessing_and_validation.ipynbë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}