{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 ì‹¤ìŠµ 2: ì „ì²˜ë¦¬ ë°ì´í„° í’ˆì§ˆ ì ê²€\n",
    "\n",
    "ì „ì²˜ë¦¬ëœ `processed_data/` í´ë”ë¥¼ ë¶ˆëŸ¬ì™€ ë°ì´í„° í’ˆì§ˆì„ ë¹ ë¥´ê²Œ ê²€í† í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ëª©ì°¨\n",
    "1. ë°ì´í„° ë¡œë“œ\n",
    "2. í•„ìˆ˜ ì»¬ëŸ¼ ë° ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "3. í…ìŠ¤íŠ¸ ê¸¸ì´/ì¤‘ë³µ ì ê²€\n",
    "4. RAFT í…œí”Œë¦¿ ìƒíƒœ í™•ì¸\n",
    "5. ë©”íƒ€ë°ì´í„° ìš”ì•½\n",
    "6. ë‹¤ìŒ ë‹¨ê³„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë°ì´í„° ë¡œë“œ\n",
    "- Colabì—ì„œ Google Driveì— ì €ì¥í•œ ê²½ìš° ìë™ìœ¼ë¡œ ë§ˆìš´íŠ¸í•©ë‹ˆë‹¤.\n",
    "- ë¡œì»¬ í™˜ê²½ì´ë¼ë©´ ê¸°ë³¸ ê²½ë¡œ `processed_data/`ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "login(token=\"\")  # HF Access Token ì…ë ¥\n",
    "\n",
    "train_df = load_dataset(\"ryanu/raft-processed\", split=\"train\")\n",
    "print(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from datasets import load_from_disk\n",
    "\n",
    "# DATA_DIR = \"processed_data\"\n",
    "# try:\n",
    "#     from google.colab import drive  # type: ignore\n",
    "#     drive.mount('/content/drive')\n",
    "#     DATA_DIR = \"/content/drive/MyDrive/exaone_day1/processed_data\"\n",
    "#     print(f\"âœ… Google Drive ë§ˆìš´íŠ¸: {DATA_DIR}\")\n",
    "# except ModuleNotFoundError:\n",
    "#     print(\"ğŸ“ ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. DATA_DIR=processed_data\")\n",
    "# except NotImplementedError:\n",
    "#     print(\"âš ï¸ ì´ í™˜ê²½ì—ì„œëŠ” drive.mount()ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•„ìš” ì‹œ DATA_DIRë¥¼ ì§ì ‘ ìˆ˜ì •í•˜ì„¸ìš”.\")\n",
    "\n",
    "# DATASET_PATH = DATA_DIR\n",
    "# METADATA_PATH = f\"{DATA_DIR}/metadata.json\"\n",
    "\n",
    "# hf_dataset = load_from_disk(DATASET_PATH)\n",
    "# train_df = hf_dataset.to_pandas()\n",
    "\n",
    "# with open(METADATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "#     metadata = json.load(f)\n",
    "\n",
    "# print(f\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(train_df):,}ê°œ ìƒ˜í”Œ\")\n",
    "# print(f\"   - ì»¬ëŸ¼: {train_df.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í•„ìˆ˜ ì»¬ëŸ¼ ë° ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "Instruction/Input/Output ì»¬ëŸ¼ì´ ëª¨ë‘ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Dataset â†’ pandas DataFrame ë³€í™˜\n",
    "train_df = train_df.to_pandas()\n",
    "\n",
    "required_columns = {\"text\", \"question\", \"answer\"}\n",
    "missing_columns = required_columns - set(train_df.columns)\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}\")\n",
    "\n",
    "print(\"âœ… í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸\")\n",
    "print(\"ê²°ì¸¡ì¹˜ ê°œìˆ˜\")\n",
    "print(train_df[sorted(required_columns)].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. í…ìŠ¤íŠ¸ ê¸¸ì´ ë° ì¤‘ë³µ ì ê²€\n",
    "ê° ì»¬ëŸ¼ì˜ ê¸¸ì´ ë²”ìœ„ì™€ ì¤‘ë³µ ì—¬ë¶€ë¥¼ ê°„ë‹¨íˆ ì‚´í´ë´…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_stats = {\n",
    "    \"text\": train_df[\"text\"].astype(str).apply(len),\n",
    "    \"question\": train_df[\"question\"].astype(str).apply(len),\n",
    "    \"answer\": train_df[\"answer\"].astype(str).apply(len),\n",
    "}\n",
    "\n",
    "for key, series in length_stats.items():\n",
    "    print(f\"ğŸ“ {key} í‰ê·  ê¸¸ì´: {series.mean():.1f}\")\n",
    "    print(f\"   ìµœì†Œ/ìµœëŒ€: {series.min()} / {series.max()}\")\n",
    "\n",
    "duplicates = train_df[\"question\"].astype(str).duplicated().sum()\n",
    "print(f\"ğŸ§¬ ì¤‘ë³µ question ê°œìˆ˜: {duplicates}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAFT í…œí”Œë¦¿ ìƒíƒœ í™•ì¸\n",
    "Distractor ë¬¸ì„œê°€ ì˜ í¬í•¨ë¼ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "if \"distractors\" in train_df.columns:\n",
    "    num_distractors = train_df[\"distractors\"].apply(\n",
    "        lambda x: len(x) if isinstance(x, (list, tuple, np.ndarray)) else 0\n",
    "    )\n",
    "\n",
    "    print(f\"ğŸ¯ í‰ê·  distractor ìˆ˜: {num_distractors.mean():.1f}\")\n",
    "    print(f\"   distractorê°€ ì—†ëŠ” ìƒ˜í”Œ: {(num_distractors == 0).sum()}\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ distractors ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. RAFT ë³€í™˜ ì—†ì´ instruction-input-output ë°ì´í„°ë§Œ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë‹¤ìŒ ë‹¨ê³„\n",
    "- ë°ì´í„°ê°€ ì •ìƒì´ë¼ë©´ main-practice/03ì—ì„œ íŒŒì¸íŠœë‹ì„ ì§„í–‰í•˜ì„¸ìš”.\n",
    "- í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ë„ˆë¬´ ê¸¸ê±°ë‚˜ distractorê°€ ë¶€ì¡±í•˜ë©´ ì‹¤ìŠµ 1(ì „ì²˜ë¦¬)ë¡œ ëŒì•„ê°€ ê·œì¹™ì„ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
