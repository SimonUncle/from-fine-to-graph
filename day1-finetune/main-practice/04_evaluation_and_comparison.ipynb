{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎯 Day 1 실습 4: 베이스라인 vs 파인튜닝 모델 성능 비교\n",
    "\n",
    "## 학습 목표\n",
    "- **핵심 목표**: 파인튜닝 전 (베이스라인) vs 후 (파인튜닝) 모델 성능을 정량적으로 비교\n",
    "- ROUGE, BLEU, 코사인 유사도 등 다양한 메트릭으로 성능 측정\n",
    "- RAG 시나리오에서의 실제 성능 개선 확인\n",
    "- 시각화를 통한 성능 차이 분석\n",
    "- 파인튜닝 효과에 대한 종합적 평가\n",
    "\n",
    "### 💡 Day 1 실습의 핵심\n",
    "**베이스라인(원본 모델) vs 파인튜닝 모델** 성능 비교를 통해 \n",
    "**RAFT 파인튜닝이 실제로 RAG 성능을 개선했는지 검증**하는 것이 이 실습의 가장 중요한 목표입니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 확인 및 설치 (이미 설치된 경우 스킵)\n",
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def check_and_install_package(package_name, import_name=None, version=None):\n",
    "    \"\"\"\n",
    "    패키지 존재 여부 확인 후 필요시에만 설치\n",
    "    \n",
    "    Args:\n",
    "        package_name: pip로 설치할 패키지명\n",
    "        import_name: import할 모듈명 (None이면 package_name 사용)\n",
    "        version: 특정 버전 지정 (None이면 버전 체크 안함)\n",
    "    \"\"\"\n",
    "    if import_name is None:\n",
    "        import_name = package_name.replace('-', '_')\n",
    "    \n",
    "    try:\n",
    "        # 패키지가 이미 설치되어 있는지 확인\n",
    "        module = importlib.import_module(import_name)\n",
    "        \n",
    "        if version is not None and hasattr(module, '__version__'):\n",
    "            current_version = module.__version__\n",
    "            print(f\"✅ {package_name} 이미 설치됨 (버전: {current_version})\")\n",
    "        else:\n",
    "            print(f\"✅ {package_name} 이미 설치됨\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(f\"📦 {package_name} 설치 중...\")\n",
    "        try:\n",
    "            if version:\n",
    "                subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", f\"{package_name}=={version}\"], \n",
    "                             check=True, capture_output=True)\n",
    "            else:\n",
    "                subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package_name], \n",
    "                             check=True, capture_output=True)\n",
    "            print(f\"✅ {package_name} 설치 완료\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"❌ {package_name} 설치 실패: {e}\")\n",
    "            return False\n",
    "\n",
    "print(\"🔍 라이브러리 의존성 확인 중...\")\n",
    "\n",
    "# 필요한 패키지들 확인 및 설치\n",
    "packages_to_check = [\n",
    "    # 코어 라이브러리들은 이미 03번에서 설치했으므로 체크만\n",
    "    (\"transformers\", \"transformers\"),\n",
    "    (\"torch\", \"torch\"),\n",
    "    (\"peft\", \"peft\"),\n",
    "    \n",
    "    # 평가 전용 라이브러리들만 필요시 설치\n",
    "    (\"rouge-score\", \"rouge_score\"),\n",
    "    (\"nltk\", \"nltk\"),\n",
    "    (\"scikit-learn\", \"sklearn\"),\n",
    "    \n",
    "    # 시각화 라이브러리들\n",
    "    (\"matplotlib\", \"matplotlib\"),\n",
    "    (\"seaborn\", \"seaborn\"),\n",
    "    \n",
    "    # 데이터 처리\n",
    "    (\"pandas\", \"pandas\"),\n",
    "    (\"numpy\", \"numpy\"),\n",
    "    (\"tqdm\", \"tqdm\")\n",
    "]\n",
    "\n",
    "print(\"📋 패키지 확인 결과:\")\n",
    "for package_name, import_name in packages_to_check:\n",
    "    check_and_install_package(package_name, import_name)\n",
    "\n",
    "print(\"\\n🎉 모든 의존성 확인 완료!\")\n",
    "print(\"💡 이미 설치된 패키지들은 재설치하지 않았습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    pipeline, BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트 설정 (matplotlib) - 모델 성능 비교 차트에서 한글이 깨지지 않도록 설정\n",
    "print(\"🔧 한글 폰트 설정 중...\")\n",
    "!apt-get update -qq\n",
    "!apt-get install fonts-nanum -qq > /dev/null\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# 나눔바른고딕 폰트 경로 설정\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "fm.fontManager.addfont(fontpath)\n",
    "\n",
    "# matplotlib 설정 업데이트 - 모든 평가 결과 차트에서 한글이 정상적으로 표시됨\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'NanumBarunGothic',  # 기본 폰트를 나눔바른고딕으로 설정\n",
    "    'axes.unicode_minus': False         # 음수 기호 표시 문제 해결 (성능 차이 표시에서 중요)\n",
    "})\n",
    "\n",
    "# 시각화 스타일 설정 - 더 전문적이고 깔끔한 성능 비교 차트를 위한 설정\n",
    "plt.style.use('default')              # 기본 스타일 사용\n",
    "sns.set_palette(\"Set2\")               # 구별하기 쉬운 색상 팔레트 설정\n",
    "\n",
    "print(\"✅ 한글 폰트 설정 완료 - 성능 평가 차트에서 한글이 정상 표시됩니다\")\n",
    "print(\"📦 라이브러리 import 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 및 평가 환경 설정\n",
    "MODEL_NAME = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 파인튜닝된 모델 경로 자동 탐지 - 개선된 버전\n",
    "def find_fine_tuned_model_path():\n",
    "    \"\"\"파인튜닝된 모델 경로를 자동으로 찾는 함수 - 개선된 탐지 알고리즘\"\"\"\n",
    "    print(\"🔍 파인튜닝된 모델 검색 중...\")\n",
    "    \n",
    "    # 1. 다양한 경로 패턴으로 검색\n",
    "    possible_patterns = [\n",
    "        \"./fine_tuned_model*\",     # 기본 타임스탬프 패턴\n",
    "        \"./exaone_raft_lora_*\",    # 03번 노트북에서 생성하는 새 패턴\n",
    "        \"./outputs/*\",             # 대체 경로\n",
    "        \"./*lora*\",               # LoRA가 포함된 모든 디렉토리\n",
    "        \"./*fine*\",               # fine이 포함된 모든 디렉토리\n",
    "        \"./*exaone*\",             # exaone이 포함된 모든 디렉토리\n",
    "    ]\n",
    "    \n",
    "    found_models = []\n",
    "    \n",
    "    for pattern in possible_patterns:\n",
    "        matches = glob.glob(pattern)\n",
    "        for match in matches:\n",
    "            if os.path.isdir(match):  # 디렉토리인지 확인\n",
    "                # 필수 파일들이 존재하는지 확인\n",
    "                adapter_config = os.path.join(match, \"adapter_config.json\")\n",
    "                adapter_model = os.path.join(match, \"adapter_model.safetensors\")\n",
    "                \n",
    "                if os.path.exists(adapter_config) and os.path.exists(adapter_model):\n",
    "                    # 파일 크기 확인 (빈 파일이 아닌지)\n",
    "                    if os.path.getsize(adapter_config) > 0 and os.path.getsize(adapter_model) > 0:\n",
    "                        modification_time = os.path.getctime(match)\n",
    "                        found_models.append({\n",
    "                            \"path\": match,\n",
    "                            \"mtime\": modification_time,\n",
    "                            \"config_size\": os.path.getsize(adapter_config),\n",
    "                            \"model_size\": os.path.getsize(adapter_model)\n",
    "                        })\n",
    "                        print(f\"  ✅ 발견: {match}\")\n",
    "                        print(f\"      Config: {os.path.getsize(adapter_config):,} bytes\")\n",
    "                        print(f\"      Model:  {os.path.getsize(adapter_model):,} bytes\")\n",
    "    \n",
    "    if not found_models:\n",
    "        print(\"  ❌ 유효한 파인튜닝 모델을 찾을 수 없음\")\n",
    "        return None\n",
    "    \n",
    "    # 2. 가장 최근 모델 선택\n",
    "    latest_model = max(found_models, key=lambda x: x[\"mtime\"])\n",
    "    selected_path = latest_model[\"path\"]\n",
    "    \n",
    "    print(f\"\\n🎯 선택된 모델: {selected_path}\")\n",
    "    print(f\"   생성 시간: {datetime.fromtimestamp(latest_model['mtime'])}\")\n",
    "    print(f\"   모델 크기: {latest_model['model_size']:,} bytes\")\n",
    "    \n",
    "    # 3. 추가 유효성 검사\n",
    "    try:\n",
    "        # adapter_config.json 파일을 읽어서 올바른 형식인지 확인\n",
    "        with open(os.path.join(selected_path, \"adapter_config.json\"), 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        if \"peft_type\" in config and config.get(\"peft_type\") == \"LORA\":\n",
    "            print(f\"   ✅ LoRA 설정 확인됨\")\n",
    "            return selected_path\n",
    "        else:\n",
    "            print(f\"   ⚠️ LoRA 설정이 올바르지 않음\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ 설정 파일 검증 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def check_03_notebook_completion():\n",
    "    \"\"\"\n",
    "    03번 노트북의 실행 상태를 확인하는 함수\n",
    "    \"\"\"\n",
    "    print(\"\\n📋 03번 노트북 실행 상태 확인:\")\n",
    "    \n",
    "    # 1. 전처리된 데이터 확인\n",
    "    data_files = [\n",
    "        \"processed_data/train_raft_ko.jsonl\",\n",
    "        \"processed_data/valid_raft_ko.jsonl\",\n",
    "        \"processed_data/metadata.json\"\n",
    "    ]\n",
    "    \n",
    "    data_ready = True\n",
    "    for file_path in data_files:\n",
    "        if os.path.exists(file_path):\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            print(f\"  ✅ {file_path}: {file_size:,} bytes\")\n",
    "        else:\n",
    "            print(f\"  ❌ {file_path}: 파일 없음\")\n",
    "            data_ready = False\n",
    "    \n",
    "    # 2. 파인튜닝 모델 확인\n",
    "    model_path = find_fine_tuned_model_path()\n",
    "    \n",
    "    # 3. 결과 요약\n",
    "    if not data_ready:\n",
    "        print(f\"\\n⚠️ 전처리 데이터가 없습니다.\")\n",
    "        print(f\"   👉 01_data_preprocessing_and_validation.ipynb를 먼저 실행하세요.\")\n",
    "    \n",
    "    if not model_path:\n",
    "        print(f\"\\n⚠️ 파인튜닝 모델이 없습니다.\")\n",
    "        print(f\"   👉 03_fine_tuning_with_lora.ipynb를 먼저 실행하세요.\")\n",
    "        print(f\"\\n💡 03번 노트북 실행 순서:\")\n",
    "        print(f\"   1. 모든 셀을 순서대로 실행\")\n",
    "        print(f\"   2. 19번 셀(학습 실행)에서 완료될 때까지 대기\")\n",
    "        print(f\"   3. 23번 셀(모델 저장)에서 저장 완료 확인\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"\\n✅ 03번 노트북이 정상적으로 완료되었습니다.\")\n",
    "    return True\n",
    "\n",
    "# 파인튜닝 모델 경로 찾기\n",
    "ADAPTER_PATH = find_fine_tuned_model_path()\n",
    "\n",
    "print(f\"🔧 평가 환경 설정:\")\n",
    "print(f\"  디바이스: {device}\")\n",
    "print(f\"  베이스 모델: {MODEL_NAME}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"  GPU 메모리: {gpu_memory:.1f}GB\")\n",
    "else:\n",
    "    print(\"  ⚠️ CUDA를 사용할 수 없습니다. CPU에서 실행됩니다.\")\n",
    "\n",
    "# 실행 상태 종합 확인\n",
    "notebook_ready = check_03_notebook_completion()\n",
    "\n",
    "if ADAPTER_PATH and notebook_ready:\n",
    "    print(f\"\\n🎉 완전한 베이스라인 vs 파인튜닝 모델 비교 준비 완료!\")\n",
    "    print(f\"  파인튜닝 모델: {ADAPTER_PATH}\")\n",
    "elif ADAPTER_PATH:\n",
    "    print(f\"\\n✅ 파인튜닝 모델 발견: {ADAPTER_PATH}\")\n",
    "    print(f\"🔥 베이스라인 vs 파인튜닝 모델 비교 가능!\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ 파인튜닝 모델을 찾을 수 없습니다!\")\n",
    "    print(f\"🔄 현재는 베이스라인 모델만 평가됩니다.\")\n",
    "    print(f\"\\n💡 파인튜닝 모델을 생성하려면:\")\n",
    "    print(f\"   1. 03_fine_tuning_with_lora.ipynb 열기\")\n",
    "    print(f\"   2. 모든 셀을 처음부터 끝까지 실행\")\n",
    "    print(f\"   3. 학습 완료 후 모델 저장 확인\")\n",
    "    print(f\"   4. 이 노트북을 다시 실행\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 평가 환경 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 평가 메트릭 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationMetrics:\n",
    "    \"\"\"다양한 평가 메트릭을 계산하는 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(\n",
    "            ['rouge1', 'rouge2', 'rougeL'], use_stemmer=True\n",
    "        )\n",
    "        self.smoothing = SmoothingFunction().method4\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "    \n",
    "    def compute_rouge(self, predictions, references):\n",
    "        \"\"\"ROUGE 점수 계산\"\"\"\n",
    "        rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "        \n",
    "        for pred, ref in zip(predictions, references):\n",
    "            scores = self.rouge_scorer.score(ref, pred)\n",
    "            for key in rouge_scores:\n",
    "                rouge_scores[key].append(scores[key].fmeasure)\n",
    "        \n",
    "        return {key: np.mean(values) for key, values in rouge_scores.items()}\n",
    "    \n",
    "    def compute_bleu(self, predictions, references):\n",
    "        \"\"\"BLEU 점수 계산\"\"\"\n",
    "        bleu_scores = []\n",
    "        \n",
    "        for pred, ref in zip(predictions, references):\n",
    "            pred_tokens = pred.split()\n",
    "            ref_tokens = [ref.split()]  # BLEU는 reference를 리스트로 요구\n",
    "            \n",
    "            try:\n",
    "                score = sentence_bleu(ref_tokens, pred_tokens, smoothing_function=self.smoothing)\n",
    "                bleu_scores.append(score)\n",
    "            except:\n",
    "                bleu_scores.append(0.0)\n",
    "        \n",
    "        return np.mean(bleu_scores)\n",
    "    \n",
    "    def compute_cosine_similarity(self, predictions, references):\n",
    "        \"\"\"코사인 유사도 계산\"\"\"\n",
    "        try:\n",
    "            all_texts = predictions + references\n",
    "            tfidf_matrix = self.tfidf.fit_transform(all_texts)\n",
    "            \n",
    "            pred_vectors = tfidf_matrix[:len(predictions)]\n",
    "            ref_vectors = tfidf_matrix[len(predictions):]\n",
    "            \n",
    "            similarities = []\n",
    "            for i in range(len(predictions)):\n",
    "                sim = cosine_similarity(\n",
    "                    pred_vectors[i:i+1], ref_vectors[i:i+1]\n",
    "                )[0][0]\n",
    "                similarities.append(sim)\n",
    "            \n",
    "            return np.mean(similarities)\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "evaluator = EvaluationMetrics()\n",
    "print(\"✅ 평가 메트릭 클래스 초기화 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 평가 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evaluation_dataset():\n",
    "    \"\"\"\n",
    "    평가 데이터셋 로드 함수 - RAG 성능 평가에 적합한 데이터를 준비\n",
    "    \"\"\"\n",
    "    print(\"🔄 평가 데이터셋 로드 중...\")\n",
    "    \n",
    "    try:\n",
    "        # 전처리된 데이터가 있는지 먼저 확인\n",
    "        if os.path.exists(\"processed_data/valid_raft_ko.jsonl\"):\n",
    "            print(\"📁 전처리된 검증 데이터 사용\")\n",
    "            import jsonlines\n",
    "            \n",
    "            valid_data = []\n",
    "            with jsonlines.open(\"processed_data/valid_raft_ko.jsonl\", \"r\") as reader:\n",
    "                valid_data = list(reader)\n",
    "            \n",
    "            # RAFT 형식에서 평가 형식으로 변환\n",
    "            eval_samples = []\n",
    "            for item in valid_data[:50]:  # 평가 시간 고려하여 50개 제한\n",
    "                if \"original_question\" in item and \"original_answer\" in item:\n",
    "                    # Context 추출 (User 메시지에서)\n",
    "                    user_content = item[\"messages\"][1][\"content\"] if \"messages\" in item else \"\"\n",
    "                    context_match = user_content.split(\"=== 질문 ===\")[0] if \"=== 질문 ===\" in user_content else \"\"\n",
    "                    context = context_match.replace(\"=== 컨텍스트 ===\", \"\").strip()\n",
    "                    \n",
    "                    eval_samples.append({\n",
    "                        \"context\": context,\n",
    "                        \"question\": item[\"original_question\"],\n",
    "                        \"answer\": item[\"original_answer\"]\n",
    "                    })\n",
    "            \n",
    "            print(f\"✅ 전처리된 검증 데이터에서 {len(eval_samples)}개 샘플 로드\")\n",
    "            return eval_samples\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 전처리된 데이터 로드 실패: {e}\")\n",
    "    \n",
    "    # 대안: 원본 데이터셋 사용\n",
    "    try:\n",
    "        print(\"📂 원본 데이터셋 로드\")\n",
    "        dataset = load_dataset(\"neural-bridge/rag-dataset-12000\")\n",
    "        \n",
    "        eval_size = min(50, len(dataset[\"train\"]))  # 평가 시간 단축\n",
    "        eval_data = dataset[\"train\"].select(range(eval_size))\n",
    "        \n",
    "        eval_samples = []\n",
    "        for item in eval_data:\n",
    "            eval_samples.append({\n",
    "                \"context\": item.get(\"context\", \"\"),\n",
    "                \"question\": item.get(\"question\", \"\"),\n",
    "                \"answer\": item.get(\"answer\", \"\")\n",
    "            })\n",
    "        \n",
    "        print(f\"✅ 원본 데이터셋에서 {len(eval_samples)}개 샘플 로드\")\n",
    "        return eval_samples\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 데이터셋 로드 실패: {e}\")\n",
    "        \n",
    "        # 최종 대안: 샘플 데이터 생성\n",
    "        print(\"🔄 샘플 평가 데이터 생성\")\n",
    "        sample_data = [\n",
    "            {\n",
    "                \"context\": \"한국의 수도는 서울입니다. 서울은 한강을 중심으로 발달했으며, 약 천만 명의 인구가 거주합니다.\",\n",
    "                \"question\": \"한국의 수도는 어디인가요?\",\n",
    "                \"answer\": \"한국의 수도는 서울입니다.\"\n",
    "            },\n",
    "            {\n",
    "                \"context\": \"김치는 한국의 전통 발효 식품입니다. 배추와 다양한 양념을 사용하여 만들며, 건강에 좋은 유산균이 풍부합니다.\",\n",
    "                \"question\": \"김치는 어떤 음식인가요?\",\n",
    "                \"answer\": \"김치는 한국의 전통 발효 식품으로, 배추와 양념으로 만들어지며 유산균이 풍부합니다.\"\n",
    "            },\n",
    "            {\n",
    "                \"context\": \"인공지능은 컴퓨터 시스템이 인간의 지능적인 행동을 모방할 수 있도록 하는 기술입니다. 머신러닝과 딥러닝이 핵심 기술입니다.\",\n",
    "                \"question\": \"인공지능이란 무엇인가요?\",\n",
    "                \"answer\": \"인공지능은 컴퓨터가 인간의 지능적 행동을 모방하는 기술로, 머신러닝과 딥러닝이 핵심입니다.\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        print(f\"📝 {len(sample_data)}개 샘플 데이터 생성 완료\")\n",
    "        return sample_data\n",
    "\n",
    "# 평가 데이터 로드\n",
    "eval_samples = load_evaluation_dataset()\n",
    "print(f\"\\n📊 평가 데이터 준비 완료: {len(eval_samples)}개 샘플\")\n",
    "if eval_samples:\n",
    "    print(f\"\\n📋 첫 번째 샘플 미리보기:\")\n",
    "    print(f\"  Context: {eval_samples[0]['context'][:100]}...\")\n",
    "    print(f\"  Question: {eval_samples[0]['question']}\")\n",
    "    print(f\"  Answer: {eval_samples[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    \"\"\"\n",
    "    베이스라인과 파인튜닝 모델을 모두 로드하는 함수\n",
    "    \"\"\"\n",
    "    print(\"🔄 모델 로드 중...\")\n",
    "    \n",
    "    # 4-bit 양자화 설정 (메모리 효율성)\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,  # 안정성을 위해 bfloat16 사용\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    \n",
    "    # 토크나이저 로드\n",
    "    print(\"📝 토크나이저 로드...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"✅ 토크나이저 로드 완료 (vocab size: {tokenizer.vocab_size:,})\")\n",
    "    \n",
    "    # 베이스라인 모델 로드\n",
    "    print(\"🤖 베이스라인 모델 로드...\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"✅ 베이스라인 모델 로드 완료\")\n",
    "    \n",
    "    # 파인튜닝 모델 로드 시도 - 개선된 오류 처리\n",
    "    fine_tuned_model = None\n",
    "    if ADAPTER_PATH and os.path.exists(ADAPTER_PATH):\n",
    "        try:\n",
    "            print(\"🎯 파인튜닝 모델 로드...\")\n",
    "            print(f\"   모델 경로: {ADAPTER_PATH}\")\n",
    "            \n",
    "            # 필수 파일 존재 확인\n",
    "            required_files = [\n",
    "                \"adapter_config.json\",\n",
    "                \"adapter_model.safetensors\"\n",
    "            ]\n",
    "            \n",
    "            missing_files = []\n",
    "            for file_name in required_files:\n",
    "                file_path = os.path.join(ADAPTER_PATH, file_name)\n",
    "                if not os.path.exists(file_path):\n",
    "                    missing_files.append(file_name)\n",
    "                else:\n",
    "                    file_size = os.path.getsize(file_path)\n",
    "                    print(f\"   ✅ {file_name}: {file_size:,} bytes\")\n",
    "            \n",
    "            if missing_files:\n",
    "                print(f\"   ❌ 필수 파일이 없습니다: {', '.join(missing_files)}\")\n",
    "                raise FileNotFoundError(f\"Missing required files: {missing_files}\")\n",
    "            \n",
    "            # PEFT 어댑터 적용\n",
    "            fine_tuned_model = PeftModel.from_pretrained(\n",
    "                base_model, \n",
    "                ADAPTER_PATH,\n",
    "                torch_dtype=torch.bfloat16\n",
    "            )\n",
    "            print(\"✅ 파인튜닝 모델 로드 완료\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"❌ 파인튜닝 모델 로드 실패: {error_msg}\")\n",
    "            \n",
    "            # 구체적인 오류 분석 및 해결 방안 제시\n",
    "            if \"adapter_config.json\" in error_msg:\n",
    "                print(f\"\\n🔍 문제 분석: LoRA 어댑터 설정 파일을 찾을 수 없습니다.\")\n",
    "                print(f\"💡 해결 방법:\")\n",
    "                print(f\"   1. 03번 노트북에서 파인튜닝이 완료되었는지 확인\")\n",
    "                print(f\"   2. 23번 셀(모델 저장)이 성공적으로 실행되었는지 확인\")\n",
    "                print(f\"   3. 저장된 모델 경로에 다음 파일들이 있는지 확인:\")\n",
    "                for file_name in required_files:\n",
    "                    file_path = os.path.join(ADAPTER_PATH, file_name)\n",
    "                    if os.path.exists(file_path):\n",
    "                        print(f\"      ✅ {file_name}\")\n",
    "                    else:\n",
    "                        print(f\"      ❌ {file_name} (없음)\")\n",
    "                        \n",
    "            elif \"safetensors\" in error_msg or \"model\" in error_msg:\n",
    "                print(f\"\\n🔍 문제 분석: LoRA 어댑터 모델 파일에 문제가 있습니다.\")\n",
    "                print(f\"💡 해결 방법:\")\n",
    "                print(f\"   1. 03번 노트북의 학습이 중단 없이 완료되었는지 확인\")\n",
    "                print(f\"   2. GPU 메모리 부족으로 저장이 실패했을 가능성 확인\")\n",
    "                print(f\"   3. 03번 노트북을 처음부터 다시 실행\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"\\n🔍 문제 분석: 일반적인 모델 로드 오류\")\n",
    "                print(f\"💡 해결 방법:\")\n",
    "                print(f\"   1. 03번 노트북 전체를 다시 실행\")\n",
    "                print(f\"   2. GPU 메모리 부족 시 런타임 재시작 후 재실행\")\n",
    "                print(f\"   3. 파인튜닝 설정을 더 보수적으로 조정\")\n",
    "            \n",
    "            print(f\"\\n📋 현재 상태:\")\n",
    "            print(f\"   베이스라인 모델: ✅ 로드됨\")\n",
    "            print(f\"   파인튜닝 모델: ❌ 로드 실패\")\n",
    "            print(f\"   평가 가능 여부: 베이스라인만 평가 가능\")\n",
    "            print(f\"💡 베이스라인 모델만으로 평가를 진행합니다.\")\n",
    "            fine_tuned_model = None\n",
    "            \n",
    "    else:\n",
    "        print(\"⚠️ 파인튜닝 모델 경로를 찾을 수 없음\")\n",
    "        print(f\"\\n📋 파인튜닝 모델 생성 가이드:\")\n",
    "        print(f\"   1. 03_fine_tuning_with_lora.ipynb 노트북 열기\")\n",
    "        print(f\"   2. 셀 1-2: 라이브러리 설치\")\n",
    "        print(f\"   3. 셀 3-11: 데이터 로드 및 전처리\")\n",
    "        print(f\"   4. 셀 12-19: 모델 설정 및 학습 실행 (시간이 오래 걸림)\")\n",
    "        print(f\"   5. 셀 20-23: 결과 분석 및 모델 저장\")\n",
    "        print(f\"   6. 이 노트북으로 돌아와서 다시 실행\")\n",
    "    \n",
    "    return tokenizer, base_model, fine_tuned_model\n",
    "\n",
    "# 파인튜닝 모델 상태 진단 함수\n",
    "def diagnose_fine_tuning_status():\n",
    "    \"\"\"\n",
    "    파인튜닝 상태를 진단하고 사용자에게 구체적인 가이드 제공\n",
    "    \"\"\"\n",
    "    print(\"🔍 파인튜닝 상태 진단:\")\n",
    "    \n",
    "    # 1. 데이터 준비 상태 확인\n",
    "    data_status = \"✅\"\n",
    "    if not os.path.exists(\"processed_data\"):\n",
    "        data_status = \"❌\"\n",
    "        print(\"   데이터 전처리: ❌ (01번 노트북 미실행)\")\n",
    "    elif not os.path.exists(\"processed_data/train_raft_ko.jsonl\"):\n",
    "        data_status = \"⚠️\"\n",
    "        print(\"   데이터 전처리: ⚠️ (일부 파일 누락)\")\n",
    "    else:\n",
    "        print(\"   데이터 전처리: ✅\")\n",
    "    \n",
    "    # 2. 파인튜닝 모델 상태 확인\n",
    "    if ADAPTER_PATH:\n",
    "        print(f\"   파인튜닝 모델: ✅ 경로 발견 ({ADAPTER_PATH})\")\n",
    "        \n",
    "        # 파일 상세 확인\n",
    "        config_path = os.path.join(ADAPTER_PATH, \"adapter_config.json\")\n",
    "        model_path = os.path.join(ADAPTER_PATH, \"adapter_model.safetensors\")\n",
    "        \n",
    "        if os.path.exists(config_path) and os.path.exists(model_path):\n",
    "            config_size = os.path.getsize(config_path)\n",
    "            model_size = os.path.getsize(model_path)\n",
    "            print(f\"      Config 파일: {config_size:,} bytes\")\n",
    "            print(f\"      Model 파일: {model_size:,} bytes\")\n",
    "            \n",
    "            if model_size < 1000000:  # 1MB보다 작으면 문제\n",
    "                print(\"      ⚠️ 모델 파일이 너무 작습니다. 학습이 제대로 되지 않았을 수 있습니다.\")\n",
    "            else:\n",
    "                print(\"      ✅ 파일 크기가 정상적입니다.\")\n",
    "        else:\n",
    "            print(\"   파인튜닝 모델: ❌ 필수 파일 누락\")\n",
    "    else:\n",
    "        print(\"   파인튜닝 모델: ❌ 모델 없음\")\n",
    "    \n",
    "    # 3. 권장 사항 제시\n",
    "    print(f\"\\n💡 다음 단계:\")\n",
    "    if data_status == \"❌\":\n",
    "        print(\"   👉 01_data_preprocessing_and_validation.ipynb 먼저 실행\")\n",
    "    elif not ADAPTER_PATH:\n",
    "        print(\"   👉 03_fine_tuning_with_lora.ipynb 실행하여 파인튜닝 수행\")\n",
    "        print(\"      - 예상 소요 시간: 30-60분 (GPU 성능에 따라)\")\n",
    "        print(\"      - 완료 후 이 노트북 다시 실행\")\n",
    "    else:\n",
    "        print(\"   👉 베이스라인 vs 파인튜닝 모델 비교 평가 진행 가능!\")\n",
    "\n",
    "# 모델들 로드\n",
    "tokenizer, base_model, fine_tuned_model = load_models()\n",
    "\n",
    "# 파인튜닝 상태 진단 (파인튜닝 모델이 없는 경우)\n",
    "if fine_tuned_model is None:\n",
    "    diagnose_fine_tuning_status()\n",
    "\n",
    "# GPU 메모리 사용량 확인\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\n💾 현재 GPU 메모리 사용량:\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        print(f\"  GPU {i}: {allocated:.1f}GB / {total:.1f}GB ({allocated/total:.1%})\")\n",
    "\n",
    "print(f\"\\n🎯 로드 완료된 모델:\")\n",
    "print(f\"  ✅ 베이스라인 모델: {MODEL_NAME}\")\n",
    "if fine_tuned_model is not None:\n",
    "    print(f\"  ✅ 파인튜닝 모델: {ADAPTER_PATH}\")\n",
    "    print(f\"  🔥 베이스라인 vs 파인튜닝 비교 가능!\")\n",
    "else:\n",
    "    print(f\"  ⚠️ 파인튜닝 모델: 로드 실패\")\n",
    "    print(f\"  📊 베이스라인 모델만 평가\")\n",
    "    print(f\"\\n🎯 완전한 비교를 위해서는:\")\n",
    "    print(f\"     03_fine_tuning_with_lora.ipynb 완료 → 이 노트북 재실행\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 추론 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, context, question, max_new_tokens=150):\n",
    "    \"\"\"\n",
    "    모델로부터 응답을 생성하는 함수 - EXAONE 채팅 템플릿 사용\n",
    "    \n",
    "    Args:\n",
    "        model: 추론에 사용할 모델\n",
    "        tokenizer: 토크나이저\n",
    "        context: 참조할 컨텍스트\n",
    "        question: 질문\n",
    "        max_new_tokens: 생성할 최대 토큰 수\n",
    "        \n",
    "    Returns:\n",
    "        생성된 응답 텍스트\n",
    "    \"\"\"\n",
    "    \n",
    "    # EXAONE 모델에 최적화된 프롬프트 형식\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"당신은 주어진 컨텍스트를 바탕으로 질문에 정확하고 도움이 되는 답변을 제공하는 AI 어시스턴트입니다. 컨텍스트에 없는 정보는 추측하지 말고, 주어진 정보만을 바탕으로 답변하세요.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"다음 컨텍스트를 참고하여 질문에 답변해주세요.\n",
    "\n",
    "=== 컨텍스트 ===\n",
    "{context}\n",
    "\n",
    "=== 질문 ===\n",
    "{question}\n",
    "\n",
    "위 컨텍스트를 바탕으로 질문에 대한 정확한 답변을 해주세요.\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # 채팅 템플릿 적용\n",
    "    try:\n",
    "        formatted_input = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    except Exception:\n",
    "        # 채팅 템플릿이 지원되지 않는 경우 대안 형식\n",
    "        formatted_input = f\"\"\"시스템: 주어진 컨텍스트를 바탕으로 질문에 답변하세요.\n",
    "\n",
    "사용자: 컨텍스트: {context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "어시스턴트:\"\"\"\n",
    "    \n",
    "    # 토크나이징\n",
    "    inputs = tokenizer(\n",
    "        formatted_input,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=2048  # 컨텍스트가 길어질 수 있으므로 충분한 길이 확보\n",
    "    )\n",
    "    \n",
    "    # GPU 사용 가능시 GPU로 이동\n",
    "    if torch.cuda.is_available() and hasattr(model, 'device'):\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # 응답 생성\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,                    # 적당한 창의성\n",
    "            do_sample=True,                     # 샘플링 활성화\n",
    "            top_p=0.9,                         # Nucleus sampling\n",
    "            repetition_penalty=1.1,            # 반복 방지\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # 생성된 부분만 디코딩 (입력 제외)\n",
    "    generated_tokens = outputs[0][inputs['input_ids'].shape[-1]:]\n",
    "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    \n",
    "    # 불필요한 접두어 제거\n",
    "    if response.startswith(\"어시스턴트:\"):\n",
    "        response = response[5:].strip()\n",
    "    elif response.startswith(\"답변:\"):\n",
    "        response = response[3:].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"✅ 응답 생성 함수 정의 완료 (EXAONE 최적화)\")\n",
    "print(\"  - EXAONE 채팅 템플릿 사용\")\n",
    "print(\"  - 컨텍스트 기반 답변 생성\")\n",
    "print(\"  - 반복 방지 및 품질 최적화\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name, eval_samples):\n",
    "    \"\"\"\n",
    "    모델 평가 함수 - RAG 성능을 종합적으로 평가\n",
    "    \n",
    "    Args:\n",
    "        model: 평가할 모델\n",
    "        model_name: 모델 이름 (결과 표시용)\n",
    "        eval_samples: 평가 데이터 (context, question, answer 포함)\n",
    "        \n",
    "    Returns:\n",
    "        평가 결과 딕셔너리\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n🔍 {model_name} 평가 시작...\")\n",
    "    print(f\"  평가 샘플 수: {len(eval_samples)}개\")\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    generation_times = []  # 응답 생성 시간도 측정\n",
    "    \n",
    "    # 추론 실행\n",
    "    for i, sample in enumerate(tqdm(eval_samples, desc=f\"{model_name} 추론\", ncols=80)):\n",
    "        context = sample.get('context', '')\n",
    "        question = sample.get('question', '')\n",
    "        answer = sample.get('answer', '')\n",
    "        \n",
    "        if not context or not question or not answer:\n",
    "            print(f\"  ⚠️ 샘플 {i}: 불완전한 데이터 건너뛰기\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # 응답 생성 시간 측정\n",
    "            import time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            prediction = generate_response(model, tokenizer, context, question)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            generation_time = end_time - start_time\n",
    "            \n",
    "            # 결과 저장\n",
    "            if prediction and prediction.strip():  # 빈 응답 필터링\n",
    "                predictions.append(prediction.strip())\n",
    "                references.append(answer.strip())\n",
    "                contexts.append(context)\n",
    "                questions.append(question)\n",
    "                generation_times.append(generation_time)\n",
    "            else:\n",
    "                print(f\"  ⚠️ 샘플 {i}: 빈 응답 생성\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ 샘플 {i} 처리 실패: {str(e)[:100]}...\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n📊 {model_name} 메트릭 계산 중... (유효 샘플: {len(predictions)}개)\")\n",
    "    \n",
    "    if not predictions:\n",
    "        print(f\"❌ {model_name}: 평가할 수 있는 예측 결과가 없습니다.\")\n",
    "        return None\n",
    "    \n",
    "    # 메트릭 계산\n",
    "    try:\n",
    "        rouge_scores = evaluator.compute_rouge(predictions, references)\n",
    "        bleu_score = evaluator.compute_bleu(predictions, references)\n",
    "        cosine_sim = evaluator.compute_cosine_similarity(predictions, references)\n",
    "        \n",
    "        # 추가 메트릭: 평균 응답 길이, 응답 시간\n",
    "        avg_prediction_length = np.mean([len(pred.split()) for pred in predictions])\n",
    "        avg_reference_length = np.mean([len(ref.split()) for ref in references])\n",
    "        avg_generation_time = np.mean(generation_times) if generation_times else 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 메트릭 계산 실패: {e}\")\n",
    "        return None\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'sample_count': len(predictions),\n",
    "        'rouge1': rouge_scores['rouge1'],\n",
    "        'rouge2': rouge_scores['rouge2'],\n",
    "        'rougeL': rouge_scores['rougeL'],\n",
    "        'bleu': bleu_score,\n",
    "        'cosine_similarity': cosine_sim,\n",
    "        'avg_prediction_length': avg_prediction_length,\n",
    "        'avg_reference_length': avg_reference_length,\n",
    "        'avg_generation_time': avg_generation_time,\n",
    "        'predictions': predictions,\n",
    "        'references': references,\n",
    "        'contexts': contexts,\n",
    "        'questions': questions\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ {model_name} 평가 완료\")\n",
    "    print(f\"  ROUGE-1: {rouge_scores['rouge1']:.3f}\")\n",
    "    print(f\"  ROUGE-L: {rouge_scores['rougeL']:.3f}\")\n",
    "    print(f\"  BLEU: {bleu_score:.3f}\")\n",
    "    print(f\"  코사인 유사도: {cosine_sim:.3f}\")\n",
    "    print(f\"  평균 응답 시간: {avg_generation_time:.2f}초\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 모델 평가 실행\n",
    "if eval_samples:\n",
    "    print(\"🎯 베이스라인 vs 파인튜닝 모델 성능 비교 시작!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 베이스라인 모델 평가\n",
    "    baseline_results = evaluate_model(base_model, \"🤖 베이스라인 모델\", eval_samples)\n",
    "    \n",
    "    # 파인튜닝 모델 평가 (있는 경우)\n",
    "    finetuned_results = None\n",
    "    if fine_tuned_model is not None:\n",
    "        finetuned_results = evaluate_model(fine_tuned_model, \"🎯 파인튜닝 모델\", eval_samples)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"✅ 모델 평가 완료!\")\n",
    "    \n",
    "    if baseline_results and finetuned_results:\n",
    "        print(\"🔥 베이스라인 vs 파인튜닝 비교 가능!\")\n",
    "    elif baseline_results:\n",
    "        print(\"📊 베이스라인 모델 결과만 사용 가능\")\n",
    "    else:\n",
    "        print(\"❌ 평가할 수 있는 결과가 없습니다.\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ 평가 데이터가 없어 모델 평가를 수행할 수 없습니다.\")\n",
    "    baseline_results = None\n",
    "    finetuned_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 평가 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_table(baseline_results, finetuned_results=None):\n",
    "    \"\"\"\n",
    "    베이스라인과 파인튜닝 모델의 성능 비교 테이블 생성\n",
    "    \n",
    "    Args:\n",
    "        baseline_results: 베이스라인 모델 평가 결과\n",
    "        finetuned_results: 파인튜닝 모델 평가 결과 (선택사항)\n",
    "        \n",
    "    Returns:\n",
    "        비교 결과 DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = ['rouge1', 'rouge2', 'rougeL', 'bleu', 'cosine_similarity', 'avg_generation_time']\n",
    "    metric_names = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'BLEU', '코사인 유사도', '응답시간(초)']\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    if baseline_results:\n",
    "        baseline_row = [baseline_results['model_name']] + [baseline_results[metric] for metric in metrics]\n",
    "        data.append(baseline_row)\n",
    "    \n",
    "    if finetuned_results:\n",
    "        finetuned_row = [finetuned_results['model_name']] + [finetuned_results[metric] for metric in metrics]\n",
    "        data.append(finetuned_row)\n",
    "    \n",
    "    columns = ['모델'] + metric_names\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_improvement(baseline_results, finetuned_results):\n",
    "    \"\"\"\n",
    "    파인튜닝으로 인한 성능 개선 계산\n",
    "    \n",
    "    Returns:\n",
    "        개선율 딕셔너리\n",
    "    \"\"\"\n",
    "    if not baseline_results or not finetuned_results:\n",
    "        return None\n",
    "    \n",
    "    metrics = ['rouge1', 'rouge2', 'rougeL', 'bleu', 'cosine_similarity']\n",
    "    improvements = {}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        baseline_score = baseline_results[metric]\n",
    "        finetuned_score = finetuned_results[metric]\n",
    "        \n",
    "        if baseline_score > 0:\n",
    "            improvement = ((finetuned_score - baseline_score) / baseline_score) * 100\n",
    "            improvements[metric] = improvement\n",
    "        else:\n",
    "            improvements[metric] = 0\n",
    "    \n",
    "    return improvements\n",
    "\n",
    "# 결과 분석 및 표시\n",
    "if baseline_results:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"📊 모델 성능 비교 결과\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 비교 테이블 생성\n",
    "    comparison_df = create_comparison_table(baseline_results, finetuned_results)\n",
    "    \n",
    "    # 결과 테이블 출력\n",
    "    print(\"\\n📋 전체 성능 비교:\")\n",
    "    print(comparison_df.round(4).to_string(index=False))\n",
    "    \n",
    "    # 개선율 계산 및 출력\n",
    "    if finetuned_results:\n",
    "        improvements = calculate_improvement(baseline_results, finetuned_results)\n",
    "        \n",
    "        if improvements:\n",
    "            print(\"\\n🚀 파인튜닝으로 인한 성능 개선:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            improvement_data = []\n",
    "            for metric, improvement in improvements.items():\n",
    "                metric_name = {\n",
    "                    'rouge1': 'ROUGE-1',\n",
    "                    'rouge2': 'ROUGE-2', \n",
    "                    'rougeL': 'ROUGE-L',\n",
    "                    'bleu': 'BLEU',\n",
    "                    'cosine_similarity': '코사인 유사도'\n",
    "                }[metric]\n",
    "                \n",
    "                if improvement > 0:\n",
    "                    status = \"📈 개선\"\n",
    "                elif improvement < 0:\n",
    "                    status = \"📉 악화\"\n",
    "                else:\n",
    "                    status = \"➖ 동일\"\n",
    "                \n",
    "                improvement_data.append([metric_name, f\"{improvement:+.2f}%\", status])\n",
    "                print(f\"  {metric_name}: {improvement:+.2f}% {status}\")\n",
    "            \n",
    "            # 종합 개선 평가\n",
    "            avg_improvement = np.mean(list(improvements.values()))\n",
    "            print(f\"\\n🎯 종합 평균 개선율: {avg_improvement:+.2f}%\")\n",
    "            \n",
    "            if avg_improvement > 5:\n",
    "                print(\"🌟 파인튜닝이 상당한 성능 향상을 가져왔습니다!\")\n",
    "            elif avg_improvement > 1:\n",
    "                print(\"✅ 파인튜닝이 성능을 개선했습니다.\")\n",
    "            elif avg_improvement > -1:\n",
    "                print(\"➖ 파인튜닝 효과가 미미합니다.\")\n",
    "            else:\n",
    "                print(\"⚠️ 파인튜닝 후 성능이 다소 저하되었습니다.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\n⚠️ 파인튜닝 모델이 없어 성능 비교를 수행할 수 없습니다.\")\n",
    "        print(\"💡 먼저 03_fine_tuning_with_lora.ipynb를 완료하세요.\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ 평가 결과가 없어 성능 비교를 수행할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 성능 비교 시각화\n",
    "\n",
    "### 📊 베이스라인 vs 파인튜닝 모델 성능 차이를 시각적으로 분석\n",
    "- **막대 차트**: 각 메트릭별 직접적인 성능 비교  \n",
    "- **방사형 차트**: 종합적인 성능 프로필 비교\n",
    "- **개선율 차트**: 파인튜닝으로 인한 개선 정도 시각화\n",
    "- **응답 예시 비교**: 실제 생성 결과 질적 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_visualization(baseline_results, finetuned_results=None):\n",
    "    \"\"\"\n",
    "    베이스라인 vs 파인튜닝 모델 성능 비교 시각화 생성\n",
    "    \n",
    "    이 함수는 여러 관점에서 모델 성능을 시각적으로 비교합니다:\n",
    "    1. 📊 메트릭별 막대 차트: 각 성능 지표의 직접적인 비교\n",
    "    2. 🎯 방사형 차트: 전체적인 성능 프로필 비교  \n",
    "    3. 📈 개선율 차트: 파인튜닝으로 인한 개선 정도\n",
    "    4. ⏱️ 응답 시간 비교: 실용성 측면의 성능 분석\n",
    "    \"\"\"\n",
    "    \n",
    "    if not baseline_results:\n",
    "        print(\"❌ 베이스라인 결과가 없어 시각화를 생성할 수 없습니다.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"🎨 성능 비교 시각화 생성 중...\")\n",
    "    \n",
    "    # 시각화 설정\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('🎯 베이스라인 vs 파인튜닝 모델 종합 성능 비교', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # 1. 메트릭별 막대 차트 (좌상단)\n",
    "    # 📊 의미: 각 평가 지표에서 두 모델의 성능을 직접적으로 비교\n",
    "    # - ROUGE: 텍스트 중복도 기반 유사성 (높을수록 좋음)\n",
    "    # - BLEU: 번역 품질 측정 지표 (높을수록 좋음) \n",
    "    # - 코사인 유사도: 의미적 유사성 (높을수록 좋음)\n",
    "    metrics = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'BLEU', '코사인 유사도']\n",
    "    baseline_scores = [\n",
    "        baseline_results['rouge1'], baseline_results['rouge2'], \n",
    "        baseline_results['rougeL'], baseline_results['bleu'], \n",
    "        baseline_results['cosine_similarity']\n",
    "    ]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = axes[0, 0].bar(x - width/2, baseline_scores, width, \n",
    "                          label='🤖 베이스라인', color='lightblue', alpha=0.8)\n",
    "    \n",
    "    if finetuned_results:\n",
    "        finetuned_scores = [\n",
    "            finetuned_results['rouge1'], finetuned_results['rouge2'],\n",
    "            finetuned_results['rougeL'], finetuned_results['bleu'],\n",
    "            finetuned_results['cosine_similarity']\n",
    "        ]\n",
    "        bars2 = axes[0, 0].bar(x + width/2, finetuned_scores, width,\n",
    "                              label='🎯 파인튜닝', color='lightcoral', alpha=0.8)\n",
    "        \n",
    "        # 개선된 메트릭에 별표 표시\n",
    "        for i, (baseline, finetuned) in enumerate(zip(baseline_scores, finetuned_scores)):\n",
    "            if finetuned > baseline:\n",
    "                axes[0, 0].text(i, max(baseline, finetuned) + 0.01, '⭐', \n",
    "                               ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    axes[0, 0].set_xlabel('평가 메트릭')\n",
    "    axes[0, 0].set_ylabel('점수')\n",
    "    axes[0, 0].set_title('📊 메트릭별 성능 비교\\n(⭐: 파인튜닝이 더 우수)')\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(metrics, rotation=45, ha='right')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. 방사형 차트 (우상단) - 종합적인 성능 프로필 비교\n",
    "    # 📊 의미: 다각형 모양으로 모델의 전체적인 강약점을 한눈에 파악\n",
    "    # - 면적이 클수록 전반적인 성능이 우수\n",
    "    # - 각 꼭짓점은 특정 성능 지표를 나타냄\n",
    "    if finetuned_results:\n",
    "        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
    "        angles += angles[:1]  # 닫힌 다각형을 위해\n",
    "        \n",
    "        baseline_scores_radar = baseline_scores + baseline_scores[:1]\n",
    "        finetuned_scores_radar = finetuned_scores + finetuned_scores[:1]\n",
    "        \n",
    "        ax_radar = plt.subplot(2, 2, 2, projection='polar')\n",
    "        ax_radar.plot(angles, baseline_scores_radar, 'o-', linewidth=2, \n",
    "                     label='🤖 베이스라인', color='blue', alpha=0.7)\n",
    "        ax_radar.fill(angles, baseline_scores_radar, alpha=0.25, color='blue')\n",
    "        \n",
    "        ax_radar.plot(angles, finetuned_scores_radar, 'o-', linewidth=2,\n",
    "                     label='🎯 파인튜닝', color='red', alpha=0.7)\n",
    "        ax_radar.fill(angles, finetuned_scores_radar, alpha=0.25, color='red')\n",
    "        \n",
    "        ax_radar.set_xticks(angles[:-1])\n",
    "        ax_radar.set_xticklabels(metrics)\n",
    "        ax_radar.set_ylim(0, 1)\n",
    "        ax_radar.set_title('🎯 종합 성능 프로필 비교\\n(면적이 클수록 우수)')\n",
    "        ax_radar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "        ax_radar.grid(True)\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, '⚠️ 파인튜닝 모델이 없어\\n방사형 차트를 생성할 수 없습니다.\\n\\n💡 03번 노트북을 먼저 완료하세요.',\n",
    "                       ha='center', va='center', fontsize=12, \n",
    "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\"))\n",
    "        axes[0, 1].set_title('🎯 종합 성능 프로필 비교')\n",
    "        axes[0, 1].axis('off')\n",
    "    \n",
    "    # 3. 개선율 차트 (좌하단)\n",
    "    # 📊 의미: 파인튜닝으로 인한 성능 변화를 백분율로 표시\n",
    "    # - 양수: 성능 향상, 음수: 성능 저하\n",
    "    # - 막대의 색깔로 개선/저하를 직관적으로 표시\n",
    "    if finetuned_results:\n",
    "        improvements = []\n",
    "        for baseline, finetuned in zip(baseline_scores, finetuned_scores):\n",
    "            if baseline > 0:\n",
    "                improvement = ((finetuned - baseline) / baseline) * 100\n",
    "                improvements.append(improvement)\n",
    "            else:\n",
    "                improvements.append(0)\n",
    "        \n",
    "        colors = ['green' if imp > 0 else 'red' if imp < 0 else 'gray' for imp in improvements]\n",
    "        bars = axes[1, 0].bar(metrics, improvements, color=colors, alpha=0.7)\n",
    "        \n",
    "        # 개선율 수치 표시\n",
    "        for bar, improvement in zip(bars, improvements):\n",
    "            height = bar.get_height()\n",
    "            axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + (1 if height > 0 else -3),\n",
    "                           f'{improvement:+.1f}%', ha='center', va='bottom' if height > 0 else 'top',\n",
    "                           fontweight='bold')\n",
    "        \n",
    "        axes[1, 0].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        axes[1, 0].set_xlabel('평가 메트릭')\n",
    "        axes[1, 0].set_ylabel('개선율 (%)')\n",
    "        axes[1, 0].set_title('📈 파인튜닝 개선율\\n(초록: 개선, 빨강: 저하)')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 평균 개선율 표시\n",
    "        avg_improvement = np.mean(improvements)\n",
    "        axes[1, 0].text(0.02, 0.98, f'평균 개선율: {avg_improvement:+.1f}%', \n",
    "                       transform=axes[1, 0].transAxes, fontsize=12, fontweight='bold',\n",
    "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\" if avg_improvement > 0 else \"lightcoral\"))\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, '⚠️ 파인튜닝 모델이 없어\\n개선율을 계산할 수 없습니다.\\n\\n💡 03번 노트북을 먼저 완료하세요.',\n",
    "                       ha='center', va='center', fontsize=12,\n",
    "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\"))\n",
    "        axes[1, 0].set_title('📈 파인튜닝 개선율')\n",
    "        axes[1, 0].axis('off')\n",
    "    \n",
    "    # 4. 응답 시간 및 길이 비교 (우하단)  \n",
    "    # 📊 의미: 모델의 실용성 측면 비교\n",
    "    # - 응답 시간: 실제 서비스에서의 사용성 (짧을수록 좋음)\n",
    "    # - 응답 길이: 답변의 상세함 정도\n",
    "    response_metrics = ['평균 응답 시간(초)', '평균 응답 길이(단어)']\n",
    "    baseline_practical = [baseline_results['avg_generation_time'], baseline_results['avg_prediction_length']]\n",
    "    \n",
    "    if finetuned_results:\n",
    "        finetuned_practical = [finetuned_results['avg_generation_time'], finetuned_results['avg_prediction_length']]\n",
    "        \n",
    "        x = np.arange(len(response_metrics))\n",
    "        bars1 = axes[1, 1].bar(x - width/2, baseline_practical, width,\n",
    "                              label='🤖 베이스라인', color='lightblue', alpha=0.8)\n",
    "        bars2 = axes[1, 1].bar(x + width/2, finetuned_practical, width,\n",
    "                              label='🎯 파인튜닝', color='lightcoral', alpha=0.8)\n",
    "        \n",
    "        # 수치 표시\n",
    "        for bars in [bars1, bars2]:\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                               f'{height:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        axes[1, 1].legend()\n",
    "    else:\n",
    "        bars = axes[1, 1].bar(response_metrics, baseline_practical, \n",
    "                             color='lightblue', alpha=0.8, label='🤖 베이스라인')\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                           f'{height:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "        axes[1, 1].legend()\n",
    "    \n",
    "    axes[1, 1].set_xlabel('실용성 지표')\n",
    "    axes[1, 1].set_ylabel('값')\n",
    "    axes[1, 1].set_title('⏱️ 실용성 비교\\n(응답 속도 및 길이)')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.93)  # 제목 여백 조정\n",
    "    \n",
    "    # 그래프 저장\n",
    "    plt.savefig('model_performance_comparison.png', dpi=300, bbox_inches='tight', \n",
    "                facecolor='white', edgecolor='none')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✅ 성능 비교 시각화 완료\")\n",
    "    print(\"💾 저장된 파일: model_performance_comparison.png\")\n",
    "    \n",
    "    # 시각화 해석 가이드 출력\n",
    "    print(\"\\n📖 시각화 해석 가이드:\")\n",
    "    print(\"  📊 메트릭별 비교: 각 지표에서 어느 모델이 우수한지 직접 비교\")\n",
    "    print(\"  🎯 방사형 차트: 모델의 전체적인 성능 밸런스를 면적으로 비교\")  \n",
    "    print(\"  📈 개선율 차트: 파인튜닝으로 인한 구체적인 개선 정도 (% 단위)\")\n",
    "    print(\"  ⏱️ 실용성 비교: 실제 사용 시 고려해야 할 속도와 응답 품질\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# 성능 비교 시각화 실행\n",
    "if baseline_results:\n",
    "    comparison_fig = create_performance_visualization(baseline_results, finetuned_results)\n",
    "else:\n",
    "    print(\"❌ 베이스라인 결과가 없어 시각화를 생성할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_response_examples(baseline_results, finetuned_results=None, num_examples=3):\n",
    "    \"\"\"\n",
    "    베이스라인과 파인튜닝 모델의 실제 응답 예시를 비교하여 출력\n",
    "    \n",
    "    이 함수는 정량적 메트릭으로는 파악하기 어려운 질적 차이를 보여줍니다:\n",
    "    - 답변의 정확성: 컨텍스트를 얼마나 정확히 활용했는가\n",
    "    - 답변의 완성도: 질문에 대한 충분한 답변을 제공했는가  \n",
    "    - 답변의 일관성: 주어진 정보에 기반한 논리적 답변인가\n",
    "    - 언어 품질: 자연스럽고 읽기 쉬운 한국어인가\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"🔍 베이스라인 vs 파인튜닝 모델 응답 예시 비교\")\n",
    "    print(\"=\" * 100)\n",
    "    print(\"📋 이 비교를 통해 정량적 메트릭으로는 보기 어려운 질적 차이를 확인할 수 있습니다.\")\n",
    "    print(\"   - 답변의 정확성과 완성도\")\n",
    "    print(\"   - 컨텍스트 활용 능력\") \n",
    "    print(\"   - 한국어 답변의 자연스러움\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    if not baseline_results:\n",
    "        print(\"❌ 베이스라인 결과가 없어 예시를 출력할 수 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    # 비교할 예시 개수 결정\n",
    "    max_examples = min(num_examples, len(baseline_results['predictions']))\n",
    "    \n",
    "    for i in range(max_examples):\n",
    "        print(f\"\\n【예시 {i+1}】\")\n",
    "        print(\"─\" * 80)\n",
    "        \n",
    "        # 컨텍스트 및 질문 출력\n",
    "        context = baseline_results['contexts'][i]\n",
    "        question = baseline_results['questions'][i]\n",
    "        reference = baseline_results['references'][i]\n",
    "        \n",
    "        print(f\"🌐 컨텍스트:\")\n",
    "        print(f\"   {context[:200]}{'...' if len(context) > 200 else ''}\")\n",
    "        print(f\"\\n❓ 질문:\")\n",
    "        print(f\"   {question}\")\n",
    "        print(f\"\\n🎯 정답:\")\n",
    "        print(f\"   {reference}\")\n",
    "        \n",
    "        print(f\"\\n💬 모델 응답 비교:\")\n",
    "        print(\"─\" * 50)\n",
    "        \n",
    "        # 베이스라인 응답\n",
    "        baseline_pred = baseline_results['predictions'][i]\n",
    "        print(f\"🤖 베이스라인 모델:\")\n",
    "        print(f\"   {baseline_pred}\")\n",
    "        \n",
    "        # 파인튜닝 응답 (있는 경우)\n",
    "        if finetuned_results and i < len(finetuned_results['predictions']):\n",
    "            finetuned_pred = finetuned_results['predictions'][i]\n",
    "            print(f\"\\n🎯 파인튜닝 모델:\")\n",
    "            print(f\"   {finetuned_pred}\")\n",
    "            \n",
    "            # 간단한 질적 비교 분석\n",
    "            print(f\"\\n📊 간단 분석:\")\n",
    "            \n",
    "            # 길이 비교\n",
    "            baseline_len = len(baseline_pred.split())\n",
    "            finetuned_len = len(finetuned_pred.split())\n",
    "            print(f\"   길이: 베이스라인 {baseline_len}단어 vs 파인튜닝 {finetuned_len}단어\")\n",
    "            \n",
    "            # 정답과의 유사성 (간단한 키워드 매칭)\n",
    "            ref_keywords = set(reference.lower().split())\n",
    "            baseline_keywords = set(baseline_pred.lower().split())\n",
    "            finetuned_keywords = set(finetuned_pred.lower().split())\n",
    "            \n",
    "            baseline_overlap = len(ref_keywords & baseline_keywords) / len(ref_keywords) if ref_keywords else 0\n",
    "            finetuned_overlap = len(ref_keywords & finetuned_keywords) / len(ref_keywords) if ref_keywords else 0\n",
    "            \n",
    "            print(f\"   정답 키워드 매칭: 베이스라인 {baseline_overlap:.1%} vs 파인튜닝 {finetuned_overlap:.1%}\")\n",
    "            \n",
    "            # 어느 모델이 더 나은지 간단한 판단\n",
    "            if finetuned_overlap > baseline_overlap + 0.1:  # 10% 이상 차이\n",
    "                print(f\"   🌟 파인튜닝 모델이 더 정확한 답변을 생성했습니다.\")\n",
    "            elif baseline_overlap > finetuned_overlap + 0.1:\n",
    "                print(f\"   🔄 베이스라인 모델이 더 정확한 답변을 생성했습니다.\")\n",
    "            else:\n",
    "                print(f\"   ➖ 두 모델의 답변 품질이 비슷합니다.\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"\\n⚠️ 파인튜닝 모델 응답이 없습니다.\")\n",
    "        \n",
    "        print(\"\\n\" + \"─\" * 80)\n",
    "        \n",
    "        # 예시 간 구분선\n",
    "        if i < max_examples - 1:\n",
    "            print(\"\\n\")\n",
    "    \n",
    "    # 전체적인 질적 평가 요약\n",
    "    if finetuned_results:\n",
    "        print(f\"\\n📋 전체 응답 품질 비교 요약:\")\n",
    "        print(\"─\" * 50)\n",
    "        \n",
    "        # 전체 응답에 대한 간단한 통계\n",
    "        total_baseline_length = np.mean([len(pred.split()) for pred in baseline_results['predictions']])\n",
    "        total_finetuned_length = np.mean([len(pred.split()) for pred in finetuned_results['predictions']])\n",
    "        \n",
    "        print(f\"평균 응답 길이: 베이스라인 {total_baseline_length:.1f}단어 vs 파인튜닝 {total_finetuned_length:.1f}단어\")\n",
    "        \n",
    "        # 전반적인 품질 평가 (ROUGE-L 기준)\n",
    "        baseline_quality = baseline_results['rougeL']\n",
    "        finetuned_quality = finetuned_results['rougeL']\n",
    "        \n",
    "        quality_improvement = ((finetuned_quality - baseline_quality) / baseline_quality) * 100 if baseline_quality > 0 else 0\n",
    "        \n",
    "        if quality_improvement > 5:\n",
    "            overall_assessment = \"🌟 파인튜닝으로 답변 품질이 크게 향상되었습니다!\"\n",
    "        elif quality_improvement > 1:\n",
    "            overall_assessment = \"✅ 파인튜닝으로 답변 품질이 개선되었습니다.\"\n",
    "        elif quality_improvement > -1:\n",
    "            overall_assessment = \"➖ 두 모델의 답변 품질이 비슷합니다.\"\n",
    "        else:\n",
    "            overall_assessment = \"🔄 베이스라인 모델의 답변이 더 나은 경우도 있습니다.\"\n",
    "        \n",
    "        print(f\"\\n전반적 평가: {overall_assessment}\")\n",
    "        print(f\"품질 개선율: {quality_improvement:+.1f}% (ROUGE-L 기준)\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"\\n⚠️ 파인튜닝 모델이 없어 질적 비교를 수행할 수 없습니다.\")\n",
    "        print(f\"💡 먼저 03_fine_tuning_with_lora.ipynb를 완료하여 파인튜닝된 모델을 생성하세요.\")\n",
    "\n",
    "# 응답 예시 비교 실행\n",
    "if baseline_results:\n",
    "    show_response_examples(baseline_results, finetuned_results, num_examples=3)\n",
    "else:\n",
    "    print(\"❌ 베이스라인 결과가 없어 응답 예시를 출력할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 결과 저장 및 요약\n",
    "def save_evaluation_results(baseline_results, finetuned_results=None):\n",
    "    \"\"\"\n",
    "    평가 결과를 JSON 파일로 저장하고 최종 요약을 생성\n",
    "    \"\"\"\n",
    "    print(\"💾 평가 결과 저장 중...\")\n",
    "    \n",
    "    # 결과 저장용 딕셔너리 생성\n",
    "    evaluation_summary = {\n",
    "        'evaluation_date': pd.Timestamp.now().isoformat(),\n",
    "        'baseline_model': {\n",
    "            'name': baseline_results['model_name'],\n",
    "            'sample_count': baseline_results['sample_count'],\n",
    "            'rouge1': float(baseline_results['rouge1']),\n",
    "            'rouge2': float(baseline_results['rouge2']),\n",
    "            'rougeL': float(baseline_results['rougeL']),\n",
    "            'bleu': float(baseline_results['bleu']),\n",
    "            'cosine_similarity': float(baseline_results['cosine_similarity']),\n",
    "            'avg_generation_time': float(baseline_results['avg_generation_time']),\n",
    "            'avg_prediction_length': float(baseline_results['avg_prediction_length'])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 파인튜닝 결과가 있는 경우 추가\n",
    "    if finetuned_results:\n",
    "        evaluation_summary['finetuned_model'] = {\n",
    "            'name': finetuned_results['model_name'],\n",
    "            'sample_count': finetuned_results['sample_count'],\n",
    "            'rouge1': float(finetuned_results['rouge1']),\n",
    "            'rouge2': float(finetuned_results['rouge2']),\n",
    "            'rougeL': float(finetuned_results['rougeL']),\n",
    "            'bleu': float(finetuned_results['bleu']),\n",
    "            'cosine_similarity': float(finetuned_results['cosine_similarity']),\n",
    "            'avg_generation_time': float(finetuned_results['avg_generation_time']),\n",
    "            'avg_prediction_length': float(finetuned_results['avg_prediction_length'])\n",
    "        }\n",
    "        \n",
    "        # 개선율 계산 및 저장\n",
    "        improvements = calculate_improvement(baseline_results, finetuned_results)\n",
    "        if improvements:\n",
    "            evaluation_summary['improvement_analysis'] = {\n",
    "                'rouge1_improvement': float(improvements['rouge1']),\n",
    "                'rouge2_improvement': float(improvements['rouge2']),\n",
    "                'rougeL_improvement': float(improvements['rougeL']),\n",
    "                'bleu_improvement': float(improvements['bleu']),\n",
    "                'cosine_similarity_improvement': float(improvements['cosine_similarity']),\n",
    "                'average_improvement': float(np.mean(list(improvements.values())))\n",
    "            }\n",
    "    \n",
    "    # JSON 파일로 저장\n",
    "    with open('baseline_vs_finetuned_evaluation.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(evaluation_summary, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"✅ 평가 결과 저장 완료\")\n",
    "    print(\"📁 저장된 파일:\")\n",
    "    print(\"  - baseline_vs_finetuned_evaluation.json: 종합 평가 결과\")\n",
    "    print(\"  - model_performance_comparison.png: 성능 비교 시각화\")\n",
    "    \n",
    "    return evaluation_summary\n",
    "\n",
    "# 평가 결과 저장 실행\n",
    "if baseline_results:\n",
    "    evaluation_summary = save_evaluation_results(baseline_results, finetuned_results)\n",
    "else:\n",
    "    print(\"❌ 저장할 평가 결과가 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 📋 Day 1 실습 최종 요약 및 결론\n",
    "\n",
    "### ✅ 완료된 작업\n",
    "1. **데이터 전처리 (01번)**: RAFT 방법론을 활용한 한국어 RAG 데이터셋 생성\n",
    "2. **데이터 품질 검증 (02번)**: 토큰 분포, 중복성, RAFT 구조 검증\n",
    "3. **QLoRA 파인튜닝 (03번)**: 4-bit 양자화를 활용한 효율적 EXAONE 모델 파인튜닝\n",
    "4. **성능 평가 및 비교 (04번)**: 베이스라인 vs 파인튜닝 모델 정량적/정성적 비교\n",
    "\n",
    "### 🎯 핵심 성과\n",
    "- **RAFT 기반 파인튜닝**: RAG 성능 향상에 특화된 데이터 전처리 및 학습\n",
    "- **메모리 효율적 학습**: QLoRA를 통한 Colab 무료 환경에서의 대규모 모델 파인튜닝\n",
    "- **종합적 평가**: ROUGE, BLEU, 코사인 유사도 등 다각도 성능 측정\n",
    "- **실용적 검증**: 실제 RAG 시나리오에서의 성능 개선 확인\n",
    "\n",
    "### 📊 기대 효과\n",
    "파인튜닝을 통해 다음과 같은 개선을 기대할 수 있습니다:\n",
    "- **정확성 향상**: 주어진 컨텍스트를 더 정확히 활용한 답변 생성\n",
    "- **일관성 개선**: RAFT 훈련을 통한 컨텍스트 기반 추론 능력 강화\n",
    "- **한국어 품질**: 한국어 특화 데이터셋으로 인한 자연스러운 답변 생성\n",
    "\n",
    "### 💡 실무 활용 방안\n",
    "1. **기업 내 RAG 시스템**: 사내 문서 기반 질의응답 시스템 구축\n",
    "2. **고객 서비스 봇**: FAQ 기반 자동 응답 시스템의 정확도 향상\n",
    "3. **교육 플랫폼**: 교재 내용 기반 학습 도우미 개발\n",
    "4. **연구 지원 도구**: 논문/자료 기반 연구 질의응답 시스템\n",
    "\n",
    "### 🔄 추가 개선 방향\n",
    "1. **더 많은 데이터**: 도메인별 특화 데이터셋으로 성능 향상\n",
    "2. **하이퍼파라미터 튜닝**: Learning rate, LoRA rank 등 최적화\n",
    "3. **앙상블 기법**: 여러 파인튜닝 모델의 결합으로 성능 향상\n",
    "4. **지속적 학습**: 새로운 데이터를 통한 모델 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Day 1 실습 4 최종 완료!\n",
    "print(\"🎉 Day 1 실습 4: 베이스라인 vs 파인튜닝 모델 성능 비교 완료!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 최종 결과 요약 출력\n",
    "if baseline_results:\n",
    "    print(\"📊 평가 완료된 모델:\")\n",
    "    print(f\"  ✅ 베이스라인 모델: {baseline_results['model_name']}\")\n",
    "    if finetuned_results:\n",
    "        print(f\"  ✅ 파인튜닝 모델: {finetuned_results['model_name']}\")\n",
    "        print(\"\\n🔥 베이스라인 vs 파인튜닝 완전 비교 성공!\")\n",
    "    else:\n",
    "        print(\"  ⚠️ 파인튜닝 모델: 비교 불가\")\n",
    "        print(\"\\n💡 파인튜닝 모델이 로드되지 않은 이유:\")\n",
    "        print(\"   1. 03번 노트북의 파인튜닝이 완료되지 않았음\")\n",
    "        print(\"   2. 모델 저장 과정에서 오류 발생\")\n",
    "        print(\"   3. 파일 경로나 권한 문제\")\n",
    "        print(\"\\n🔧 해결 방법:\")\n",
    "        print(\"   👉 03_fine_tuning_with_lora.ipynb를 처음부터 끝까지 완료\")\n",
    "        print(\"   👉 특히 23번 셀(모델 저장)에서 '✅ 모델 저장 성공!' 메시지 확인\")\n",
    "        print(\"   👉 이 노트북을 다시 실행하여 완전한 비교 수행\")\n",
    "    \n",
    "    print(f\"\\n📈 평가 샘플 수: {baseline_results['sample_count']}개\")\n",
    "    \n",
    "    # 핵심 메트릭 요약\n",
    "    if finetuned_results:\n",
    "        print(\"\\n🎯 핵심 성능 지표:\")\n",
    "        metrics = ['rouge1', 'rougeL', 'bleu', 'cosine_similarity']\n",
    "        metric_names = ['ROUGE-1', 'ROUGE-L', 'BLEU', '코사인 유사도']\n",
    "        \n",
    "        for metric, name in zip(metrics, metric_names):\n",
    "            baseline_score = baseline_results[metric]\n",
    "            finetuned_score = finetuned_results[metric]\n",
    "            improvement = ((finetuned_score - baseline_score) / baseline_score * 100) if baseline_score > 0 else 0\n",
    "            \n",
    "            if improvement > 1:\n",
    "                status = \"📈\"\n",
    "            elif improvement < -1:\n",
    "                status = \"📉\"\n",
    "            else:\n",
    "                status = \"➖\"\n",
    "            \n",
    "            print(f\"  {name}: {baseline_score:.3f} → {finetuned_score:.3f} ({improvement:+.1f}%) {status}\")\n",
    "        \n",
    "        # 전체 개선율\n",
    "        improvements = calculate_improvement(baseline_results, finetuned_results)\n",
    "        if improvements:\n",
    "            avg_improvement = np.mean(list(improvements.values()))\n",
    "            print(f\"\\n🌟 평균 성능 개선: {avg_improvement:+.1f}%\")\n",
    "            \n",
    "            if avg_improvement > 5:\n",
    "                print(\"🚀 파인튜닝이 상당한 성능 향상을 달성했습니다!\")\n",
    "            elif avg_improvement > 1:\n",
    "                print(\"✅ 파인튜닝으로 성능이 개선되었습니다.\")\n",
    "            else:\n",
    "                print(\"💡 파인튜닝 효과를 더 높이기 위한 추가 최적화를 고려해보세요.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\n📊 베이스라인 모델 성능 (단독 평가):\")\n",
    "        print(f\"  ROUGE-1: {baseline_results['rouge1']:.3f}\")\n",
    "        print(f\"  ROUGE-L: {baseline_results['rougeL']:.3f}\")\n",
    "        print(f\"  BLEU: {baseline_results['bleu']:.3f}\")\n",
    "        print(f\"  코사인 유사도: {baseline_results['cosine_similarity']:.3f}\")\n",
    "        print(f\"\\n📝 베이스라인 결과 해석:\")\n",
    "        print(f\"  - 베이스라인 모델의 RAG 성능을 측정했습니다\")\n",
    "        print(f\"  - 파인튜닝 모델과 비교하면 개선 효과를 확인할 수 있습니다\")\n",
    "        print(f\"  - 현재 결과는 향후 파인튜닝 효과 측정의 기준점이 됩니다\")\n",
    "    \n",
    "    print(f\"\\n📁 생성된 결과 파일:\")\n",
    "    print(f\"  - baseline_vs_finetuned_evaluation.json (평가 결과)\")\n",
    "    print(f\"  - model_performance_comparison.png (시각화)\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ 평가 결과가 없습니다.\")\n",
    "    print(\"💡 베이스라인 모델도 로드되지 않았다면:\")\n",
    "    print(\"   1. GPU 메모리 부족일 수 있습니다 → 런타임 재시작\")\n",
    "    print(\"   2. 라이브러리 설치 문제일 수 있습니다 → 1-2번 셀 재실행\") \n",
    "    print(\"   3. 평가 데이터가 없을 수 있습니다 → 01번 노트북 먼저 실행\")\n",
    "\n",
    "print(f\"\\n🎓 Day 1 실습 시리즈 완료!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"✅ 01번: RAFT 데이터 전처리 및 검증\")\n",
    "print(\"✅ 02번: 데이터 품질 분석 및 시각화\") \n",
    "print(\"✅ 03번: QLoRA 파인튜닝 실행\")\n",
    "print(\"✅ 04번: 베이스라인 vs 파인튜닝 성능 비교\")\n",
    "print(\"\")\n",
    "\n",
    "# 실습 완료 수준 확인\n",
    "if 'finetuned_results' in locals() and finetuned_results:\n",
    "    completion_level = \"🌟 완전 완료\"\n",
    "    print(f\"{completion_level}: 모든 실습이 성공적으로 완료되었습니다!\")\n",
    "    print(\"🏆 베이스라인 vs 파인튜닝 모델의 성능 비교까지 모든 단계를 마쳤습니다!\")\n",
    "elif 'baseline_results' in locals() and baseline_results:\n",
    "    completion_level = \"⚠️ 부분 완료\"\n",
    "    print(f\"{completion_level}: 베이스라인 평가까지는 완료되었습니다.\")\n",
    "    print(\"🎯 완전한 실습 완료를 위해서는:\")\n",
    "    print(\"   1. 03_fine_tuning_with_lora.ipynb 완료\")\n",
    "    print(\"   2. 이 노트북 재실행으로 파인튜닝 모델 비교\")\n",
    "else:\n",
    "    completion_level = \"🔄 재실행 필요\"\n",
    "    print(f\"{completion_level}: 실습 환경에 문제가 있었습니다.\")\n",
    "    print(\"💡 해결책:\")\n",
    "    print(\"   1. 런타임 재시작 후 처음부터 재실행\")\n",
    "    print(\"   2. 01번 → 02번 → 03번 → 04번 순서로 완료\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"🚀 이제 여러분은 다음 능력을 갖추었습니다:\")\n",
    "print(\"   • RAG 성능 향상을 위한 RAFT 데이터 전처리\")\n",
    "print(\"   • QLoRA를 활용한 효율적 대규모 모델 파인튜닝\")\n",
    "print(\"   • 다양한 메트릭을 활용한 종합적 모델 성능 평가\")\n",
    "print(\"   • 베이스라인과 파인튜닝 모델의 정량적/정성적 비교\")\n",
    "print(\"\")\n",
    "print(\"💡 다음 단계 제안:\")\n",
    "if 'finetuned_results' in locals() and finetuned_results:\n",
    "    print(\"   🎉 축하합니다! 모든 실습이 완료되었습니다.\")\n",
    "    print(\"   • 실제 프로덕션 환경에 모델 적용 고려\")\n",
    "    print(\"   • 도메인별 특화 데이터로 추가 파인튜닝 시도\")\n",
    "    print(\"   • 더 큰 모델이나 다른 파인튜닝 기법 실험\")\n",
    "    print(\"   • 허깅페이스 Hub에 모델 업로드 및 공유\")\n",
    "else:\n",
    "    print(\"   🔄 03번 노트북을 완료하여 파인튜닝 모델 생성\")\n",
    "    print(\"   🔁 이 노트북을 다시 실행하여 완전한 성능 비교\")\n",
    "    print(\"   📈 파인튜닝 효과를 정량적으로 측정\")\n",
    "    print(\"   🎯 베이스라인 대비 성능 개선 확인\")\n",
    "\n",
    "print(f\"\\n🌟 RAFT 방법론을 활용한 한국어 RAG 모델 파인튜닝 실습\")\n",
    "print(f\"   완료 수준: {completion_level}\")\n",
    "print(\"🎓 한국어 RAG 시스템 구축 역량을 획득하셨습니다!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
