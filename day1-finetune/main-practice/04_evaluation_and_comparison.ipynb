{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 ì‹¤ìŠµ 4: Baseline vs LoRA ëª¨ë¸ ë¹„êµ\n",
    "\n",
    "## ğŸ¯ ì´ ë…¸íŠ¸ë¶ì˜ ëª©ì \n",
    "\n",
    "03ë²ˆì—ì„œ í•™ìŠµí•œ LoRA ëª¨ë¸ì´ **ì‹¤ì œë¡œ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆëŠ”ì§€** ê²€ì¦í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ë¹„êµ ëŒ€ìƒ\n",
    "\n",
    "- **Baseline**: íŒŒì¸íŠœë‹ ì „ ì›ë³¸ EXAONE-3.5 ëª¨ë¸\n",
    "- **LoRA**: í•œêµ­ì–´ ìš”ì•½ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹í•œ ëª¨ë¸\n",
    "\n",
    "### í‰ê°€ ë°©ë²•\n",
    "\n",
    "1. ê°™ì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë‘ ëª¨ë¸ ëª¨ë‘ ìš”ì•½ ìƒì„±\n",
    "2. ROUGE-1, ì„ë² ë”© ìœ ì‚¬ë„ë¡œ ì •ë‹µê³¼ ë¹„êµ\n",
    "3. í‰ê·  ì ìˆ˜ ë¹„êµ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "\n",
    "Colabì´ë¼ë©´ ì•„ë˜ë¥¼ ì‹¤í–‰í•˜ê³  **ëŸ°íƒ€ì„ ì¬ì‹œì‘**í•˜ì„¸ìš”.\n",
    "\n",
    "```python\n",
    "!pip install -q --upgrade bitsandbytes accelerate sentence-transformers evaluate rouge-score\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q --upgrade bitsandbytes accelerate sentence-transformers evaluate rouge-score\n",
    "\n",
    "# ì„¤ì¹˜ í›„ ëŸ°íƒ€ì„ ì¬ì‹œì‘ í•„ìš”!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ëª¨ë¸ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "BASE_MODEL = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n",
    "LOCAL_ADAPTER = \"./exaone_summary_lora_final\"  # 03ë²ˆì—ì„œ ì €ì¥í•œ ë¡œì»¬ ê²½ë¡œ\n",
    "HF_ADAPTER = \"ryanu/exaone-summary-lora\"  # í—ˆê¹…í˜ì´ìŠ¤ ì—…ë¡œë“œ ê²½ë¡œ\n",
    "\n",
    "print(\"ğŸ“¥ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16,\n",
    ")\n",
    "\n",
    "print(\"ğŸ“¥ Baseline ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "baseline_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quant_config,\n",
    ")\n",
    "\n",
    "print(\"ğŸ“¥ LoRA ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "if os.path.isdir(LOCAL_ADAPTER):\n",
    "    ADAPTER_DIR = LOCAL_ADAPTER\n",
    "    print(f\"   âœ… ë¡œì»¬ ì–´ëŒ‘í„° ì‚¬ìš©: {ADAPTER_DIR}\")\n",
    "else:\n",
    "    ADAPTER_DIR = HF_ADAPTER\n",
    "    print(f\"   âœ… í—ˆê¹…í˜ì´ìŠ¤ ì–´ëŒ‘í„° ì‚¬ìš©: {ADAPTER_DIR}\")\n",
    "\n",
    "lora_model = PeftModel.from_pretrained(baseline_model, ADAPTER_DIR)\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í‰ê°€ ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"ğŸ“¥ í‰ê°€ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "test_dataset = load_dataset(\"daekeun-ml/naver-news-summarization-ko\", split=\"test\")\n",
    "\n",
    "# í† í° ê¸¸ì´ í•„í„°ë§ (03ë²ˆê³¼ ë™ì¼)\n",
    "def is_valid_sample(example):\n",
    "    article = str(example.get(\"document\", \"\"))\n",
    "    summary = str(example.get(\"summary\", \"\"))\n",
    "    estimated_tokens = len(article) / 1.5 + len(summary) / 1.5\n",
    "    return estimated_tokens < 512 * 0.8\n",
    "\n",
    "print(f\"ğŸ“Š ì›ë³¸ ë°ì´í„°: {len(test_dataset):,}ê°œ\")\n",
    "\n",
    "test_dataset = test_dataset.filter(is_valid_sample)\n",
    "\n",
    "print(f\"âœ‚ï¸ í•„í„°ë§ í›„: {len(test_dataset):,}ê°œ\")\n",
    "\n",
    "MAX_EVAL_SAMPLES = 20\n",
    "test_samples = test_dataset.select(range(min(MAX_EVAL_SAMPLES, len(test_dataset))))\n",
    "\n",
    "print(f\"âœ… í‰ê°€ ìƒ˜í”Œ: {len(test_samples)}ê°œ\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì‘ë‹µ ìƒì„± í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_summary(model, tokenizer, article, max_new_tokens=60):\n",
    "    \"\"\"ìš”ì•½ ìƒì„± í•¨ìˆ˜\"\"\"\n",
    "    prompt = article + \"ìš”ì•½:\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # í”„ë¡¬í”„íŠ¸ ì œê±°\n",
    "    if \"ìš”ì•½:\" in generated:\n",
    "        return generated.split(\"ìš”ì•½:\")[-1].strip()\n",
    "    return generated[len(prompt):].strip()\n",
    "\n",
    "print(\"âœ… ìƒì„± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. í‰ê°€ ì§€í‘œ (ROUGE-1, ì„ë² ë”© ìœ ì‚¬ë„)\n",
    "\n",
    "### ğŸ’¡ í‰ê°€ ì§€í‘œ ì„¤ëª…\n",
    "\n",
    "| ì§€í‘œ | ì„¤ëª… | í•´ì„ |\n",
    "|------|------|------|\n",
    "| **ROUGE-1** | ë‹¨ì–´ ë‹¨ìœ„ ê²¹ì¹¨ ë¹„ìœ¨ (0~1) | ë†’ì„ìˆ˜ë¡ ì •ë‹µê³¼ ë¹„ìŠ·í•œ ë‹¨ì–´ ì‚¬ìš© |\n",
    "| **ì„ë² ë”© ìœ ì‚¬ë„** | ì˜ë¯¸ì  ìœ ì‚¬ë„ (cosine, -1~1) | ë†’ì„ìˆ˜ë¡ ì˜ë¯¸ê°€ ë¹„ìŠ·í•¨ |\n",
    "\n",
    "### ì™œ 2ê°œë¥¼ ì‚¬ìš©?\n",
    "\n",
    "- ROUGE-1: ë‹¨ì–´ê°€ ì •í™•íˆ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸\n",
    "- ì„ë² ë”© ìœ ì‚¬ë„: ë‹¤ë¥¸ í‘œí˜„ì´ì–´ë„ ì˜ë¯¸ê°€ ê°™ìœ¼ë©´ ë†’ì€ ì ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "print(\"ğŸ“¥ í‰ê°€ ë„êµ¬ ë¡œë“œ ì¤‘...\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "embed_model = SentenceTransformer(\"BAAI/bge-m3\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def compute_rouge1(reference, candidate):\n",
    "    try:\n",
    "        return rouge.compute(predictions=[candidate], references=[reference])[\"rouge1\"]\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def compute_embedding_similarity(reference, candidate):\n",
    "    try:\n",
    "        if not reference.strip() or not candidate.strip():\n",
    "            return 0.0\n",
    "        embeddings = embed_model.encode([reference, candidate], convert_to_numpy=True, normalize_embeddings=True)\n",
    "        return float(np.dot(embeddings[0], embeddings[1]))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "print(\"âœ… í‰ê°€ ë„êµ¬ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. í‰ê°€ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "print(\"ğŸ” í‰ê°€ ì‹œì‘...\\n\")\n",
    "\n",
    "for idx, item in enumerate(test_samples):\n",
    "    article = item[\"document\"][:500]  # ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ\n",
    "    reference = item[\"summary\"]\n",
    "    \n",
    "    # Baseline ìƒì„±\n",
    "    baseline_output = generate_summary(baseline_model, tokenizer, article)\n",
    "    \n",
    "    # LoRA ìƒì„±\n",
    "    lora_output = generate_summary(lora_model, tokenizer, article)\n",
    "    \n",
    "    # ì ìˆ˜ ê³„ì‚°\n",
    "    baseline_rouge = compute_rouge1(reference, baseline_output)\n",
    "    lora_rouge = compute_rouge1(reference, lora_output)\n",
    "    baseline_emb = compute_embedding_similarity(reference, baseline_output)\n",
    "    lora_emb = compute_embedding_similarity(reference, lora_output)\n",
    "    \n",
    "    results.append({\n",
    "        \"article\": article[:100] + \"...\",\n",
    "        \"reference\": reference,\n",
    "        \"baseline\": baseline_output,\n",
    "        \"lora\": lora_output,\n",
    "        \"baseline_rouge1\": baseline_rouge,\n",
    "        \"lora_rouge1\": lora_rouge,\n",
    "        \"baseline_embedding\": baseline_emb,\n",
    "        \"lora_embedding\": lora_emb,\n",
    "    })\n",
    "    \n",
    "    if (idx + 1) % 5 == 0:\n",
    "        print(f\"   {idx + 1}/{len(test_samples)} ì™„ë£Œ\")\n",
    "\n",
    "results_df = pd.DataFrame(results).round(3)\n",
    "\n",
    "print(\"\\nâœ… í‰ê°€ ì™„ë£Œ!\\n\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. í‰ê·  ì ìˆ˜ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "summary = pd.DataFrame({\n",
    "    \"Metric\": [\"ROUGE-1\", \"Embedding Cosine\"],\n",
    "    \"Baseline\": [\n",
    "        results_df[\"baseline_rouge1\"].mean(),\n",
    "        results_df[\"baseline_embedding\"].mean(),\n",
    "    ],\n",
    "    \"LoRA\": [\n",
    "        results_df[\"lora_rouge1\"].mean(),\n",
    "        results_df[\"lora_embedding\"].mean(),\n",
    "    ],\n",
    "}).round(3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š í‰ê·  ì ìˆ˜ ë¹„êµ\")\n",
    "print(\"=\"*60)\n",
    "display(summary)\n",
    "\n",
    "print(\"\\nê°œì„ ë„:\")\n",
    "for metric, delta in zip(summary[\"Metric\"], summary[\"LoRA\"] - summary[\"Baseline\"]):\n",
    "    arrow = \"ğŸ”¼\" if delta > 0 else (\"ğŸ”½\" if delta < 0 else \"â–\")\n",
    "    print(f\"{arrow} {metric}: {delta:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ê²°ê³¼ í•´ì„\n",
    "\n",
    "### ğŸ“Š ì ìˆ˜ í•´ì„ ê¸°ì¤€\n",
    "\n",
    "| ì§€í‘œ | Baseline | LoRA ê¸°ëŒ€ì¹˜ | ì˜ë¯¸ |\n",
    "|------|----------|-------------|------|\n",
    "| **ROUGE-1** | 0.1~0.3 | +0.05~0.15 | ë‹¨ì–´ ê²¹ì¹¨ ê°œì„  |\n",
    "| **ì„ë² ë”© ìœ ì‚¬ë„** | 0.5~0.7 | +0.05~0.10 | ì˜ë¯¸ì  ìœ ì‚¬ë„ ê°œì„  |\n",
    "\n",
    "### âœ… ì„±ê³µ ê¸°ì¤€\n",
    "\n",
    "- LoRAê°€ Baselineë³´ë‹¤ **ëª¨ë“  ì§€í‘œì—ì„œ ë†’ìœ¼ë©´ ì„±ê³µ**\n",
    "- ROUGE-1ì´ 0.1 ì´ìƒ ì°¨ì´ë‚˜ë©´ **í° ê°œì„ **\n",
    "- ì„ë² ë”© ìœ ì‚¬ë„ê°€ 0.05 ì´ìƒ ì°¨ì´ë‚˜ë©´ **ì˜ë¯¸ì ìœ¼ë¡œ í–¥ìƒ**\n",
    "\n",
    "### âš ï¸ ë§Œì•½ LoRAê°€ ë” ë‚®ë‹¤ë©´?\n",
    "\n",
    "- í•™ìŠµ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŒ (epoch ëŠ˜ë¦¬ê¸°)\n",
    "- LoRA rankê°€ ë„ˆë¬´ ë‚®ìŒ (r=8 â†’ r=16)\n",
    "- í•™ìŠµë¥ ì´ ë„ˆë¬´ ë†’ê±°ë‚˜ ë‚®ìŒ\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "- **05ë²ˆ ë…¸íŠ¸ë¶**: ì—¬ëŸ¬ ëª¨ë¸ë¡œ LoRA ë¹„êµ ì‹¤í—˜"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}