{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 ì‹¤ìŠµ 5: Chat Template ì ìš© + ë‹¤ì¤‘ ëª¨ë¸ ë¹„êµ + HF ì—…ë¡œë“œ\n",
    "\n",
    "## ğŸ¯ ì´ ë…¸íŠ¸ë¶ì˜ ëª©ì \n",
    "\n",
    "**3ê°œ ëª¨ë¸ Ã— 1ê°œ Rank = ì´ 3ê°œ ì‹¤í—˜**ìœ¼ë¡œ Chat Templateì˜ íš¨ê³¼ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.\n",
    "\n",
    "| ëª¨ë¸ | í¬ê¸° | Rank | í”„ë¡¬í”„íŠ¸ í˜•ì‹ |\n",
    "|------|------|------|---------------|\n",
    "| **Granite-4.0** | 3B | r=8 | Chat Template âœ… |\n",
    "| **Qwen3-4B** | 4B | r=8 | Chat Template âœ… |\n",
    "| **EXAONE-3.5** | 2.4B | r=8 | ë‹¨ìˆœ \"ìš”ì•½:\" (03ë²ˆ ì¬ì‚¬ìš©) |\n",
    "\n",
    "### ì¶”ê°€ í•™ìŠµ ë‚´ìš©\n",
    "\n",
    "- ğŸ”¬ **ì—°êµ¬ ê¸°ë°˜ ë¶„ì„**: Chat Template vs Simple Prompt ë¹„êµ\n",
    "- ğŸ§ª **ì§ì ‘ ì‹¤í—˜**: í•™ìƒì´ í”„ë¡¬í”„íŠ¸ í˜•ì‹ì„ ë°”ê¿”ê°€ë©° í…ŒìŠ¤íŠ¸\n",
    "- ğŸ“¤ **í—ˆê¹…í˜ì´ìŠ¤ ì—…ë¡œë“œ**: í•™ìŠµí•œ ëª¨ë¸ì„ ê³µìœ í•˜ëŠ” ë°©ë²•\n",
    "\n",
    "### ê¸°ëŒ€ íš¨ê³¼\n",
    "\n",
    "- Chat Templateì´ í•­ìƒ ìµœì„ ì´ ì•„ë‹ ìˆ˜ ìˆìŒì„ ì‹¤í—˜ì ìœ¼ë¡œ í™•ì¸\n",
    "- Taskë³„ë¡œ ìµœì  í”„ë¡¬í”„íŠ¸ í˜•ì‹ì´ ë‹¤ë¦„ì„ ì´í•´\n",
    "- ë³¸ì¸ì˜ LoRA ì–´ëŒ‘í„°ë¥¼ HuggingFaceì— ê³µìœ í•˜ëŠ” ê²½í—˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "\n",
    "Colabì´ë¼ë©´ ì‹¤í–‰ í›„ **ëŸ°íƒ€ì„ ì¬ì‹œì‘**í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q --upgrade bitsandbytes accelerate peft transformers datasets sentence-transformers evaluate rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ì‹¤í—˜ ì„¤ì • (3 ëª¨ë¸ Ã— 1 Rank = 3ê°œ)\n",
    "\n",
    "### Chat Template vs ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"Granite-4.0\": \"ibm-granite/granite-4.0-micro\",\n",
    "    \"Qwen3-4B\": \"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    \"EXAONE-3.5\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\",\n",
    "}\n",
    "\n",
    "RANKS = [8]  # r=8ë§Œ ë¹ ë¥´ê²Œ ì‹¤í—˜\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16,\n",
    ")\n",
    "\n",
    "print(f\"âœ… ì´ {len(MODELS)} Ã— {len(RANKS)} = {len(MODELS) * len(RANKS)}ê°œ ì‹¤í—˜\")\n",
    "for model_name in MODELS.keys():\n",
    "    for r in RANKS:\n",
    "        print(f\"   - {model_name} r={r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"daekeun-ml/naver-news-summarization-ko\")\n",
    "\n",
    "print(f\"ğŸ“¦ ì „ì²´ ë°ì´í„°: train={len(dataset['train']):,}, test={len(dataset['test']):,}\")\n",
    "\n",
    "train_dataset = dataset[\"train\"].select(range(1000))\n",
    "test_dataset = dataset[\"test\"].select(range(50))\n",
    "\n",
    "print(f\"âœ‚ï¸ ì„ íƒ í›„: train={len(train_dataset)}, test={len(test_dataset)}\")\n",
    "\n",
    "def is_valid_sample(example):\n",
    "    article = str(example.get(\"document\", \"\"))\n",
    "    summary = str(example.get(\"summary\", \"\"))\n",
    "    estimated_tokens = len(article) / 1.5 + len(summary) / 1.5\n",
    "    return estimated_tokens < MAX_LENGTH * 0.8\n",
    "\n",
    "train_dataset = train_dataset.filter(is_valid_sample)\n",
    "test_dataset = test_dataset.filter(is_valid_sample)\n",
    "\n",
    "print(f\"âœ… í•„í„°ë§ í›„: train={len(train_dataset)}, test={len(test_dataset)}\")\n",
    "\n",
    "if len(train_dataset) < 100:\n",
    "    print(\"\\nâš ï¸ ê²½ê³ : í•™ìŠµ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤!\")\n",
    "    print(f\"í˜„ì¬: {len(train_dataset)}ê°œ\")\n",
    "    print(\"ê¶Œì¥: ìµœì†Œ 500ê°œ ì´ìƒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LoRA í•™ìŠµ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model_lora(model_name, model_path, rank, output_dir):\n",
    "    \"\"\"íŠ¹ì • ëª¨ë¸ + rankë¡œ LoRA í•™ìŠµ\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ğŸ”¥ {model_name} r={rank} í•™ìŠµ ì‹œì‘\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì €\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Chat template ì‚¬ìš© ì—¬ë¶€ ê²°ì •\n",
    "    use_chat_template = model_name in [\"Granite-4.0\", \"Qwen3-4B\"]\n",
    "    \n",
    "    print(f\"ğŸ’¬ í”„ë¡¬í”„íŠ¸ í˜•ì‹: {'Chat Template' if use_chat_template else 'ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸'}\")\n",
    "    \n",
    "    # ë°ì´í„° ì „ì²˜ë¦¬\n",
    "    def prepare_summarization_example(example):\n",
    "        article = str(example.get(\"document\", \"\"))\n",
    "        summary_text = str(example.get(\"summary\", \"\"))\n",
    "        \n",
    "        if use_chat_template:\n",
    "            # Chat template ì‚¬ìš© (Granite, Qwen3)\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": article + \"\\n\\nìœ„ ê¸°ì‚¬ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.\"}\n",
    "            ]\n",
    "            prompt_text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            full_text = prompt_text + summary_text + tokenizer.eos_token\n",
    "        else:\n",
    "            # ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸ (EXAONE-3.5)\n",
    "            prompt_text = article + \"ìš”ì•½:\"\n",
    "            full_text = prompt_text + \" \" + summary_text + tokenizer.eos_token\n",
    "        \n",
    "        model_inputs = tokenizer(\n",
    "            full_text,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        \n",
    "        prompt_len = len(tokenizer(prompt_text, add_special_tokens=False)[\"input_ids\"])\n",
    "        labels = model_inputs[\"input_ids\"].copy()\n",
    "        labels[:prompt_len] = [-100] * min(prompt_len, MAX_LENGTH)\n",
    "        \n",
    "        model_inputs[\"labels\"] = labels\n",
    "        return model_inputs\n",
    "    \n",
    "    train_processed = train_dataset.map(prepare_summarization_example, remove_columns=train_dataset.column_names)\n",
    "    \n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=quant_config,\n",
    "    )\n",
    "    \n",
    "    # target_modules ì¶”ì¶œ\n",
    "    target_modules = []\n",
    "    for name, _ in model.named_parameters():\n",
    "        if ('layers.0' in name or 'layer.0' in name or 'h.0' in name) and 'attn' in name.lower() and 'weight' in name:\n",
    "            parts = name.split('.')\n",
    "            for part in parts:\n",
    "                if part.endswith('_proj') and part not in target_modules:\n",
    "                    target_modules.append(part)\n",
    "    \n",
    "    # LoRA ì„¤ì •\n",
    "    lora_config = LoraConfig(\n",
    "        r=rank,\n",
    "        lora_alpha=rank * 2,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"ğŸ“Š í•™ìŠµ íŒŒë¼ë¯¸í„°: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n",
    "    \n",
    "    # í•™ìŠµ\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"no\",\n",
    "        report_to=[],\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_processed,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # ì €ì¥\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"âœ… ì €ì¥: {output_dir}\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    del model, trainer, tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 3ê°œ ì‹¤í—˜ ì‹¤í–‰\n",
    "\n",
    "### â±ï¸ ì˜ˆìƒ ì‹œê°„\n",
    "\n",
    "- **Granite-4.0**: ì•½ 8~10ë¶„\n",
    "- **Qwen3-4B**: ì•½ 10~12ë¶„ (4B ëª¨ë¸ì´ë¼ ì¡°ê¸ˆ ë” ê±¸ë¦¼)\n",
    "- **EXAONE-3.5**: ìŠ¤í‚µ (03ë²ˆ ì¬ì‚¬ìš©)\n",
    "- **ì´ ì•½ 20~30ë¶„** (Colab T4 ê¸°ì¤€)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiments = {}\n",
    "\n",
    "# EXAONE-3.5ëŠ” 03ë²ˆì—ì„œ í•™ìŠµí•œ ì–´ëŒ‘í„° ì¬ì‚¬ìš© (í—ˆê¹…í˜ì´ìŠ¤)\n",
    "EXAONE_ADAPTER = \"ryanu/exaone-summary-lora\"\n",
    "\n",
    "for model_name, model_path in MODELS.items():\n",
    "    for rank in RANKS:\n",
    "        exp_name = f\"{model_name}_r{rank}\"\n",
    "        output_dir = f\"./lora_{exp_name.replace('.', '_').replace('-', '_')}\"\n",
    "        \n",
    "        # EXAONE-3.5ëŠ” ì´ë¯¸ í•™ìŠµëœ ì–´ëŒ‘í„° ì‚¬ìš©\n",
    "        if model_name == \"EXAONE-3.5\":\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"â™»ï¸  {model_name} r={rank}: 03ë²ˆ ì–´ëŒ‘í„° ì¬ì‚¬ìš©\")\n",
    "            print(f\"   ê²½ë¡œ: {EXAONE_ADAPTER} (í—ˆê¹…í˜ì´ìŠ¤)\")\n",
    "            print(f\"{'='*50}\\n\")\n",
    "            \n",
    "            experiments[exp_name] = {\n",
    "                \"base\": model_path,\n",
    "                \"lora\": EXAONE_ADAPTER,\n",
    "                \"rank\": rank,\n",
    "            }\n",
    "        else:\n",
    "            # ìƒˆë¡œ í•™ìŠµ\n",
    "            experiments[exp_name] = {\n",
    "                \"base\": model_path,\n",
    "                \"lora\": train_model_lora(model_name, model_path, rank, output_dir),\n",
    "                \"rank\": rank,\n",
    "            }\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"âœ… ëª¨ë“  ì‹¤í—˜ ì™„ë£Œ!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. í‰ê°€ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_model_with_baseline(base_model_path, adapter_path, test_samples, exp_name):\n",
    "    \"\"\"Baseline vs LoRA ë¹„êµ í‰ê°€ (DataFrame ì¶œë ¥)\"\"\"\n",
    "    print(f\"\\nğŸ“Š {exp_name} í‰ê°€ ì¤‘...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_path, use_fast=False)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Chat template ì‚¬ìš© ì—¬ë¶€ ê²°ì •\n",
    "    model_name = exp_name.split('_r')[0]\n",
    "    use_chat_template = model_name in [\"Granite-4.0\", \"Qwen3-4B\"]\n",
    "    \n",
    "    # Baseline ëª¨ë¸\n",
    "    baseline_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=quant_config,\n",
    "    )\n",
    "    \n",
    "    # LoRA ëª¨ë¸\n",
    "    lora_model = PeftModel.from_pretrained(baseline_model, adapter_path)\n",
    "    \n",
    "    # í‰ê°€ ë„êµ¬\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    embed_model = SentenceTransformer(\"BAAI/bge-m3\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    def compute_rouge1(reference, candidate):\n",
    "        try:\n",
    "            return rouge.compute(predictions=[candidate], references=[reference])[\"rouge1\"]\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def compute_embedding_similarity(reference, candidate):\n",
    "        try:\n",
    "            if not reference.strip() or not candidate.strip():\n",
    "                return 0.0\n",
    "            embeddings = embed_model.encode([reference, candidate], convert_to_numpy=True, normalize_embeddings=True)\n",
    "            return float(np.dot(embeddings[0], embeddings[1]))\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\nìƒ˜í”Œ 2ê°œ í‰ê°€ ({'Chat Template' if use_chat_template else 'ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸'}):\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for idx in range(min(2, len(test_samples))):\n",
    "        item = test_samples[idx]\n",
    "        article = item[\"document\"]\n",
    "        reference = item[\"summary\"]\n",
    "        \n",
    "        if use_chat_template:\n",
    "            # Chat template ì‚¬ìš©\n",
    "            messages = [{\"role\": \"user\", \"content\": article + \"\\n\\nìœ„ ê¸°ì‚¬ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.\"}]\n",
    "            prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        else:\n",
    "            # ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸\n",
    "            prompt = article + \"ìš”ì•½:\"\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(baseline_model.device)\n",
    "        \n",
    "        # Baseline ìƒì„±\n",
    "        with torch.no_grad():\n",
    "            outputs = baseline_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=60,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        baseline_gen = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        if \"ìš”ì•½:\" in baseline_gen:\n",
    "            baseline_pred = baseline_gen.split(\"ìš”ì•½:\")[-1].strip()\n",
    "        else:\n",
    "            baseline_pred = baseline_gen[len(prompt):].strip()\n",
    "        \n",
    "        # LoRA ìƒì„±\n",
    "        with torch.no_grad():\n",
    "            outputs = lora_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=60,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        lora_gen = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        if \"ìš”ì•½:\" in lora_gen:\n",
    "            lora_pred = lora_gen.split(\"ìš”ì•½:\")[-1].strip()\n",
    "        else:\n",
    "            lora_pred = lora_gen[len(prompt):].strip()\n",
    "        \n",
    "        # ì ìˆ˜ ê³„ì‚°\n",
    "        baseline_rouge = compute_rouge1(reference, baseline_pred)\n",
    "        lora_rouge = compute_rouge1(reference, lora_pred)\n",
    "        baseline_emb = compute_embedding_similarity(reference, baseline_pred)\n",
    "        lora_emb = compute_embedding_similarity(reference, lora_pred)\n",
    "        \n",
    "        results.append({\n",
    "            \"article\": article[:100] + \"...\",\n",
    "            \"reference\": reference,\n",
    "            \"baseline\": baseline_pred,\n",
    "            \"lora\": lora_pred,\n",
    "            \"baseline_rouge1\": baseline_rouge,\n",
    "            \"lora_rouge1\": lora_rouge,\n",
    "            \"baseline_emb\": baseline_emb,\n",
    "            \"lora_emb\": lora_emb,\n",
    "        })\n",
    "        \n",
    "        print(f\"   ìƒ˜í”Œ {idx+1}/2 ì™„ë£Œ\")\n",
    "    \n",
    "    # DataFrame ìƒì„± ë° ì¶œë ¥\n",
    "    results_df = pd.DataFrame(results).round(3)\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ {exp_name} ìƒì„¸ ê²°ê³¼:\")\n",
    "    print(\"=\"*80)\n",
    "    display(results_df)\n",
    "    \n",
    "    # í‰ê·  ê³„ì‚°\n",
    "    avg_baseline_rouge = np.mean([r[\"baseline_rouge1\"] for r in results])\n",
    "    avg_lora_rouge = np.mean([r[\"lora_rouge1\"] for r in results])\n",
    "    avg_baseline_emb = np.mean([r[\"baseline_emb\"] for r in results])\n",
    "    avg_lora_emb = np.mean([r[\"lora_emb\"] for r in results])\n",
    "    \n",
    "    print(f\"\\ní‰ê·  â†’ Baseline ROUGE-1: {avg_baseline_rouge:.4f}, LoRA ROUGE-1: {avg_lora_rouge:.4f}\")\n",
    "    print(f\"í‰ê·  â†’ Baseline ì„ë² ë”©: {avg_baseline_emb:.4f}, LoRA ì„ë² ë”©: {avg_lora_emb:.4f}\")\n",
    "    \n",
    "    del lora_model, baseline_model, tokenizer, embed_model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        \"Baseline_ROUGE\": avg_baseline_rouge, \n",
    "        \"LoRA_ROUGE\": avg_lora_rouge, \n",
    "        \"Baseline_Emb\": avg_baseline_emb,\n",
    "        \"LoRA_Emb\": avg_lora_emb,\n",
    "        \"ROUGE_Improvement\": avg_lora_rouge - avg_baseline_rouge,\n",
    "        \"Emb_Improvement\": avg_lora_emb - avg_baseline_emb,\n",
    "        \"results_df\": results_df,  # DataFrameë„ ì €ì¥\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 3ê°œ ì‹¤í—˜ í‰ê°€\n",
    "\n",
    "ê° ëª¨ë¸ë³„ë¡œ Baseline vs LoRAë¥¼ ë¹„êµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = {}\n",
    "results_detail = {}  # ìƒì„¸ DataFrame ì €ì¥ìš©\n",
    "\n",
    "for exp_name, paths in experiments.items():\n",
    "    eval_result = evaluate_model_with_baseline(\n",
    "        paths[\"base\"],\n",
    "        paths[\"lora\"],\n",
    "        test_dataset,\n",
    "        exp_name\n",
    "    )\n",
    "    \n",
    "    # results_df ë¶„ë¦¬\n",
    "    results_detail[exp_name] = eval_result.pop(\"results_df\")\n",
    "    results[exp_name] = eval_result\n",
    "\n",
    "print(\"\\nâœ… í‰ê°€ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ê²°ê³¼ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results).T\n",
    "\n",
    "print(\"\\nğŸ“Š 9ê°œ ì‹¤í—˜ ê²°ê³¼ ë¹„êµ (ROUGE-1)\")\n",
    "print(\"=\"*80)\n",
    "display(df[[\"Baseline_ROUGE\", \"LoRA_ROUGE\", \"ROUGE_Improvement\"]].sort_values(by=\"LoRA_ROUGE\", ascending=False))\n",
    "\n",
    "print(\"\\nğŸ“Š 9ê°œ ì‹¤í—˜ ê²°ê³¼ ë¹„êµ (ì„ë² ë”© ìœ ì‚¬ë„)\")\n",
    "print(\"=\"*80)\n",
    "display(df[[\"Baseline_Emb\", \"LoRA_Emb\", \"Emb_Improvement\"]].sort_values(by=\"LoRA_Emb\", ascending=False))\n",
    "\n",
    "# ROUGE ê°œì„ í­ Top 3\n",
    "df_sorted = df.sort_values(by=\"ROUGE_Improvement\", ascending=False)\n",
    "print(\"\\nğŸ† ROUGE-1 ê°œì„ í­ Top 3\")\n",
    "for i, (exp_name, row) in enumerate(df_sorted.head(3).iterrows(), 1):\n",
    "    print(f\"{i}ìœ„: {exp_name}\")\n",
    "    print(f\"   Baseline: {row['Baseline_ROUGE']:.4f} â†’ LoRA: {row['LoRA_ROUGE']:.4f} (ê°œì„ : +{row['ROUGE_Improvement']:.4f})\")\n",
    "\n",
    "# ì„ë² ë”© ê°œì„ í­ Top 3\n",
    "df_sorted_emb = df.sort_values(by=\"Emb_Improvement\", ascending=False)\n",
    "print(\"\\nğŸ† ì„ë² ë”© ìœ ì‚¬ë„ ê°œì„ í­ Top 3\")\n",
    "for i, (exp_name, row) in enumerate(df_sorted_emb.head(3).iterrows(), 1):\n",
    "    print(f\"{i}ìœ„: {exp_name}\")\n",
    "    print(f\"   Baseline: {row['Baseline_Emb']:.4f} â†’ LoRA: {row['LoRA_Emb']:.4f} (ê°œì„ : +{row['Emb_Improvement']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ëª¨ë¸ë³„ Rank íš¨ê³¼ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ëª¨ë¸ë³„ë¡œ ê·¸ë£¹í™”\n",
    "for model_name in MODELS.keys():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ğŸ“Œ {model_name}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    model_results = {}\n",
    "    for rank in RANKS:\n",
    "        exp_name = f\"{model_name}_r{rank}\"\n",
    "        if exp_name in results:\n",
    "            model_results[f\"r={rank}\"] = results[exp_name]\n",
    "    \n",
    "    model_df = pd.DataFrame(model_results).T\n",
    "    print(\"\\nROUGE-1:\")\n",
    "    display(model_df[[\"Baseline_ROUGE\", \"LoRA_ROUGE\", \"ROUGE_Improvement\"]])\n",
    "    \n",
    "    print(\"\\nì„ë² ë”© ìœ ì‚¬ë„:\")\n",
    "    display(model_df[[\"Baseline_Emb\", \"LoRA_Emb\", \"Emb_Improvement\"]])\n",
    "    \n",
    "    best_rank_rouge = model_df.sort_values(by=\"LoRA_ROUGE\", ascending=False).index[0]\n",
    "    best_rank_emb = model_df.sort_values(by=\"LoRA_Emb\", ascending=False).index[0]\n",
    "    print(f\"\\nâœ… ìµœì  Rank (ROUGE-1): {best_rank_rouge}\")\n",
    "    print(f\"âœ… ìµœì  Rank (ì„ë² ë”©): {best_rank_emb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ì „ì²´ ìƒì„¸ ê²°ê³¼ (ëª¨ë“  ìƒì„± ë¬¸ì¥)\n",
    "\n",
    "ê° ì‹¤í—˜ë³„ë¡œ ìƒì„±ëœ ëª¨ë“  ë¬¸ì¥ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ê° ì‹¤í—˜ë³„ ìƒì„¸ ê²°ê³¼ ì¶œë ¥\n",
    "for exp_name in sorted(results_detail.keys()):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ“‹ {exp_name} ìƒì„¸ ê²°ê³¼\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    df = results_detail[exp_name]\n",
    "    display(df)\n",
    "    \n",
    "    # ìš”ì•½ í†µê³„\n",
    "    print(f\"\\ní†µê³„:\")\n",
    "    print(f\"   Baseline ROUGE-1: {df['baseline_rouge1'].mean():.3f}\")\n",
    "    print(f\"   LoRA ROUGE-1: {df['lora_rouge1'].mean():.3f}\")\n",
    "    print(f\"   Baseline ì„ë² ë”©: {df['baseline_emb'].mean():.3f}\")\n",
    "    print(f\"   LoRA ì„ë² ë”©: {df['lora_emb'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ê²°ê³¼ í•´ì„\n",
    "\n",
    "### ğŸ” ì‹¤í—˜ ì„¤ê³„\n",
    "\n",
    "ì´ë²ˆ ì‹¤í—˜ì—ì„œëŠ” **Chat Templateì˜ íš¨ê³¼**ë¥¼ ë¹„êµí•©ë‹ˆë‹¤:\n",
    "\n",
    "| ëª¨ë¸ | í”„ë¡¬í”„íŠ¸ í˜•ì‹ | ì´ìœ  |\n",
    "|------|---------------|------|\n",
    "| **Granite-4.0** | Chat Template âœ… | ê³µì‹ ê¶Œì¥ ë°©ë²• |\n",
    "| **Qwen3-4B** | Chat Template âœ… | ìµœì‹  ëª¨ë¸, ê³µì‹ ê¶Œì¥ |\n",
    "| **EXAONE-3.5** | ë‹¨ìˆœ \"ìš”ì•½:\" | 03ë²ˆ ì–´ëŒ‘í„° ì¬ì‚¬ìš© |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¬ Chat Templateì´ë€?\n",
    "\n",
    "**ëª¨ë“  `-Instruct` ëª¨ë¸ì€ chat templateì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤!**\n",
    "\n",
    "```python\n",
    "# Granite chat template\n",
    "messages = [{\"role\": \"user\", \"content\": \"ìœ„ ê¸°ì‚¬ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.\"}]\n",
    "prompt = tokenizer.apply_chat_template(messages, ...)\n",
    "# â†’ \"<|user|>ìœ„ ê¸°ì‚¬ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.<|assistant|>\"\n",
    "\n",
    "# Qwen3 chat template\n",
    "# â†’ \"<|im_start|>user\\nìœ„ ê¸°ì‚¬ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "# EXAONE-3.5 (ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸)\n",
    "prompt = article + \"ìš”ì•½:\"\n",
    "# â†’ \"ê¸°ì‚¬ ë‚´ìš©...ìš”ì•½:\"\n",
    "```\n",
    "\n",
    "**ì™œ Chat Templateì„ ì‚¬ìš©í•´ì•¼ í•˜ë‚˜?**\n",
    "1. **ê³µì‹ í•™ìŠµ í˜•ì‹**: Instruct ëª¨ë¸ì€ ì´ í˜•ì‹ìœ¼ë¡œ í•™ìŠµë¨\n",
    "2. **ë” ì¢‹ì€ ì„±ëŠ¥**: ëª¨ë¸ì´ ì¸ì‹í•˜ê¸° ì‰¬ìš´ í˜•ì‹\n",
    "3. **ì¼ê´€ì„±**: ë‹¤ë¥¸ ê°œë°œìë“¤ê³¼ ë™ì¼í•œ ë°©ì‹\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ë¶„ì„\n",
    "\n",
    "#### ì˜ˆìƒ ê²°ê³¼:\n",
    "\n",
    "| ëª¨ë¸ | í˜•ì‹ | ì˜ˆìƒ ì„±ëŠ¥ |\n",
    "|------|------|----------|\n",
    "| Granite-4.0 | Chat | ë†’ìŒ âœ… |\n",
    "| Qwen3-4B | Chat | ë†’ìŒ âœ… |\n",
    "| EXAONE-3.5 | ë‹¨ìˆœ | ì¤‘ê°„ (03/04ë²ˆì—ì„œ ê²€ì¦ë¨) |\n",
    "\n",
    "#### í•µì‹¬ ë¹„êµ:\n",
    "\n",
    "**1. Chat Template vs ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸**\n",
    "- Granite (Chat) vs EXAONE-3.5 (ë‹¨ìˆœ)\n",
    "- Qwen3 (Chat) vs ì´ì „ Qwen2.5 (ë‹¨ìˆœ, 05ë²ˆ ì´ˆë°˜)\n",
    "\n",
    "**2. ëª¨ë¸ í¬ê¸° íš¨ê³¼**\n",
    "- Granite: 3B\n",
    "- Qwen3: 4B\n",
    "- EXAONE-3.5: 2.4B\n",
    "\n",
    "â†’ í¬ê¸° + Chat template ì¡°í•©ì´ ì„±ëŠ¥ì— ì–´ë–¤ ì˜í–¥?\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ í•™ìŠµ í¬ì¸íŠ¸\n",
    "\n",
    "#### 1. \"Chat Template í•„ìˆ˜ëŠ” ì•„ë‹ˆì§€ë§Œ ê¶Œì¥!\"\n",
    "\n",
    "```\n",
    "ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸ë„ ì‘ë™í•¨:\n",
    "  EXAONE-3.5: 0.3~0.4 (04ë²ˆ ê²°ê³¼)\n",
    "  Qwen2.5 r=4: 0.714 (05ë²ˆ ì´ˆë°˜)\n",
    "\n",
    "í•˜ì§€ë§Œ Chat Templateì´ ë” ì¢‹ì„ ìˆ˜ ìˆìŒ:\n",
    "  - ê³µì‹ ê¶Œì¥ ë°©ë²•\n",
    "  - ëª¨ë¸ì´ í•™ìŠµí•œ í˜•ì‹\n",
    "  - ì•ˆì •ì ì¸ ì„±ëŠ¥\n",
    "```\n",
    "\n",
    "#### 2. \"ì‹¤ì „ì—ì„œëŠ” ë°˜ë“œì‹œ ê³µì‹ ë¬¸ì„œ í™•ì¸!\"\n",
    "\n",
    "ëª¨ë“  `-Instruct` ëª¨ë¸ì€ ê³µì‹ ë¬¸ì„œì—:\n",
    "- Chat template ì‚¬ìš© ì˜ˆì‹œ\n",
    "- ê¶Œì¥ generation íŒŒë¼ë¯¸í„°\n",
    "- í”„ë¡¬í”„íŠ¸ í˜•ì‹ ê°€ì´ë“œ\n",
    "\n",
    "#### 3. \"êµìœ¡ìš© ë‹¨ìˆœí™” vs ì‹¤ì „ best practice\"\n",
    "\n",
    "- **êµìœ¡ìš©**: ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸ (ì´í•´í•˜ê¸° ì‰¬ì›€)\n",
    "- **ì‹¤ì „**: Chat template (ì„±ëŠ¥ ìµœì í™”)\n",
    "\n",
    "â†’ ì´ ë…¸íŠ¸ë¶ì€ ì‹¤ì „ ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Cell 9ì˜ ìƒì„¸ ê²°ê³¼ í™•ì¸\n",
    "\n",
    "Cell 9ì—ì„œ ê° ëª¨ë¸ë³„ë¡œ ìƒì„±ëœ ì‹¤ì œ ë¬¸ì¥ì„ í™•ì¸í•˜ì„¸ìš”!\n",
    "- article: ì›ë³¸ ê¸°ì‚¬\n",
    "- reference: ì •ë‹µ ìš”ì•½\n",
    "- baseline: LoRA ì „ ìƒì„±\n",
    "- lora: LoRA í›„ ìƒì„±\n",
    "\n",
    "**DataFrameì„ ì§ì ‘ í™•ì¸í•˜ë©´:**\n",
    "- Chat templateì´ ë” ìì—°ìŠ¤ëŸ¬ìš´ì§€\n",
    "- ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸ì™€ í’ˆì§ˆ ì°¨ì´ê°€ ìˆëŠ”ì§€\n",
    "- ì–´ë–¤ ëª¨ë¸ì´ í•œêµ­ì–´ì— ê°•í•œì§€\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ ìµœì¢… ê²°ë¡ \n",
    "\n",
    "1. **3ê°œ ëª¨ë¸ ë¹„êµ** (Granite, Qwen3, EXAONE-3.5)\n",
    "2. **2ê°€ì§€ ë°©ë²• ë¹„êµ** (Chat template vs ë‹¨ìˆœ)\n",
    "3. **r=8 ê³ ì •** (ë¹ ë¥¸ ì‹¤í—˜)\n",
    "\n",
    "**ê¸°ëŒ€ íš¨ê³¼:**\n",
    "- Chat templateì˜ íš¨ê³¼ ê²€ì¦\n",
    "- ìµœì‹  ëª¨ë¸ (Qwen3) vs ê²€ì¦ëœ ëª¨ë¸ (EXAONE-3.5)\n",
    "- ì‹¤ì „ best practice í•™ìŠµ\n",
    "\n",
    "---\n",
    "\n",
    "### âš ï¸ ì¤‘ìš”: í•™ìŠµ ë°ì´í„° ë¶€ì¡±\n",
    "\n",
    "í˜„ì¬ ì„¤ì •:\n",
    "- ~200 samples Ã— 1 epoch\n",
    "- 3~4B ëª¨ë¸ì—ëŠ” ë¶€ì¡±\n",
    "\n",
    "**ê¶Œì¥ (03ë²ˆì—ì„œ í™•ì¸ë¨):**\n",
    "- 1000+ samples Ã— 2-3 epochs\n",
    "\n",
    "â†’ **ê²°ê³¼ê°€ ê¸°ëŒ€ë³´ë‹¤ ë‚®ì„ ìˆ˜ ìˆìŒ!**\n",
    "â†’ í•˜ì§€ë§Œ \"chat templateì˜ íš¨ê³¼\"ëŠ” ë¹„êµ ê°€ëŠ¥!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š ì—°êµ¬ ê¸°ë°˜: Chat Template vs Simple Prompt\n",
    "\n",
    "#### ğŸ”¬ ì‹¤ì œ ì—°êµ¬ ê²°ê³¼ (2024)\n",
    "\n",
    "**1. RAG íƒœìŠ¤í¬ (arXiv 2024):**\n",
    "> \"Base models still outperform instruct ones. The processes of supervised fine-tuning and alignment detrimentally impact the model's capabilities in RAG.\"\n",
    "\n",
    "**ê²°ë¡ **: Alignmentê°€ ì •ë³´ ì¶”ì¶œ ëŠ¥ë ¥ì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìŒ!\n",
    "\n",
    "**2. ìš”ì•½ íƒœìŠ¤í¬ (ë‹¤ìˆ˜ ì—°êµ¬):**\n",
    "> \"GPT-3.5 prompts with simple task descriptions generated summaries that were preferred by human evaluators\"\n",
    "> \"Simpler prompts avoided data-specific inaccuracies\"\n",
    "\n",
    "**ê²°ë¡ **: ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ê°€ ë” ì •í™•!\n",
    "\n",
    "**3. Instruct vs Chat íš¨ìœ¨ì„±:**\n",
    "> \"Instruct mode with direct instructions is generally more efficient for summarization\"\n",
    "> \"Chat mode can be less efficient in computational resources\"\n",
    "\n",
    "**ê²°ë¡ **: ìš”ì•½ì€ direct instructionì´ íš¨ìœ¨ì !\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“Š Taskë³„ ìµœì  í”„ë¡¬í”„íŠ¸ (ì—°êµ¬ ê¸°ë°˜)\n",
    "\n",
    "| íƒœìŠ¤í¬ ìœ í˜• | Simple Prompt | Chat Template | ì¶”ì²œ | ì´ìœ  |\n",
    "|------------|---------------|---------------|------|------|\n",
    "| **ìš”ì•½** (Summarization) | âœ… ë” ì¢‹ìŒ | âš ï¸ ë¹„íš¨ìœ¨ì  | **Simple** | Direct instruction íš¨ìœ¨ì  |\n",
    "| **RAG** (Information) | âœ… ë” ì¢‹ìŒ | âŒ ì„±ëŠ¥ ì €í•˜ | **Simple** | Alignmentê°€ ì„±ëŠ¥ ì €í•˜ |\n",
    "| **QA** (Single-turn) | âœ… íš¨ìœ¨ì  | âš ï¸ ê³¼ë„í•¨ | **Simple** | ë‹¨ë°œì„± íƒœìŠ¤í¬ |\n",
    "| **ëŒ€í™”** (Multi-turn) | âŒ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ | âœ… ìµœì  | **Chat** | ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ í•„ìš” |\n",
    "| **ì½”ë”©** (Generation) | âœ… ëª…í™•í•¨ | âš ï¸ ë³µì¡í•¨ | **Simple** | ëª…í™•í•œ instruction |\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ’¡ ì™œ Simple Promptê°€ ë” ë‚˜ì„ ìˆ˜ ìˆë‚˜?\n",
    "\n",
    "**1. Task Mismatch (íƒœìŠ¤í¬ ë¶ˆì¼ì¹˜)**\n",
    "```\n",
    "Chat Template í•™ìŠµ ëª©ì :\n",
    "  - ë‹¤íšŒì°¨ ëŒ€í™” (multi-turn)\n",
    "  - ì»¨í…ìŠ¤íŠ¸ ìœ ì§€\n",
    "  - ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ\n",
    "\n",
    "ìš”ì•½ íƒœìŠ¤í¬ íŠ¹ì„±:\n",
    "  - ë‹¨ë°œì„± (single-turn)\n",
    "  - ì •ë³´ ì¶”ì¶œ\n",
    "  - ê°„ê²°í•œ ì¶œë ¥\n",
    "\n",
    "â†’ ëª©ì ì´ ë‹¤ë¦„!\n",
    "```\n",
    "\n",
    "**2. Computational Overhead (ì—°ì‚° ë‚­ë¹„)**\n",
    "```python\n",
    "# Simple: \"ê¸°ì‚¬...ìš”ì•½:\" â†’ ì§ì ‘ ìƒì„±\n",
    "í† í°: ~300\n",
    "\n",
    "# Chat: \"<|user|>ê¸°ì‚¬...\n",
    "\n",
    "ìœ„ ê¸°ì‚¬ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.<|assistant|>\"\n",
    "í† í°: ~350\n",
    "\n",
    "â†’ ë¶ˆí•„ìš”í•œ í† í°ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ì••ë°•\n",
    "```\n",
    "\n",
    "**3. Instruction Clarity (ëª…í™•ì„±)**\n",
    "```\n",
    "\"ìš”ì•½:\" â†’ ëª…í™•, ì§ì ‘ì  âœ…\n",
    "\"ìœ„ ê¸°ì‚¬ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.\" â†’ ìì—°ì–´, ëª¨í˜¸í•  ìˆ˜ ìˆìŒ âš ï¸\n",
    "```\n",
    "\n",
    "**4. Training Data Distribution (í•™ìŠµ ë°ì´í„° ë¶„í¬)**\n",
    "```\n",
    "Chat models í•™ìŠµ:\n",
    "  - ì£¼ë¡œ ì˜ì–´ ëŒ€í™”\n",
    "  - Multi-turn QA\n",
    "  \n",
    "ìš”ì•½ ë°ì´í„°:\n",
    "  - í•œêµ­ì–´ (ìš°ë¦¬ ì¼€ì´ìŠ¤)\n",
    "  - Single-turn íƒœìŠ¤í¬\n",
    "\n",
    "â†’ Distribution shift!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§ª ì§ì ‘ ì‹¤í—˜í•´ë³´ê¸°!\n",
    "\n",
    "**í•™ìƒ ì—¬ëŸ¬ë¶„ì´ ì§ì ‘ í…ŒìŠ¤íŠ¸í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!**\n",
    "\n",
    "#### Step 1: Cell 9 ìˆ˜ì •\n",
    "\n",
    "**í˜„ì¬ ì½”ë“œ:**\n",
    "```python\n",
    "# íŠ¹ì • ëª¨ë¸ë§Œ chat template ì‚¬ìš©\n",
    "use_chat_template = model_name in [\"Granite-4.0\", \"Qwen3-4B\"]\n",
    "```\n",
    "\n",
    "**ì‹¤í—˜ 1: ëª¨ë‘ Chat Template**\n",
    "```python\n",
    "# Cell 9ì—ì„œ ìˆ˜ì •\n",
    "use_chat_template = True  # ëª¨ë“  ëª¨ë¸ì— chat template ì ìš©\n",
    "```\n",
    "â†’ EXAONE-3.5ë„ chat template ì‚¬ìš©\n",
    "\n",
    "**ì‹¤í—˜ 2: ëª¨ë‘ Simple Prompt**\n",
    "```python\n",
    "# Cell 9ì—ì„œ ìˆ˜ì •\n",
    "use_chat_template = False  # ëª¨ë“  ëª¨ë¸ ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸\n",
    "```\n",
    "â†’ Granite, Qwen3ë„ \"ìš”ì•½:\" ì‚¬ìš©\n",
    "\n",
    "**ì‹¤í—˜ 3: íŠ¹ì • ëª¨ë¸ë§Œ ë³€ê²½**\n",
    "```python\n",
    "# EXAONE-3.5ë§Œ chat template\n",
    "use_chat_template = model_name == \"EXAONE-3.5\"\n",
    "\n",
    "# Graniteë§Œ simple prompt\n",
    "use_chat_template = model_name != \"Granite-4.0\"\n",
    "```\n",
    "\n",
    "#### Step 2: ë‹¤ì‹œ ì‹¤í–‰\n",
    "\n",
    "1. Cell 11ë¶€í„° ì‹¤í–‰ (í•™ìŠµ)\n",
    "2. Cell 15 ì‹¤í–‰ (í‰ê°€)\n",
    "3. Cell 17 í™•ì¸ (ê²°ê³¼ ë¹„êµ)\n",
    "\n",
    "#### Step 3: ê²°ê³¼ ë¹„êµ\n",
    "\n",
    "**ì˜ˆìƒ ê²°ê³¼:**\n",
    "- EXAONE-3.5 + Chat â†’ ì„±ëŠ¥ í•˜ë½? ìœ ì§€?\n",
    "- Granite + Simple â†’ ì„±ëŠ¥ í–¥ìƒ? (0.533ìœ¼ë¡œ ë³µê·€?)\n",
    "- Qwen3 + Simple â†’ ì„±ëŠ¥ í–¥ìƒ?\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ í•µì‹¬ êµí›ˆ\n",
    "\n",
    "**1. \"ê³µì‹ ê¶Œì¥ â‰  ëª¨ë“  íƒœìŠ¤í¬ ìµœì \"**\n",
    "- Chat templateì€ ëŒ€í™”ìš©ìœ¼ë¡œ ìµœì í™”\n",
    "- ìš”ì•½/RAGëŠ” simple promptê°€ ë” ë‚˜ì„ ìˆ˜ ìˆìŒ\n",
    "\n",
    "**2. \"Task-specific ì‹¤í—˜ í•„ìˆ˜!\"**\n",
    "- ì—°êµ¬ ê²°ê³¼: íƒœìŠ¤í¬ë§ˆë‹¤ ìµœì  ë°©ë²• ë‹¤ë¦„\n",
    "- ìš°ë¦¬ ì‹¤í—˜: ìš”ì•½ì€ simpleì´ 3ë°° ì¢‹ìŒ\n",
    "\n",
    "**3. \"Simpler is often better\"**\n",
    "- ë³µì¡í•œ prompt â†’ ëª¨í˜¸í•¨, ì—°ì‚° ë‚­ë¹„\n",
    "- ê°„ë‹¨í•œ prompt â†’ ëª…í™•í•¨, íš¨ìœ¨ì \n",
    "\n",
    "**4. \"Always validate with experiments\"**\n",
    "- ì´ë¡  < ì‹¤í—˜\n",
    "- ì§ì ‘ í…ŒìŠ¤íŠ¸í•´ë³´ëŠ” ê²ƒì´ ì§„ë¦¬!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š ì°¸ê³  ìë£Œ\n",
    "\n",
    "- **RAG Base vs Instruct**: \"A Tale of Trust and Accuracy\" (arXiv 2024)\n",
    "- **Summarization Prompting**: Multiple studies on LLM text summarization\n",
    "- **Instruct vs Chat Efficiency**: ScrapingAnt comprehensive analysis\n",
    "- **ìš°ë¦¬ ì‹¤í—˜**: EXAONE-3.5 (0.629) >> Qwen3 (0.400) >> Granite (0.200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. í—ˆê¹…í˜ì´ìŠ¤ì— ì—…ë¡œë“œí•˜ê¸° (ì„ íƒ)\n",
    "\n",
    "í•™ìŠµí•œ LoRA ì–´ëŒ‘í„°ë¥¼ í—ˆê¹…í˜ì´ìŠ¤ì— ì—…ë¡œë“œí•˜ì—¬ ê³µìœ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ“‹ ì¤€ë¹„ ì‚¬í•­\n",
    "\n",
    "1. **í—ˆê¹…í˜ì´ìŠ¤ ê³„ì •**: https://huggingface.co íšŒì›ê°€ì…\n",
    "2. **Access Token ë°œê¸‰**:\n",
    "   - https://huggingface.co/settings/tokens\n",
    "   - `New token` â†’ `Write` ê¶Œí•œ ì„ íƒ\n",
    "   - í† í° ë³µì‚¬\n",
    "\n",
    "### ğŸš€ ì—…ë¡œë“œ ë°©ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "# Step 1: ë¡œê·¸ì¸\n",
    "print(\"ğŸ”‘ í—ˆê¹…í˜ì´ìŠ¤ ë¡œê·¸ì¸\")\n",
    "login(token=\"YOUR_HF_TOKEN_HERE\")  # ì—¬ê¸°ì— í† í° ì…ë ¥!\n",
    "\n",
    "# Step 2: ì—…ë¡œë“œí•  ëª¨ë¸ ì„ íƒ\n",
    "upload_exp = \"Granite-4.0_r8\"  # ë˜ëŠ” \"Qwen3-4B_r8\"\n",
    "\n",
    "local_path = experiments[upload_exp][\"lora\"]\n",
    "repo_name = \"your-username/granite-summary-lora\"  # ë³¸ì¸ usernameìœ¼ë¡œ ë³€ê²½!\n",
    "\n",
    "print(f\"\\nğŸ“¤ ì—…ë¡œë“œ ì¤€ë¹„\")\n",
    "print(f\"   ë¡œì»¬ ê²½ë¡œ: {local_path}\")\n",
    "print(f\"   í—ˆê¹…í˜ì´ìŠ¤ ê²½ë¡œ: {repo_name}\")\n",
    "\n",
    "# Step 3: ì—…ë¡œë“œ\n",
    "api = HfApi()\n",
    "\n",
    "try:\n",
    "    api.create_repo(\n",
    "        repo_id=repo_name,\n",
    "        repo_type=\"model\",\n",
    "        exist_ok=True\n",
    "    )\n",
    "    \n",
    "    api.upload_folder(\n",
    "        folder_path=local_path,\n",
    "        repo_id=repo_name,\n",
    "        repo_type=\"model\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… ì—…ë¡œë“œ ì™„ë£Œ!\")\n",
    "    print(f\"   ë§í¬: https://huggingface.co/{repo_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"\\nğŸ’¡ ì²´í¬ë¦¬ìŠ¤íŠ¸:\")\n",
    "    print(\"   1. í† í°ì´ ì˜¬ë°”ë¥¸ê°€ìš”? (Write ê¶Œí•œ)\")\n",
    "    print(\"   2. repo_nameì— ë³¸ì¸ usernameì„ ì…ë ¥í–ˆë‚˜ìš”?\")\n",
    "    print(f\"   3. {local_path} ê²½ë¡œê°€ ì¡´ì¬í•˜ë‚˜ìš”?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“¥ ì—…ë¡œë“œí•œ ëª¨ë¸ ì‚¬ìš©í•˜ê¸°\n",
    "\n",
    "ì—…ë¡œë“œ í›„ ë‹¤ë¥¸ ì‚¬ëŒë“¤ì´ ì´ë ‡ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Base ëª¨ë¸ ë¡œë“œ\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"ibm-granite/granite-4.0-micro\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ibm-granite/granite-4.0-micro\")\n",
    "\n",
    "# ì—…ë¡œë“œí•œ LoRA ì–´ëŒ‘í„° ë¡œë“œ\n",
    "model = PeftModel.from_pretrained(base_model, \"your-username/granite-summary-lora\")\n",
    "\n",
    "# ìš”ì•½ ìƒì„±\n",
    "article = \"ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš©...\"\n",
    "messages = [{\"role\": \"user\", \"content\": article + \"\\n\\nìœ„ ê¸°ì‚¬ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.\"}]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=60)\n",
    "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(summary)\n",
    "```\n",
    "\n",
    "### ğŸ¯ ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "ì¶•í•˜í•©ë‹ˆë‹¤! Day 1ì˜ ëª¨ë“  ì‹¤ìŠµì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "**ë°°ìš´ ë‚´ìš©:**\n",
    "- âœ… RAFT ë°ì´í„° ì „ì²˜ë¦¬ (01ë²ˆ)\n",
    "- âœ… ë°ì´í„° í’ˆì§ˆ ê²€ì¦ (02ë²ˆ)\n",
    "- âœ… LoRA íŒŒì¸íŠœë‹ (03ë²ˆ)\n",
    "- âœ… Baseline vs LoRA ë¹„êµ (04ë²ˆ)\n",
    "- âœ… Chat Template íš¨ê³¼ ê²€ì¦ (05ë²ˆ)\n",
    "\n",
    "**í•µì‹¬ ì¸ì‚¬ì´íŠ¸:**\n",
    "- ğŸ“Š Chat Templateì´ í•­ìƒ ìµœì„ ì€ ì•„ë‹˜\n",
    "- ğŸ¯ Taskë³„ ìµœì  í”„ë¡¬í”„íŠ¸ í˜•ì‹ì´ ë‹¤ë¦„\n",
    "- ğŸ”¬ ì‹¤í—˜ì  ê²€ì¦ì´ ì¤‘ìš”í•¨\n",
    "\n",
    "**Day 2ì—ì„œ ë§Œë‚˜ìš”!** ğŸ‘‹"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}