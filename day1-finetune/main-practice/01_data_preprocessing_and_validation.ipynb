{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š Day 1 ì‹¤ìŠµ 1: ë°ì´í„° ì „ì²˜ë¦¬ ë° ê²€ì¦\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- í•œêµ­ì–´ ë°ì´í„°ì…‹ì„ í™œìš©í•œ RAG íŒŒì¸íŠœë‹ ë°ì´í„° ì¤€ë¹„\n",
    "- RAFT(Retrieval Augmented Fine Tuning) ë°©ë²•ë¡  ì ìš©\n",
    "- Instruction/Input/Output ìŠ¤í‚¤ë§ˆ ì„¤ê³„\n",
    "- ë°ì´í„° ì „ì²˜ë¦¬ ê·œì¹™ ì ìš© (ê³µë°±/ì œì–´ë¬¸ì/ì¤‘ë³µ/ê¸¸ì´/PII ì²˜ë¦¬)\n",
    "- í…œí”Œë¦¿ ê³ ì • ë° í‘œì¤€í™”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸ ë° ì„¤ì¹˜ (ìˆœì„œëŒ€ë¡œ ì§„í–‰ë˜ëŠ” ì‹¤ìŠµ ê³ ë ¤)\n",
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def check_and_install_package(package_name, import_name=None, version=None):\n",
    "    \"\"\"\n",
    "    íŒ¨í‚¤ì§€ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í›„ í•„ìš”ì‹œì—ë§Œ ì„¤ì¹˜\n",
    "    \"\"\"\n",
    "    if import_name is None:\n",
    "        import_name = package_name.replace('-', '_')\n",
    "    \n",
    "    try:\n",
    "        module = importlib.import_module(import_name)\n",
    "        print(f\"âœ… {package_name} ì´ë¯¸ ì„¤ì¹˜ë¨\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(f\"ğŸ“¦ {package_name} ì„¤ì¹˜ ì¤‘...\")\n",
    "        try:\n",
    "            if version:\n",
    "                subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", f\"{package_name}=={version}\"], \n",
    "                             check=True, capture_output=True)\n",
    "            else:\n",
    "                subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package_name], \n",
    "                             check=True, capture_output=True)\n",
    "            print(f\"âœ… {package_name} ì„¤ì¹˜ ì™„ë£Œ\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"âŒ {package_name} ì„¤ì¹˜ ì‹¤íŒ¨: {e}\")\n",
    "            return False\n",
    "\n",
    "print(\"ğŸš€ Day 1 ì‹¤ìŠµ 1: ë°ì´í„° ì „ì²˜ë¦¬ ë° ê²€ì¦\")\n",
    "print(\"ğŸ” í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸ ì¤‘...\")\n",
    "\n",
    "# 01ë²ˆ ë…¸íŠ¸ë¶ì—ì„œ í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤\n",
    "packages = [\n",
    "    (\"datasets\", \"datasets\"),\n",
    "    (\"transformers\", \"transformers\"), \n",
    "    (\"torch\", \"torch\"),\n",
    "    (\"jsonlines\", \"jsonlines\"),\n",
    "    (\"pandas\", \"pandas\"),\n",
    "    (\"numpy\", \"numpy\"),\n",
    "    (\"matplotlib\", \"matplotlib\"),\n",
    "    (\"seaborn\", \"seaborn\"),\n",
    "    (\"tqdm\", \"tqdm\"),\n",
    "    (\"scikit-learn\", \"sklearn\")\n",
    "]\n",
    "\n",
    "print(\"ğŸ“‹ íŒ¨í‚¤ì§€ í™•ì¸ ê²°ê³¼:\")\n",
    "for package_name, import_name in packages:\n",
    "    check_and_install_package(package_name, import_name)\n",
    "\n",
    "print(\"\\nğŸ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(\"ğŸ’¡ ë‹¤ìŒ ì…€ë¶€í„° ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from typing import Dict, List, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (matplotlib) - ê·¸ë˜í”„ì—ì„œ í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šë„ë¡ ì„¤ì •\n",
    "# Colab í™˜ê²½ì—ì„œ ë‚˜ëˆ” ê¸€ê¼´ì„ ì„¤ì¹˜í•˜ê³  matplotlibì— ì ìš©\n",
    "print(\"ğŸ”§ í•œê¸€ í°íŠ¸ ì„¤ì • ì¤‘...\")\n",
    "!apt-get update -qq\n",
    "!apt-get install fonts-nanum -qq > /dev/null\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# ë‚˜ëˆ”ë°”ë¥¸ê³ ë”• í°íŠ¸ ê²½ë¡œ ì„¤ì •\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "# í°íŠ¸ ë§¤ë‹ˆì €ì— í°íŠ¸ ì¶”ê°€ - ê·¸ë˜í”„ì—ì„œ í•œê¸€ í‘œì‹œë¥¼ ìœ„í•´ í•„ìš”\n",
    "fm.fontManager.addfont(fontpath)\n",
    "\n",
    "# matplotlib ì„¤ì • ì—…ë°ì´íŠ¸ - ëª¨ë“  ê·¸ë˜í”„ì—ì„œ í•œê¸€ì´ ì •ìƒì ìœ¼ë¡œ í‘œì‹œë¨\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'NanumBarunGothic',  # ê¸°ë³¸ í°íŠ¸ë¥¼ ë‚˜ëˆ”ë°”ë¥¸ê³ ë”•ìœ¼ë¡œ ì„¤ì •\n",
    "    'axes.unicode_minus': False         # ìŒìˆ˜ ê¸°í˜¸ í‘œì‹œ ë¬¸ì œ í•´ê²°\n",
    "})\n",
    "\n",
    "print(\"âœ… í•œê¸€ í°íŠ¸ ì„¤ì • ì™„ë£Œ - ê·¸ë˜í”„ì—ì„œ í•œê¸€ì´ ì •ìƒ í‘œì‹œë©ë‹ˆë‹¤\")\n",
    "print(\"ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í•œêµ­ì–´ ë°ì´í„°ì…‹ ë¡œë“œ ë° íƒìƒ‰\n",
    "\n",
    "### ğŸ“‹ ì‚¬ìš©í•  ë°ì´í„°ì…‹\n",
    "- **ì£¼ìš” ë°ì´í„°ì…‹**: `neural-bridge/rag-dataset-12000` (Context-Question-Answer êµ¬ì¡°)\n",
    "- **ë³´ì¡° ë°ì´í„°ì…‹**: `maywell/ko_wikidata_QA` (í•œêµ­ì–´ QA)\n",
    "- **RAFT ì „ì²˜ë¦¬**: RAG ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ context ê¸°ë°˜ í•™ìŠµ ë°ì´í„° ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG ë°ì´í„°ì…‹ ë¡œë“œ (Context-Question-Answer êµ¬ì¡°)\n",
    "print(\"ğŸ”„ RAG ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...\")\n",
    "\n",
    "try:\n",
    "    # RAG ë°ì´í„°ì…‹ ë¡œë“œ (ì´ë¯¸ contextê°€ ìˆëŠ” êµ¬ì¡°)\n",
    "    rag_dataset = load_dataset(\"neural-bridge/rag-dataset-12000\", split=\"train\")\n",
    "    print(f\"âœ… RAG ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(rag_dataset)}ê°œ ìƒ˜í”Œ\")\n",
    "    \n",
    "    # ìƒ˜í”Œ ë°ì´í„° í™•ì¸\n",
    "    print(\"\\nğŸ“‹ ë°ì´í„° êµ¬ì¡°:\")\n",
    "    sample = rag_dataset[0]\n",
    "    print(f\"ì»¬ëŸ¼: {list(sample.keys())}\")\n",
    "    for key, value in sample.items():\n",
    "        if isinstance(value, str):\n",
    "            print(f\"{key}: {value[:100]}{'...' if len(value) > 100 else ''}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ğŸ”„ ëŒ€ì•ˆ ë°ì´í„°ì…‹ ì‚¬ìš©...\")\n",
    "    \n",
    "    # ëŒ€ì•ˆ: ê°„ë‹¨í•œ í•œê¸€ RAG ë°ì´í„°ì…‹ ìƒì„±\n",
    "    sample_data = [\n",
    "        {\n",
    "            \"question\": \"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\",\n",
    "            \"context\": \"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸íŠ¹ë³„ì‹œì…ë‹ˆë‹¤. ì„œìš¸ì€ í•œê°•ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë°œë‹¬í•œ ë„ì‹œë¡œ, ì•½ 950ë§Œ ëª…ì˜ ì¸êµ¬ê°€ ê±°ì£¼í•˜ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "            \"answer\": \"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸íŠ¹ë³„ì‹œì…ë‹ˆë‹¤.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"ê¹€ì¹˜ì˜ ì£¼ìš” ì¬ë£ŒëŠ” ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "            \"context\": \"ê¹€ì¹˜ëŠ” í•œêµ­ì˜ ì „í†µ ë°œíš¨ì‹í’ˆì…ë‹ˆë‹¤. ê¹€ì¹˜ì˜ ì£¼ìš” ì¬ë£ŒëŠ” ë°°ì¶”, ê³ ì¶§ê°€ë£¨, ë§ˆëŠ˜, ìƒê°•, ì “ê°ˆ ë“±ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ë°œíš¨ ê³¼ì •ì„ í†µí•´ ìœ ì‚°ê· ì´ í’ë¶€í•´ì§‘ë‹ˆë‹¤.\",\n",
    "            \"answer\": \"ê¹€ì¹˜ì˜ ì£¼ìš” ì¬ë£ŒëŠ” ë°°ì¶”, ê³ ì¶§ê°€ë£¨, ë§ˆëŠ˜, ìƒê°•, ì “ê°ˆ ë“±ì…ë‹ˆë‹¤.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"í•œêµ­ì˜ ì „í†µ ìŒì‹ì—ëŠ” ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”?\",\n",
    "            \"context\": \"í•œêµ­ì˜ ì „í†µ ìŒì‹ìœ¼ë¡œëŠ” ê¹€ì¹˜, ë¹„ë¹”ë°¥, ë¶ˆê³ ê¸°, ê°ˆë¹„, ëƒ‰ë©´, ì‚¼ê³„íƒ• ë“±ì´ ìˆìŠµë‹ˆë‹¤. ì´ë“¤ ìŒì‹ì€ ë°œíš¨, ì¡°ë¦¼, êµ¬ì´ ë“± ë‹¤ì–‘í•œ ì¡°ë¦¬ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\",\n",
    "            \"answer\": \"í•œêµ­ì˜ ì „í†µ ìŒì‹ìœ¼ë¡œëŠ” ê¹€ì¹˜, ë¹„ë¹”ë°¥, ë¶ˆê³ ê¸°, ê°ˆë¹„, ëƒ‰ë©´, ì‚¼ê³„íƒ• ë“±ì´ ìˆìŠµë‹ˆë‹¤.\"\n",
    "        }\n",
    "    ]\n",
    "    rag_dataset = Dataset.from_list(sample_data)\n",
    "    print(f\"âœ… ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„±: {len(rag_dataset)}ê°œ ìƒ˜í”Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì •ì˜ ë° ê²€ì¦\n",
    "\n",
    "### ğŸ—ï¸ Instruction/Input/Output ìŠ¤í‚¤ë§ˆ\n",
    "- **Instruction**: ëª¨ë¸ì´ ìˆ˜í–‰í•´ì•¼ í•  ì‘ì—… ì„¤ëª…\n",
    "- **Input**: ì‘ì—…ì— í•„ìš”í•œ ì…ë ¥ ì •ë³´ (ì§ˆë¬¸ + context)\n",
    "- **Output**: ê¸°ëŒ€í•˜ëŠ” ì¶œë ¥ ê²°ê³¼ (ì •ë‹µ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data_schema(dataset: Dataset) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    ë°ì´í„°ì…‹ ìŠ¤í‚¤ë§ˆ ê²€ì¦ í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        dataset: ê²€ì¦í•  ë°ì´í„°ì…‹\n",
    "        \n",
    "    Returns:\n",
    "        ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬\n",
    "    \"\"\"\n",
    "    print(\"ğŸ” ë°ì´í„° ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì¤‘...\")\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ ìƒ˜í”Œë¡œ ì»¬ëŸ¼ í™•ì¸\n",
    "    if len(dataset) == 0:\n",
    "        return {\"error\": \"ë¹ˆ ë°ì´í„°ì…‹\"}\n",
    "    \n",
    "    sample = dataset[0]\n",
    "    columns = list(sample.keys())\n",
    "    total_samples = len(dataset)\n",
    "    \n",
    "    # ê²°ì¸¡ì¹˜ ë° ë¹ˆ ê°’ í™•ì¸\n",
    "    missing_values = {}\n",
    "    empty_values = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        missing_count = 0\n",
    "        empty_count = 0\n",
    "        \n",
    "        for item in dataset:\n",
    "            value = item.get(col)\n",
    "            if value is None:\n",
    "                missing_count += 1\n",
    "            elif isinstance(value, str) and value.strip() == \"\":\n",
    "                empty_count += 1\n",
    "        \n",
    "        missing_values[col] = missing_count\n",
    "        empty_values[col] = empty_count\n",
    "    \n",
    "    return {\n",
    "        \"total_samples\": total_samples,\n",
    "        \"columns\": columns,\n",
    "        \"missing_values\": missing_values,\n",
    "        \"empty_values\": empty_values\n",
    "    }\n",
    "\n",
    "# ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤í–‰\n",
    "validation_result = validate_data_schema(rag_dataset)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"\\nğŸ“Š ê²€ì¦ ê²°ê³¼:\")\n",
    "print(f\"ì´ ìƒ˜í”Œ ìˆ˜: {validation_result['total_samples']}\")\n",
    "print(f\"ì»¬ëŸ¼: {validation_result['columns']}\")\n",
    "print(f\"\\nê²°ì¸¡ì¹˜ í˜„í™©:\")\n",
    "for col, count in validation_result['missing_values'].items():\n",
    "    print(f\"  {col}: {count}ê°œ\")\n",
    "\n",
    "print(f\"\\në¹ˆ ê°’ í˜„í™©:\")\n",
    "for col, count in validation_result['empty_values'].items():\n",
    "    print(f\"  {col}: {count}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ë°ì´í„° ì „ì²˜ë¦¬ ê·œì¹™ ì ìš©\n",
    "\n",
    "### ğŸ§¹ ì „ì²˜ë¦¬ ê·œì¹™\n",
    "1. **ê³µë°±/ì œì–´ë¬¸ì ì •ë¦¬**: ë¶ˆí•„ìš”í•œ ê³µë°±, ì¤„ë°”ê¿ˆ, íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "2. **ì¤‘ë³µ ì œê±°**: ë™ì¼í•œ instruction-input ì¡°í•© ì œê±°\n",
    "3. **ê¸¸ì´ ì œí•œ**: ë„ˆë¬´ ê¸¸ê±°ë‚˜ ì§§ì€ ìƒ˜í”Œ í•„í„°ë§\n",
    "4. **PII ì œê±°**: ê°œì¸ì •ë³´ íŒ¨í„´ íƒì§€ ë° ë§ˆìŠ¤í‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ ì •ë¦¬ í•¨ìˆ˜: ë¶ˆí•„ìš”í•œ ê³µë°±, ì œì–´ë¬¸ì ë“± ì œê±°\n",
    "    \n",
    "    Args:\n",
    "        text: ì •ë¦¬í•  í…ìŠ¤íŠ¸\n",
    "        \n",
    "    Returns:\n",
    "        ì •ë¦¬ëœ í…ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # ì œì–´ë¬¸ì ì œê±°\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', text)\n",
    "    \n",
    "    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ë³€í™˜\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # ì•ë’¤ ê³µë°± ì œê±°\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_pii(text: str) -> str:\n",
    "    \"\"\"\n",
    "    ê°œì¸ì •ë³´ ì œê±°/ë§ˆìŠ¤í‚¹ í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        text: PII ì œê±°í•  í…ìŠ¤íŠ¸\n",
    "        \n",
    "    Returns:\n",
    "        PIIê°€ ë§ˆìŠ¤í‚¹ëœ í…ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # ì „í™”ë²ˆí˜¸ íŒ¨í„´ ë§ˆìŠ¤í‚¹\n",
    "    text = re.sub(r'\\d{2,3}-\\d{3,4}-\\d{4}', '[ì „í™”ë²ˆí˜¸]', text)\n",
    "    text = re.sub(r'\\d{3}\\d{4}\\d{4}', '[ì „í™”ë²ˆí˜¸]', text)\n",
    "    \n",
    "    # ì´ë©”ì¼ íŒ¨í„´ ë§ˆìŠ¤í‚¹\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '[ì´ë©”ì¼]', text)\n",
    "    \n",
    "    # ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ íŒ¨í„´ ë§ˆìŠ¤í‚¹ (ì˜ˆ: 123456-1234567)\n",
    "    text = re.sub(r'\\d{6}-[1-4]\\d{6}', '[ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸]', text)\n",
    "    \n",
    "    # ê³„ì¢Œë²ˆí˜¸ íŒ¨í„´ ë§ˆìŠ¤í‚¹ (ì˜ˆ: 123-456-789012)\n",
    "    text = re.sub(r'\\d{3}-\\d{2,3}-\\d{6,8}', '[ê³„ì¢Œë²ˆí˜¸]', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_rag_dataset(dataset: Dataset, \n",
    "                         min_length: int = 10, \n",
    "                         max_length: int = 2048) -> Dataset:\n",
    "    \"\"\"\n",
    "    RAG ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        dataset: ì›ë³¸ RAG ë°ì´í„°ì…‹ (context, question, answer êµ¬ì¡°)\n",
    "        min_length: ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´\n",
    "        max_length: ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´\n",
    "        \n",
    "    Returns:\n",
    "        ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§¹ RAG ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...\")\n",
    "    \n",
    "    processed_data = []\n",
    "    seen_combinations = set()  # ì¤‘ë³µ ì œê±°ìš©\n",
    "    \n",
    "    for item in tqdm(dataset, desc=\"ì „ì²˜ë¦¬ ì¤‘\"):\n",
    "        # í…ìŠ¤íŠ¸ ì •ë¦¬ (RAG ë°ì´í„° êµ¬ì¡°ì— ë§ì¶¤)\n",
    "        question = clean_text(item.get('question', ''))\n",
    "        context = clean_text(item.get('context', ''))\n",
    "        answer = clean_text(item.get('answer', ''))\n",
    "        \n",
    "        # PII ì œê±°\n",
    "        question = remove_pii(question)\n",
    "        context = remove_pii(context)\n",
    "        answer = remove_pii(answer)\n",
    "        \n",
    "        # ê¸¸ì´ í•„í„°ë§\n",
    "        if (len(answer) < min_length or \n",
    "            len(context) > max_length or\n",
    "            len(question) < 5):\n",
    "            continue\n",
    "        \n",
    "        # ì¤‘ë³µ í™•ì¸ (question + context ì¡°í•©)\n",
    "        combination = f\"{question}|{context}\"\n",
    "        if combination in seen_combinations:\n",
    "            continue\n",
    "        seen_combinations.add(combination)\n",
    "        \n",
    "        processed_data.append({\n",
    "            'question': question,\n",
    "            'context': context,\n",
    "            'answer': answer\n",
    "        })\n",
    "    \n",
    "    print(f\"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(dataset)} â†’ {len(processed_data)}ê°œ ìƒ˜í”Œ\")\n",
    "    return Dataset.from_list(processed_data)\n",
    "\n",
    "# ì „ì²˜ë¦¬ ì‹¤í–‰\n",
    "processed_dataset = preprocess_rag_dataset(rag_dataset)\n",
    "print(f\"\\nğŸ“Š ì „ì²˜ë¦¬ ê²°ê³¼: {len(processed_dataset)}ê°œ ìƒ˜í”Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAFT ë°©ë²•ë¡  ì ìš©\n",
    "\n",
    "### ğŸ¯ RAFT (Retrieval Augmented Fine Tuning) ê°œë…\n",
    "- **ëª©ì **: RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ íŠ¹ìˆ˜í•œ íŒŒì¸íŠœë‹ ë°©ë²•\n",
    "- **í•µì‹¬ ì•„ì´ë””ì–´**: ëª¨ë¸ì´ ê´€ë ¨ contextë¥¼ ì˜ í™œìš©í•˜ë„ë¡ í•™ìŠµ\n",
    "- **ë°©ë²•**: Positive/Negative ìƒ˜í”Œì„ í†µí•œ ëŒ€ì¡° í•™ìŠµ\n",
    "\n",
    "### ğŸ“‹ RAFT ë°ì´í„° êµ¬ì¡°\n",
    "- **Positive ìƒ˜í”Œ** (60%): ì •ë‹µ context + 3ê°œ distractor context\n",
    "- **Negative ìƒ˜í”Œ** (40%): 4ê°œ distractor context (ì •ë‹µ ì—†ìŒ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raft_dataset_from_rag(dataset: Dataset, \n",
    "                                num_samples: int = 1000,\n",
    "                                positive_ratio: float = 0.6) -> Dataset:\n",
    "    \"\"\"\n",
    "    RAG ë°ì´í„°ì…‹ìœ¼ë¡œë¶€í„° RAFT ìŠ¤íƒ€ì¼ ë°ì´í„°ì…‹ ìƒì„± í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        dataset: ì›ë³¸ RAG ë°ì´í„°ì…‹ (question, context, answer êµ¬ì¡°)\n",
    "        num_samples: ìƒì„±í•  ì´ ìƒ˜í”Œ ìˆ˜\n",
    "        positive_ratio: positive ìƒ˜í”Œ ë¹„ìœ¨\n",
    "        \n",
    "    Returns:\n",
    "        RAFT í˜•íƒœë¡œ ë³€í™˜ëœ ë°ì´í„°ì…‹\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ RAFT ë°ì´í„°ì…‹ ìƒì„± ì¤‘...\")\n",
    "    \n",
    "    # ìƒ˜í”Œ ìˆ˜ ì œí•œ\n",
    "    if len(dataset) > num_samples:\n",
    "        dataset = dataset.shuffle(seed=42).select(range(num_samples))\n",
    "    \n",
    "    num_positive = int(len(dataset) * positive_ratio)\n",
    "    num_negative = len(dataset) - num_positive\n",
    "    \n",
    "    # ëª¨ë“  contextë¥¼ context poolë¡œ ì‚¬ìš©\n",
    "    all_contexts = [item['context'] for item in dataset]\n",
    "    \n",
    "    raft_data = []\n",
    "    used_indices = set()\n",
    "    \n",
    "    # Positive ìƒ˜í”Œ ìƒì„±\n",
    "    print(f\"âœ… Positive ìƒ˜í”Œ ìƒì„±: {num_positive}ê°œ\")\n",
    "    positive_count = 0\n",
    "    \n",
    "    for i in tqdm(range(len(dataset)), desc=\"Positive ìƒ˜í”Œ ìƒì„±\"):\n",
    "        if positive_count >= num_positive:\n",
    "            break\n",
    "            \n",
    "        if i in used_indices:\n",
    "            continue\n",
    "            \n",
    "        item = dataset[i]\n",
    "        correct_context = item['context']\n",
    "        question = item['question']\n",
    "        answer = item['answer']\n",
    "        \n",
    "        # 3ê°œì˜ distractor ì„ íƒ (ì •ë‹µì´ ì•„ë‹Œ ë‹¤ë¥¸ contextë“¤)\n",
    "        distractors = random.sample(\n",
    "            [ctx for j, ctx in enumerate(all_contexts) if j != i and ctx != correct_context], \n",
    "            min(3, len(all_contexts) - 1)\n",
    "        )\n",
    "        \n",
    "        # context ë¦¬ìŠ¤íŠ¸ êµ¬ì„± (ì •ë‹µ contextë¥¼ ëœë¤ ìœ„ì¹˜ì— ë°°ì¹˜)\n",
    "        contexts = [correct_context] + distractors\n",
    "        random.shuffle(contexts)\n",
    "        \n",
    "        raft_data.append({\n",
    "            'type': 'positive',\n",
    "            'question': question,\n",
    "            'contexts': contexts,\n",
    "            'answer': answer,\n",
    "            'instruction': \"ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\"\n",
    "        })\n",
    "        \n",
    "        used_indices.add(i)\n",
    "        positive_count += 1\n",
    "    \n",
    "    # Negative ìƒ˜í”Œ ìƒì„±\n",
    "    print(f\"âœ… Negative ìƒ˜í”Œ ìƒì„±: {num_negative}ê°œ\")\n",
    "    negative_count = 0\n",
    "    \n",
    "    for i in tqdm(range(len(dataset)), desc=\"Negative ìƒ˜í”Œ ìƒì„±\"):\n",
    "        if negative_count >= num_negative:\n",
    "            break\n",
    "            \n",
    "        if i in used_indices:\n",
    "            continue\n",
    "            \n",
    "        item = dataset[i]\n",
    "        question = item['question']\n",
    "        correct_answer = item['answer']\n",
    "        correct_context = item['context']\n",
    "        \n",
    "        # 4ê°œì˜ distractorë§Œ ì„ íƒ (ì •ë‹µ context ì œì™¸)\n",
    "        distractors = random.sample(\n",
    "            [ctx for j, ctx in enumerate(all_contexts) if j != i and ctx != correct_context],\n",
    "            min(4, len(all_contexts) - 1)\n",
    "        )\n",
    "        \n",
    "        raft_data.append({\n",
    "            'type': 'negative', \n",
    "            'question': question,\n",
    "            'contexts': distractors,\n",
    "            'answer': correct_answer,  # ì •ë‹µì€ ìˆì§€ë§Œ contextì—ëŠ” ì—†ìŒ\n",
    "            'instruction': \"ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\"\n",
    "        })\n",
    "        \n",
    "        used_indices.add(i)\n",
    "        negative_count += 1\n",
    "    \n",
    "    # ë°ì´í„° ì„ê¸°\n",
    "    random.shuffle(raft_data)\n",
    "    \n",
    "    print(f\"âœ… RAFT ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(raft_data)}ê°œ ìƒ˜í”Œ\")\n",
    "    print(f\"   - Positive: {positive_count}ê°œ\")\n",
    "    print(f\"   - Negative: {negative_count}ê°œ\")\n",
    "    \n",
    "    return Dataset.from_list(raft_data)\n",
    "\n",
    "# RAFT ë°ì´í„°ì…‹ ìƒì„±\n",
    "raft_dataset = create_raft_dataset_from_rag(processed_dataset, num_samples=500)\n",
    "\n",
    "# ìƒ˜í”Œ í™•ì¸\n",
    "print(\"\\nğŸ“‹ RAFT ë°ì´í„° ìƒ˜í”Œ:\")\n",
    "sample = raft_dataset[0]\n",
    "print(f\"Type: {sample['type']}\")\n",
    "print(f\"Question: {sample['question'][:100]}...\")\n",
    "print(f\"Contexts: {len(sample['contexts'])}ê°œ\")\n",
    "print(f\"Answer: {sample['answer'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. í…œí”Œë¦¿ ì„¤ê³„ ë° ê³ ì •\n",
    "\n",
    "### ğŸ“ EXAONE ëª¨ë¸ìš© Chat Template\n",
    "- **System Role**: ëª¨ë¸ì˜ ì—­í•  ì •ì˜\n",
    "- **User Role**: ì‚¬ìš©ì ì…ë ¥ (ì§ˆë¬¸ + contexts)\n",
    "- **Assistant Role**: ëª¨ë¸ ì‘ë‹µ (ë‹µë³€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_template(item: Dict[str, Any]) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    EXAONE ëª¨ë¸ìš© ì±„íŒ… í…œí”Œë¦¿ ìƒì„±\n",
    "    \n",
    "    Args:\n",
    "        item: RAFT ë°ì´í„° ì•„ì´í…œ\n",
    "        \n",
    "    Returns:\n",
    "        ì±„íŒ… ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    # System ë©”ì‹œì§€\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. \"\n",
    "            \"ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ ë‹µë³€í•˜ë˜, ì •ë³´ê°€ ì—†ë‹¤ë©´ ëª¨ë¥¸ë‹¤ê³  ì†”ì§íˆ ë‹µë³€í•˜ì„¸ìš”.\"\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # User ë©”ì‹œì§€ - êµ¬ì¡°í™”ëœ ì…ë ¥\n",
    "    contexts_text = \"\\n\\n\".join([f\"ì»¨í…ìŠ¤íŠ¸ {i+1}: {ctx}\" for i, ctx in enumerate(item['contexts'])])\n",
    "    \n",
    "    user_content = f\"\"\"ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "=== ì»¨í…ìŠ¤íŠ¸ ===\n",
    "{contexts_text}\n",
    "\n",
    "=== ì§ˆë¬¸ ===\n",
    "{item['question']}\n",
    "\n",
    "=== ìš”ì²­ì‚¬í•­ ===\n",
    "{item['instruction']}\"\"\"\n",
    "    \n",
    "    user_message = {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": user_content\n",
    "    }\n",
    "    \n",
    "    # Assistant ë©”ì‹œì§€ (í•™ìŠµì‹œì—ë§Œ í¬í•¨)\n",
    "    assistant_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": item['answer']\n",
    "    }\n",
    "    \n",
    "    return [system_message, user_message, assistant_message]\n",
    "\n",
    "def apply_chat_template_to_dataset(dataset: Dataset) -> Dataset:\n",
    "    \"\"\"\n",
    "    ë°ì´í„°ì…‹ ì „ì²´ì— ì±„íŒ… í…œí”Œë¦¿ ì ìš©\n",
    "    \n",
    "    Args:\n",
    "        dataset: RAFT ë°ì´í„°ì…‹\n",
    "        \n",
    "    Returns:\n",
    "        í…œí”Œë¦¿ì´ ì ìš©ëœ ë°ì´í„°ì…‹\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ ì±„íŒ… í…œí”Œë¦¿ ì ìš© ì¤‘...\")\n",
    "    \n",
    "    templated_data = []\n",
    "    \n",
    "    for item in tqdm(dataset, desc=\"í…œí”Œë¦¿ ì ìš©\"):\n",
    "        chat_messages = create_chat_template(item)\n",
    "        \n",
    "        templated_item = {\n",
    "            'messages': chat_messages,\n",
    "            'type': item['type'],\n",
    "            'original_question': item['question'],\n",
    "            'original_answer': item['answer']\n",
    "        }\n",
    "        templated_data.append(templated_item)\n",
    "    \n",
    "    print(f\"âœ… í…œí”Œë¦¿ ì ìš© ì™„ë£Œ: {len(templated_data)}ê°œ ìƒ˜í”Œ\")\n",
    "    return Dataset.from_list(templated_data)\n",
    "\n",
    "# í…œí”Œë¦¿ ì ìš©\n",
    "templated_dataset = apply_chat_template_to_dataset(raft_dataset)\n",
    "\n",
    "# í…œí”Œë¦¿ ì ìš© ê²°ê³¼ í™•ì¸\n",
    "print(\"\\nğŸ“‹ í…œí”Œë¦¿ ì ìš© ê²°ê³¼:\")\n",
    "sample = templated_dataset[0]\n",
    "print(f\"Type: {sample['type']}\")\n",
    "print(f\"Messages: {len(sample['messages'])}ê°œ\")\n",
    "print(f\"\\nì²« ë²ˆì§¸ ë©”ì‹œì§€:\")\n",
    "print(f\"Role: {sample['messages'][0]['role']}\")\n",
    "print(f\"Content: {sample['messages'][0]['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. í† í¬ë‚˜ì´ì € ë¡œë“œ ë° ê¸¸ì´ ê²€ì¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAONE í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "print(\"ğŸ”„ EXAONE í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\")\n",
    "\n",
    "def analyze_token_lengths(dataset: Dataset, tokenizer, max_length: int = 4096) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    í† í° ê¸¸ì´ ë¶„ì„ í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        dataset: ë¶„ì„í•  ë°ì´í„°ì…‹\n",
    "        tokenizer: ì‚¬ìš©í•  í† í¬ë‚˜ì´ì €\n",
    "        max_length: ìµœëŒ€ í† í° ê¸¸ì´\n",
    "        \n",
    "    Returns:\n",
    "        í† í° ê¸¸ì´ ë¶„ì„ ê²°ê³¼\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š í† í° ê¸¸ì´ ë¶„ì„ ì¤‘...\")\n",
    "    \n",
    "    token_lengths = []\n",
    "    overflow_count = 0\n",
    "    \n",
    "    for item in tqdm(dataset, desc=\"í† í° ê¸¸ì´ ê³„ì‚°\"):\n",
    "        # ì „ì²´ ëŒ€í™”ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "        full_text = tokenizer.apply_chat_template(\n",
    "            item['messages'], \n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        \n",
    "        # í† í° ê°œìˆ˜ ê³„ì‚°\n",
    "        tokens = tokenizer.encode(full_text)\n",
    "        token_length = len(tokens)\n",
    "        token_lengths.append(token_length)\n",
    "        \n",
    "        if token_length > max_length:\n",
    "            overflow_count += 1\n",
    "    \n",
    "    analysis_result = {\n",
    "        \"total_samples\": len(token_lengths),\n",
    "        \"mean_length\": np.mean(token_lengths),\n",
    "        \"median_length\": np.median(token_lengths),\n",
    "        \"min_length\": np.min(token_lengths),\n",
    "        \"max_length\": np.max(token_lengths),\n",
    "        \"std_length\": np.std(token_lengths),\n",
    "        \"overflow_count\": overflow_count,\n",
    "        \"overflow_rate\": overflow_count / len(token_lengths),\n",
    "        \"token_lengths\": token_lengths\n",
    "    }\n",
    "    \n",
    "    return analysis_result\n",
    "\n",
    "# í† í° ê¸¸ì´ ë¶„ì„ ì‹¤í–‰\n",
    "token_analysis = analyze_token_lengths(templated_dataset, tokenizer)\n",
    "\n",
    "# ë¶„ì„ ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"\\nğŸ“Š í† í° ê¸¸ì´ ë¶„ì„ ê²°ê³¼:\")\n",
    "print(f\"ì´ ìƒ˜í”Œ ìˆ˜: {token_analysis['total_samples']}ê°œ\")\n",
    "print(f\"í‰ê·  ê¸¸ì´: {token_analysis['mean_length']:.1f} í† í°\")\n",
    "print(f\"ì¤‘ê°„ê°’: {token_analysis['median_length']:.1f} í† í°\")\n",
    "print(f\"ìµœì†Œ ê¸¸ì´: {token_analysis['min_length']} í† í°\")\n",
    "print(f\"ìµœëŒ€ ê¸¸ì´: {token_analysis['max_length']} í† í°\")\n",
    "print(f\"í‘œì¤€í¸ì°¨: {token_analysis['std_length']:.1f} í† í°\")\n",
    "print(f\"4096 í† í° ì´ˆê³¼: {token_analysis['overflow_count']}ê°œ ({token_analysis['overflow_rate']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train/Valid ë¶„í•  ë° ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Valid ë¶„í•  (8:2 ë¹„ìœ¨)\n",
    "print(\"ğŸ”„ Train/Valid ë¶„í•  ì¤‘...\")\n",
    "\n",
    "train_indices, valid_indices = train_test_split(\n",
    "    list(range(len(templated_dataset))),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=[item['type'] for item in templated_dataset]  # positive/negative ë¹„ìœ¨ ìœ ì§€\n",
    ")\n",
    "\n",
    "train_dataset = templated_dataset.select(train_indices)\n",
    "valid_dataset = templated_dataset.select(valid_indices)\n",
    "\n",
    "print(f\"âœ… ë¶„í•  ì™„ë£Œ:\")\n",
    "print(f\"  - Train: {len(train_dataset)}ê°œ ìƒ˜í”Œ\")\n",
    "print(f\"  - Valid: {len(valid_dataset)}ê°œ ìƒ˜í”Œ\")\n",
    "\n",
    "# ë¶„í•  ê²°ê³¼ í™•ì¸\n",
    "train_types = [item['type'] for item in train_dataset]\n",
    "valid_types = [item['type'] for item in valid_dataset]\n",
    "\n",
    "print(f\"\\nğŸ“Š ë¶„í•  ê²°ê³¼ í™•ì¸:\")\n",
    "print(f\"Train - Positive: {train_types.count('positive')}ê°œ, Negative: {train_types.count('negative')}ê°œ\")\n",
    "print(f\"Valid - Positive: {valid_types.count('positive')}ê°œ, Negative: {valid_types.count('negative')}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "import os\n",
    "os.makedirs('processed_data', exist_ok=True)\n",
    "\n",
    "# ë°ì´í„°ì…‹ì„ JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥\n",
    "def save_dataset_jsonl(dataset: Dataset, file_path: str):\n",
    "    \"\"\"ë°ì´í„°ì…‹ì„ JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥\"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for item in dataset:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Train/Valid ë°ì´í„°ì…‹ ì €ì¥\n",
    "print(\"ğŸ’¾ ë°ì´í„°ì…‹ ì €ì¥ ì¤‘...\")\n",
    "save_dataset_jsonl(train_dataset, 'processed_data/train_raft_ko.jsonl')\n",
    "save_dataset_jsonl(valid_dataset, 'processed_data/valid_raft_ko.jsonl')\n",
    "\n",
    "# ë©”íƒ€ë°ì´í„° ì €ì¥\n",
    "metadata = {\n",
    "    \"dataset_info\": {\n",
    "        \"total_samples\": len(templated_dataset),\n",
    "        \"train_samples\": len(train_dataset),\n",
    "        \"valid_samples\": len(valid_dataset),\n",
    "        \"positive_samples\": len([item for item in templated_dataset if item['type'] == 'positive']),\n",
    "        \"negative_samples\": len([item for item in templated_dataset if item['type'] == 'negative'])\n",
    "    },\n",
    "    \"token_analysis\": {\n",
    "        \"mean_length\": float(token_analysis['mean_length']),\n",
    "        \"median_length\": float(token_analysis['median_length']),\n",
    "        \"max_length\": int(token_analysis['max_length']),\n",
    "        \"overflow_count\": int(token_analysis['overflow_count']),\n",
    "        \"overflow_rate\": float(token_analysis['overflow_rate'])\n",
    "    },\n",
    "    \"processing_info\": {\n",
    "        \"source_dataset\": \"neural-bridge/rag-dataset-12000\",\n",
    "        \"preprocessing\": \"RAFT + Chat Template\",\n",
    "        \"model_target\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('processed_data/metadata.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ:\")\n",
    "print(f\"  - processed_data/train_raft_ko.jsonl\")\n",
    "print(f\"  - processed_data/valid_raft_ko.jsonl\")\n",
    "print(f\"  - processed_data/metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë° ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í† í° ê¸¸ì´ ë¶„í¬ ë° ë°ì´í„° í’ˆì§ˆ ì¢…í•© ì‹œê°í™”\n",
    "# ì´ ì°¨íŠ¸ë“¤ì€ RAFT ë°ì´í„°ì…‹ì˜ í’ˆì§ˆê³¼ íŠ¹ì„±ì„ í•œëˆˆì— íŒŒì•…í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# 2x2 ì„œë¸Œí”Œë¡¯ êµ¬ì„± - 4ê°œì˜ ë‹¤ë¥¸ ê´€ì ì—ì„œ ë°ì´í„°ë¥¼ ë¶„ì„\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. í† í° ê¸¸ì´ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ \n",
    "# ğŸ“Š ì˜ë¯¸: ëª¨ë¸ ì…ë ¥ ê¸¸ì´ì˜ ë¶„í¬ë¥¼ ë³´ì—¬ì¤Œ. ëŒ€ë¶€ë¶„ì˜ ë°ì´í„°ê°€ 4096 í† í° ì´í•˜ì¸ì§€ í™•ì¸\n",
    "ax1.hist(token_analysis['token_lengths'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.axvline(token_analysis['mean_length'], color='red', linestyle='--', \n",
    "           label=f'í‰ê·  ê¸¸ì´: {token_analysis[\"mean_length\"]:.1f} í† í°')\n",
    "ax1.axvline(4096, color='orange', linestyle='--', label='ëª¨ë¸ ìµœëŒ€ ê¸¸ì´: 4096 í† í°')\n",
    "ax1.set_xlabel('í† í° ê¸¸ì´ (ê°œ)')\n",
    "ax1.set_ylabel('ìƒ˜í”Œ ë¹ˆë„ìˆ˜')\n",
    "ax1.set_title('ğŸ“ í† í° ê¸¸ì´ ë¶„í¬\\n(ëŒ€ë¶€ë¶„ ìƒ˜í”Œì´ ëª¨ë¸ ì œí•œ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸)', fontsize=12, pad=15)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ë¶„í¬ í•´ì„ì„ ìœ„í•œ ì¶”ê°€ ì •ë³´ í‘œì‹œ\n",
    "ax1.text(0.7, 0.9, f'ì¤‘ê°„ê°’: {token_analysis[\"median_length\"]:.0f}\\n'\n",
    "                   f'í‘œì¤€í¸ì°¨: {token_analysis[\"std_length\"]:.0f}\\n'\n",
    "                   f'ì˜¤ë²„í”Œë¡œìš°: {token_analysis[\"overflow_count\"]}ê°œ', \n",
    "         transform=ax1.transAxes, fontsize=9, \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"wheat\", alpha=0.8))\n",
    "\n",
    "# 2. RAFT Positive/Negative ìƒ˜í”Œ ë¹„ìœ¨ íŒŒì´ì°¨íŠ¸\n",
    "# ğŸ“Š ì˜ë¯¸: RAFT ë°©ë²•ë¡ ì˜ í•µì‹¬ì¸ positive/negative ìƒ˜í”Œ ê· í˜• í™•ì¸\n",
    "# - Positive (60%): ì •ë‹µì´ í¬í•¨ëœ contextë¡œ í•™ìŠµ (ì •í™•í•œ ì •ë³´ í™œìš© í•™ìŠµ)\n",
    "# - Negative (40%): ì •ë‹µì´ ì—†ëŠ” contextë¡œ í•™ìŠµ (ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ëŠ” ë²• í•™ìŠµ)\n",
    "type_counts = [train_types.count('positive'), train_types.count('negative')]\n",
    "colors = ['lightcoral', 'lightblue']\n",
    "wedges, texts, autotexts = ax2.pie(type_counts, labels=['Positive ìƒ˜í”Œ', 'Negative ìƒ˜í”Œ'], \n",
    "                                  autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "ax2.set_title('ğŸ¯ RAFT ìƒ˜í”Œ ë¹„ìœ¨\\n(RAG ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ëŒ€ì¡° í•™ìŠµ)', fontsize=12, pad=15)\n",
    "\n",
    "# íŒŒì´ì°¨íŠ¸ í…ìŠ¤íŠ¸ í¬ê¸° ì¡°ì •\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "# 3. í† í° ê¸¸ì´ ìƒì„¸ ë¶„í¬ (ë°•ìŠ¤í”Œë¡¯)\n",
    "# ğŸ“Š ì˜ë¯¸: í† í° ê¸¸ì´ì˜ ì‚¬ë¶„ìœ„ìˆ˜, ì´ìƒê°’ ë“±ì„ ë³´ì—¬ì¤Œ\n",
    "# - ì¤‘ì•™ê°’, 1/3ë¶„ìœ„ìˆ˜ë¡œ ë°ì´í„°ì˜ ì§‘ì¤‘ë„ íŒŒì•…\n",
    "# - ì´ìƒê°’(outlier)ìœ¼ë¡œ ë¹„ì •ìƒì ìœ¼ë¡œ ê¸´ ìƒ˜í”Œ ì‹ë³„\n",
    "box_plot = ax3.boxplot(token_analysis['token_lengths'], patch_artist=True)\n",
    "box_plot['boxes'][0].set_facecolor('lightgreen')\n",
    "ax3.set_ylabel('í† í° ê¸¸ì´ (ê°œ)')\n",
    "ax3.set_title('ğŸ“¦ í† í° ê¸¸ì´ ìƒì„¸ ë¶„í¬\\n(ì‚¬ë¶„ìœ„ìˆ˜ì™€ ì´ìƒê°’ ë¶„ì„)', fontsize=12, pad=15)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# ë°•ìŠ¤í”Œë¡¯ í•´ì„ì„ ìœ„í•œ ì£¼ì„ ì¶”ê°€\n",
    "q1 = np.percentile(token_analysis['token_lengths'], 25)\n",
    "q3 = np.percentile(token_analysis['token_lengths'], 75)\n",
    "ax3.text(1.1, 0.8, f'Q1 (25%): {q1:.0f}\\n'\n",
    "                   f'ì¤‘ì•™ê°’: {token_analysis[\"median_length\"]:.0f}\\n'\n",
    "                   f'Q3 (75%): {q3:.0f}', \n",
    "         transform=ax3.transAxes, fontsize=9,\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.3))\n",
    "\n",
    "# 4. Context ê°œìˆ˜ë³„ ë¶„í¬ ë§‰ëŒ€ê·¸ë˜í”„\n",
    "# ğŸ“Š ì˜ë¯¸: ê° ìƒ˜í”Œë‹¹ ì œê³µë˜ëŠ” context ê°œìˆ˜ ë¶„í¬\n",
    "# - RAFTì—ì„œëŠ” ë³´í†µ 4ê°œ context ì‚¬ìš© (1ê°œ ì •ë‹µ + 3ê°œ distractor ë˜ëŠ” 4ê°œ distractor)\n",
    "context_counts = [len(item['contexts']) for item in raft_dataset]\n",
    "context_count_freq = pd.Series(context_counts).value_counts().sort_index()\n",
    "bars = ax4.bar(context_count_freq.index, context_count_freq.values, \n",
    "               alpha=0.7, color='mediumpurple', edgecolor='black')\n",
    "ax4.set_xlabel('ìƒ˜í”Œë‹¹ Context ê°œìˆ˜')\n",
    "ax4.set_ylabel('ìƒ˜í”Œ ìˆ˜')\n",
    "ax4.set_title('ğŸ“š Context ê°œìˆ˜ë³„ ë¶„í¬\\n(RAFT êµ¬ì¡°ì˜ ì¼ê´€ì„± í™•ì¸)', fontsize=12, pad=15)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# ë§‰ëŒ€ê·¸ë˜í”„ ìœ„ì— ê°’ í‘œì‹œ\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{int(height)}ê°œ', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# ì „ì²´ ë ˆì´ì•„ì›ƒ ì¡°ì • ë° ì €ì¥\n",
    "plt.tight_layout(pad=3.0)  # ì„œë¸Œí”Œë¡¯ ê°„ê²© ì¡°ì •\n",
    "plt.savefig('processed_data/data_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ì°¨íŠ¸ ì €ì¥ ì™„ë£Œ: processed_data/data_analysis.png\")\n",
    "print(\"\\nğŸ” ì°¨íŠ¸ í•´ì„ ê°€ì´ë“œ:\")\n",
    "print(\"  ğŸ“ í† í° ê¸¸ì´ ë¶„í¬: ëŒ€ë¶€ë¶„ ìƒ˜í”Œì´ ëª¨ë¸ ìµœëŒ€ ê¸¸ì´(4096) ì´ë‚´ì¸ì§€ í™•ì¸\")\n",
    "print(\"  ğŸ¯ RAFT ìƒ˜í”Œ ë¹„ìœ¨: Positive(60%) vs Negative(40%) ê· í˜•ìœ¼ë¡œ ëŒ€ì¡° í•™ìŠµ íš¨ê³¼ ê·¹ëŒ€í™”\")\n",
    "print(\"  ğŸ“¦ í† í° ê¸¸ì´ ë°•ìŠ¤í”Œë¡¯: ë°ì´í„° ì§‘ì¤‘ë„ì™€ ì´ìƒê°’ìœ¼ë¡œ í’ˆì§ˆ ë¬¸ì œ íŒŒì•…\")  \n",
    "print(\"  ğŸ“š Context ê°œìˆ˜ ë¶„í¬: RAFT êµ¬ì¡°ì˜ ì¼ê´€ì„± í™•ì¸ (ë³´í†µ 3-4ê°œ)\")\n",
    "print(\"\\nğŸ’¡ ì´ ì°¨íŠ¸ë“¤ì„ í†µí•´ ë‹¤ìŒ ì‹¤ìŠµì—ì„œ ì‚¬ìš©í•  ë°ì´í„°ì˜ í’ˆì§ˆì„ ì‚¬ì „ ê²€ì¦í–ˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… ì™„ë£Œëœ ì‘ì—…\n",
    "1. **RAG ë°ì´í„°ì…‹ ë¡œë“œ**: neural-bridge/rag-dataset-12000 í™œìš©\n",
    "2. **ë°ì´í„° ì „ì²˜ë¦¬**: ê³µë°±/ì œì–´ë¬¸ì/ì¤‘ë³µ/ê¸¸ì´/PII ì²˜ë¦¬\n",
    "3. **RAFT ë°©ë²•ë¡  ì ìš©**: Positive/Negative ìƒ˜í”Œ ìƒì„±\n",
    "4. **í…œí”Œë¦¿ í‘œì¤€í™”**: EXAONE ëª¨ë¸ìš© Chat Template\n",
    "5. **í† í° ê¸¸ì´ ê²€ì¦**: 4096 í† í° ì œí•œ ì¤€ìˆ˜ í™•ì¸\n",
    "6. **Train/Valid ë¶„í• **: 8:2 ë¹„ìœ¨, ê· ë“± ë¶„í• \n",
    "7. **ë°ì´í„° ì €ì¥**: JSONL í˜•íƒœë¡œ ì €ì¥\n",
    "8. **í’ˆì§ˆ ê²€ì¦**: ì‹œê°í™” ë° í†µê³„ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ê²€ì¦ ë° ìš”ì•½\n",
    "print(\"ğŸ¯ Day 1 ì‹¤ìŠµ 1 ì™„ë£Œ ìš”ì•½\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ“ ì €ì¥ëœ íŒŒì¼:\")\n",
    "print(f\"  - processed_data/train_raft_ko.jsonl ({len(train_dataset)}ê°œ ìƒ˜í”Œ)\")\n",
    "print(f\"  - processed_data/valid_raft_ko.jsonl ({len(valid_dataset)}ê°œ ìƒ˜í”Œ)\")\n",
    "print(f\"  - processed_data/metadata.json\")\n",
    "print(f\"  - processed_data/data_analysis.png\")\n",
    "\n",
    "print(f\"\\nğŸ“Š ë°ì´í„° í’ˆì§ˆ ì§€í‘œ:\")\n",
    "print(f\"  - í‰ê·  í† í° ê¸¸ì´: {token_analysis['mean_length']:.1f}\")\n",
    "print(f\"  - ìµœëŒ€ í† í° ê¸¸ì´: {token_analysis['max_length']}\")\n",
    "print(f\"  - 4096 í† í° ì´ˆê³¼ìœ¨: {token_analysis['overflow_rate']:.1%}\")\n",
    "print(f\"  - Positive ìƒ˜í”Œ: {train_types.count('positive')}ê°œ\")\n",
    "print(f\"  - Negative ìƒ˜í”Œ: {train_types.count('negative')}ê°œ\")\n",
    "\n",
    "print(f\"\\nâœ… ë‹¤ìŒ ì‹¤ìŠµì—ì„œ ì‚¬ìš©í•  ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(f\"   02_data_quality_check.ipynbì—ì„œ ë” ìì„¸í•œ í’ˆì§ˆ ê²€ì¦ì„ ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n",
    "print(f\"\\nğŸ“‹ ìƒì„±ëœ ë°ì´í„° ìƒ˜í”Œ:\")\n",
    "sample_item = train_dataset[0]\n",
    "print(f\"Type: {sample_item['type']}\")\n",
    "print(f\"System: {sample_item['messages'][0]['content'][:100]}...\")\n",
    "print(f\"User: {sample_item['messages'][1]['content'][:200]}...\")\n",
    "print(f\"Assistant: {sample_item['messages'][2]['content'][:100]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
