{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 ì‹¤ìŠµ 1: ë°ì´í„° ì „ì²˜ë¦¬ & ê²€ì¦\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” íŒŒì¸íŠœë‹ì— ì‚¬ìš©í•  í•œêµ­ì–´ RAG ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê³ , RAFT í…œí”Œë¦¿ì— ë§ê²Œ ê²€ì¦ê¹Œì§€ ì§„í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ëª©ì°¨\n",
    "1. ë°ì´í„° ë¡œë“œ\n",
    "2. ìŠ¤í‚¤ë§ˆ ì •ì˜ ë° ê²€ì¦\n",
    "3. ì „ì²˜ë¦¬ ê·œì¹™ ì ìš©\n",
    "4. RAFT í…œí”Œë¦¿ ë³€í™˜\n",
    "5. í†µê³„ í™•ì¸\n",
    "6. ì €ì¥ ë° Google Drive ë°±ì—…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë°ì´í„° ë¡œë“œ\n",
    "- Hugging Faceì˜ `neural-bridge/rag-dataset-12000`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- ì´ë¯¸ context/question/answer êµ¬ì¡°ë¥¼ í¬í•¨í•˜ê³  ìˆì–´ RAG ì‹¤í—˜ì— ì í•©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "DATASET_NAME = \"neural-bridge/rag-dataset-12000\"\n",
    "print(f\"ğŸ“¥ ë°ì´í„° ë¡œë“œ: {DATASET_NAME}\")\n",
    "\n",
    "raw_dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "print(f\"   - ìƒ˜í”Œ ìˆ˜: {len(raw_dataset):,}\")\n",
    "print(f\"   - í•„ë“œ: {raw_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ìŠ¤í‚¤ë§ˆ ì •ì˜ ë° ê²€ì¦\n",
    "Instruction/Input/Output í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ íŒŒì¸íŠœë‹ì— ì“°ê¸° ì¢‹ê²Œ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_record(record: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"RAG ë ˆì½”ë“œë¥¼ Instruction/Input/Output í˜•íƒœë¡œ ë³€í™˜.\"\"\"\n",
    "    instruction = \"ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.\"\n",
    "    question = str(record.get(\"question\", \"\"))\n",
    "    context = str(record.get(\"positive_ctx\", record.get(\"context\", \"\")))\n",
    "    answer = str(record.get(\"answer\", record.get(\"positive_answer\", \"\")))\n",
    "\n",
    "    return {\n",
    "        \"instruction\": instruction,\n",
    "        \"input\": f\"ì§ˆë¬¸: {question}ë¬¸ì„œ: {context}\",\n",
    "        \"output\": answer,\n",
    "    }\n",
    "\n",
    "normalized_samples = [normalize_record(raw_dataset[i]) for i in range(3)]\n",
    "normalized_samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì „ì²˜ë¦¬ ê·œì¹™ ì ìš©\n",
    "- ê³µë°± ì •ë¦¬, ì¤‘ë³µ ì œê±° ë“± ê¸°ë³¸ ì •ë¦¬ë¥¼ ì ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = text.replace('â€‹', ' ').replace('Â ', ' ')\n",
    "    text = ' '.join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "processed_records = []\n",
    "seen_pairs = set()\n",
    "\n",
    "for record in raw_dataset:\n",
    "    normalized = normalize_record(record)\n",
    "    normalized = {k: clean_text(v) for k, v in normalized.items()}\n",
    "\n",
    "    pair = (normalized[\"instruction\"], normalized[\"input\"])\n",
    "    if pair in seen_pairs:\n",
    "        continue\n",
    "    seen_pairs.add(pair)\n",
    "\n",
    "    processed_records.append(normalized)\n",
    "\n",
    "print(\"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "print(f\"   - ì¤‘ë³µ ì œê±° í›„ ìƒ˜í”Œ ìˆ˜: {len(processed_records):,}\")\n",
    "processed_records[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAFT í…œí”Œë¦¿ ë³€í™˜\n",
    "- ì •ë‹µ ë¬¸ì„œ(positive)ì™€ distractor ë¬¸ì„œë¥¼ í¬í•¨í•˜ëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "all_contexts = [normalize_record(item)[\"input\"] for item in raw_dataset]\n",
    "\n",
    "raft_records = []\n",
    "for record in tqdm(processed_records, total=len(processed_records)):\n",
    "    question_line = record[\"input\"]\n",
    "    answer = record[\"output\"]\n",
    "    context = question_line\n",
    "\n",
    "    # distractor ì„ íƒ (ê°„ë‹¨íˆ ëœë¤ ì„ íƒ)\n",
    "    distractors = []\n",
    "    while len(distractors) < 2:\n",
    "        candidate = random.choice(all_contexts)\n",
    "        if candidate != context and candidate not in distractors:\n",
    "            distractors.append(candidate)\n",
    "\n",
    "    text = \" \".join([\n",
    "        question_line,\n",
    "        \"ì˜¤ë‹µ ë¬¸ì„œ: \" + \" || \".join(distractors),\n",
    "        \"ìœ„ ìë£Œë¥¼ ì°¸ê³ í•´ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.\",\n",
    "    ])\n",
    "\n",
    "    raft_records.append({\n",
    "        \"text\": text,\n",
    "        \"question\": record[\"input\"],\n",
    "        \"answer\": answer,\n",
    "        \"distractors\": distractors,\n",
    "    })\n",
    "\n",
    "print(\"âœ… RAFT í…œí”Œë¦¿ ë³€í™˜ ì™„ë£Œ\")\n",
    "print(f\"   - RAFT ìƒ˜í”Œ ìˆ˜: {len(raft_records):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. í†µê³„ í™•ì¸\n",
    "ì „ì²˜ë¦¬ ê²°ê³¼ê°€ ê¸°ëŒ€í•œ ë¶„í¬ì¸ì§€ ê°„ë‹¨íˆ ì‚´í´ë´…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lengths = [len(item[\"text\"]) for item in raft_records]\n",
    "print(\"ğŸ“Š ì „ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½\")\n",
    "print(f\"  â€¢ í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {np.mean(text_lengths):.1f}\")\n",
    "print(f\"  â€¢ ì¤‘ì•™ê°’ í…ìŠ¤íŠ¸ ê¸¸ì´: {np.median(text_lengths):.1f}\")\n",
    "print(f\"  â€¢ ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´: {max(text_lengths):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ì €ì¥ ë° Google Drive ë°±ì—…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raft_dataset = Dataset.from_list(raft_records)\n",
    "SAVE_DIR = \"processed_data\"\n",
    "raft_dataset.save_to_disk(SAVE_DIR)\n",
    "\n",
    "stats = {\n",
    "    \"total_samples\": len(raft_records),\n",
    "    \"avg_text_length\": float(np.mean(text_lengths)),\n",
    "}\n",
    "\n",
    "with open(\"processed_data/metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(stats, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"ğŸ’¾ ì €ì¥ ì™„ë£Œ: processed_data/ ë””ë ‰í„°ë¦¬\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from datasets import load_from_disk\n",
    "from datasets import DatasetDict\n",
    "ds = load_from_disk(\"processed_data\")   # Dataset ë˜ëŠ” DatasetDict\n",
    "print(ds)\n",
    "\n",
    "login(token=\"\")  # HF Access Token ì…ë ¥\n",
    "\n",
    "tmp = ds.train_test_split(test_size=0.1, seed=42)\n",
    "ds = DatasetDict({\n",
    "    \"train\": tmp[\"train\"],\n",
    "    \"validation\": tmp[\"test\"]\n",
    "})\n",
    "\n",
    "repo_id = \"ryanu/raft-processed\"   # ì›í•˜ëŠ” repo ì´ë¦„\n",
    "ds.push_to_hub(repo_id, private=True)      # ê³µê°œí•˜ë ¤ë©´ private=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOGLE_DRIVE_DIR = \"/content/drive/MyDrive/exaone_day1/processed_data\"\n",
    "# try:\n",
    "#     from google.colab import drive  # type: ignore\n",
    "#     import shutil\n",
    "#     drive.mount('/content/drive')\n",
    "#     shutil.copytree(\"processed_data\", GOOGLE_DRIVE_DIR, dirs_exist_ok=True)\n",
    "#     print(f\"âœ… Google Driveì— ë°±ì—… ì™„ë£Œ: {GOOGLE_DRIVE_DIR}\")\n",
    "# except ModuleNotFoundError:\n",
    "#     print(\"âš ï¸ Colab í™˜ê²½ì´ ì•„ë‹™ë‹ˆë‹¤. í•„ìš”í•˜ë©´ ì§ì ‘ ê²½ë¡œë¥¼ ìˆ˜ì •í•´ ì €ì¥í•˜ì„¸ìš”.\")\n",
    "# except NotImplementedError:\n",
    "#     print(\"âš ï¸ ì´ í™˜ê²½ì—ì„œëŠ” drive.mount()ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Colabì—ì„œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ë‹¤ìŒ ë‹¨ê³„\n",
    "- **00.03-raft-preprocessing.ipynb** ë¡œ ì´ë™í•´ RAFT í…œí”Œë¦¿ ì˜ˆì‹œë¥¼ ë” ì‚´í´ë³´ì„¸ìš”.\n",
    "- **main-practice/02**ì—ì„œëŠ” ì €ì¥ëœ `processed_data/`ë¥¼ ë¶ˆëŸ¬ì™€ í’ˆì§ˆ ê²€ì¦ì„ ì§„í–‰í•©ë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
