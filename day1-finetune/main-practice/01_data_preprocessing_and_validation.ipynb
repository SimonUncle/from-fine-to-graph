{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 Day 1 실습 1: 데이터 전처리 및 검증\n",
    "\n",
    "## 학습 목표\n",
    "- 한국어 데이터셋을 활용한 RAG 파인튜닝 데이터 준비\n",
    "- RAFT(Retrieval Augmented Fine Tuning) 방법론 적용\n",
    "- Instruction/Input/Output 스키마 설계\n",
    "- 데이터 전처리 규칙 적용 (공백/제어문자/중복/길이/PII 처리)\n",
    "- 템플릿 고정 및 표준화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 확인 및 설치 (순서대로 진행되는 실습 고려)\n",
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def check_and_install_package(package_name, import_name=None, version=None):\n",
    "    \"\"\"\n",
    "    패키지 존재 여부 확인 후 필요시에만 설치\n",
    "    \"\"\"\n",
    "    if import_name is None:\n",
    "        import_name = package_name.replace('-', '_')\n",
    "    \n",
    "    try:\n",
    "        module = importlib.import_module(import_name)\n",
    "        print(f\"✅ {package_name} 이미 설치됨\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(f\"📦 {package_name} 설치 중...\")\n",
    "        try:\n",
    "            if version:\n",
    "                subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", f\"{package_name}=={version}\"], \n",
    "                             check=True, capture_output=True)\n",
    "            else:\n",
    "                subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package_name], \n",
    "                             check=True, capture_output=True)\n",
    "            print(f\"✅ {package_name} 설치 완료\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"❌ {package_name} 설치 실패: {e}\")\n",
    "            return False\n",
    "\n",
    "print(\"🚀 Day 1 실습 1: 데이터 전처리 및 검증\")\n",
    "print(\"🔍 필요한 라이브러리 확인 중...\")\n",
    "\n",
    "# 01번 노트북에서 필요한 패키지들\n",
    "packages = [\n",
    "    (\"datasets\", \"datasets\"),\n",
    "    (\"transformers\", \"transformers\"), \n",
    "    (\"torch\", \"torch\"),\n",
    "    (\"jsonlines\", \"jsonlines\"),\n",
    "    (\"pandas\", \"pandas\"),\n",
    "    (\"numpy\", \"numpy\"),\n",
    "    (\"matplotlib\", \"matplotlib\"),\n",
    "    (\"seaborn\", \"seaborn\"),\n",
    "    (\"tqdm\", \"tqdm\"),\n",
    "    (\"scikit-learn\", \"sklearn\")\n",
    "]\n",
    "\n",
    "print(\"📋 패키지 확인 결과:\")\n",
    "for package_name, import_name in packages:\n",
    "    check_and_install_package(package_name, import_name)\n",
    "\n",
    "print(\"\\n🎉 라이브러리 준비 완료!\")\n",
    "print(\"💡 다음 셀부터 데이터 전처리를 시작합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from typing import Dict, List, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트 설정 (matplotlib) - 그래프에서 한글이 깨지지 않도록 설정\n",
    "# Colab 환경에서 나눔 글꼴을 설치하고 matplotlib에 적용\n",
    "print(\"🔧 한글 폰트 설정 중...\")\n",
    "!apt-get update -qq\n",
    "!apt-get install fonts-nanum -qq > /dev/null\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# 나눔바른고딕 폰트 경로 설정\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "# 폰트 매니저에 폰트 추가 - 그래프에서 한글 표시를 위해 필요\n",
    "fm.fontManager.addfont(fontpath)\n",
    "\n",
    "# matplotlib 설정 업데이트 - 모든 그래프에서 한글이 정상적으로 표시됨\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'NanumBarunGothic',  # 기본 폰트를 나눔바른고딕으로 설정\n",
    "    'axes.unicode_minus': False         # 음수 기호 표시 문제 해결\n",
    "})\n",
    "\n",
    "print(\"✅ 한글 폰트 설정 완료 - 그래프에서 한글이 정상 표시됩니다\")\n",
    "print(\"📦 라이브러리 import 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 한국어 데이터셋 로드 및 탐색\n",
    "\n",
    "### 📋 사용할 데이터셋\n",
    "- **주요 데이터셋**: `neural-bridge/rag-dataset-12000` (Context-Question-Answer 구조)\n",
    "- **보조 데이터셋**: `maywell/ko_wikidata_QA` (한국어 QA)\n",
    "- **RAFT 전처리**: RAG 성능 향상을 위한 context 기반 학습 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 데이터셋 로드 (Context-Question-Answer 구조)\n",
    "print(\"🔄 RAG 데이터셋 로드 중...\")\n",
    "\n",
    "try:\n",
    "    # RAG 데이터셋 로드 (이미 context가 있는 구조)\n",
    "    rag_dataset = load_dataset(\"neural-bridge/rag-dataset-12000\", split=\"train\")\n",
    "    print(f\"✅ RAG 데이터셋 로드 완료: {len(rag_dataset)}개 샘플\")\n",
    "    \n",
    "    # 샘플 데이터 확인\n",
    "    print(\"\\n📋 데이터 구조:\")\n",
    "    sample = rag_dataset[0]\n",
    "    print(f\"컬럼: {list(sample.keys())}\")\n",
    "    for key, value in sample.items():\n",
    "        if isinstance(value, str):\n",
    "            print(f\"{key}: {value[:100]}{'...' if len(value) > 100 else ''}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 데이터셋 로드 실패: {e}\")\n",
    "    print(\"🔄 대안 데이터셋 사용...\")\n",
    "    \n",
    "    # 대안: 간단한 한글 RAG 데이터셋 생성\n",
    "    sample_data = [\n",
    "        {\n",
    "            \"question\": \"대한민국의 수도는 어디인가요?\",\n",
    "            \"context\": \"대한민국의 수도는 서울특별시입니다. 서울은 한강을 중심으로 발달한 도시로, 약 950만 명의 인구가 거주하고 있습니다.\",\n",
    "            \"answer\": \"대한민국의 수도는 서울특별시입니다.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"김치의 주요 재료는 무엇인가요?\",\n",
    "            \"context\": \"김치는 한국의 전통 발효식품입니다. 김치의 주요 재료는 배추, 고춧가루, 마늘, 생강, 젓갈 등으로 구성됩니다. 발효 과정을 통해 유산균이 풍부해집니다.\",\n",
    "            \"answer\": \"김치의 주요 재료는 배추, 고춧가루, 마늘, 생강, 젓갈 등입니다.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"한국의 전통 음식에는 어떤 것들이 있나요?\",\n",
    "            \"context\": \"한국의 전통 음식으로는 김치, 비빔밥, 불고기, 갈비, 냉면, 삼계탕 등이 있습니다. 이들 음식은 발효, 조림, 구이 등 다양한 조리법을 사용합니다.\",\n",
    "            \"answer\": \"한국의 전통 음식으로는 김치, 비빔밥, 불고기, 갈비, 냉면, 삼계탕 등이 있습니다.\"\n",
    "        }\n",
    "    ]\n",
    "    rag_dataset = Dataset.from_list(sample_data)\n",
    "    print(f\"✅ 샘플 데이터셋 생성: {len(rag_dataset)}개 샘플\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 스키마 정의 및 검증\n",
    "\n",
    "### 🏗️ Instruction/Input/Output 스키마\n",
    "- **Instruction**: 모델이 수행해야 할 작업 설명\n",
    "- **Input**: 작업에 필요한 입력 정보 (질문 + context)\n",
    "- **Output**: 기대하는 출력 결과 (정답)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data_schema(dataset: Dataset) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    데이터셋 스키마 검증 함수\n",
    "    \n",
    "    Args:\n",
    "        dataset: 검증할 데이터셋\n",
    "        \n",
    "    Returns:\n",
    "        검증 결과 딕셔너리\n",
    "    \"\"\"\n",
    "    print(\"🔍 데이터 스키마 검증 중...\")\n",
    "    \n",
    "    # 첫 번째 샘플로 컬럼 확인\n",
    "    if len(dataset) == 0:\n",
    "        return {\"error\": \"빈 데이터셋\"}\n",
    "    \n",
    "    sample = dataset[0]\n",
    "    columns = list(sample.keys())\n",
    "    total_samples = len(dataset)\n",
    "    \n",
    "    # 결측치 및 빈 값 확인\n",
    "    missing_values = {}\n",
    "    empty_values = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        missing_count = 0\n",
    "        empty_count = 0\n",
    "        \n",
    "        for item in dataset:\n",
    "            value = item.get(col)\n",
    "            if value is None:\n",
    "                missing_count += 1\n",
    "            elif isinstance(value, str) and value.strip() == \"\":\n",
    "                empty_count += 1\n",
    "        \n",
    "        missing_values[col] = missing_count\n",
    "        empty_values[col] = empty_count\n",
    "    \n",
    "    return {\n",
    "        \"total_samples\": total_samples,\n",
    "        \"columns\": columns,\n",
    "        \"missing_values\": missing_values,\n",
    "        \"empty_values\": empty_values\n",
    "    }\n",
    "\n",
    "# 스키마 검증 실행\n",
    "validation_result = validate_data_schema(rag_dataset)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"\\n📊 검증 결과:\")\n",
    "print(f\"총 샘플 수: {validation_result['total_samples']}\")\n",
    "print(f\"컬럼: {validation_result['columns']}\")\n",
    "print(f\"\\n결측치 현황:\")\n",
    "for col, count in validation_result['missing_values'].items():\n",
    "    print(f\"  {col}: {count}개\")\n",
    "\n",
    "print(f\"\\n빈 값 현황:\")\n",
    "for col, count in validation_result['empty_values'].items():\n",
    "    print(f\"  {col}: {count}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 데이터 전처리 규칙 적용\n",
    "\n",
    "### 🧹 전처리 규칙\n",
    "1. **공백/제어문자 정리**: 불필요한 공백, 줄바꿈, 특수문자 제거\n",
    "2. **중복 제거**: 동일한 instruction-input 조합 제거\n",
    "3. **길이 제한**: 너무 길거나 짧은 샘플 필터링\n",
    "4. **PII 제거**: 개인정보 패턴 탐지 및 마스킹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    텍스트 정리 함수: 불필요한 공백, 제어문자 등 제거\n",
    "    \n",
    "    Args:\n",
    "        text: 정리할 텍스트\n",
    "        \n",
    "    Returns:\n",
    "        정리된 텍스트\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 제어문자 제거\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', text)\n",
    "    \n",
    "    # 연속된 공백을 하나로 변환\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 앞뒤 공백 제거\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_pii(text: str) -> str:\n",
    "    \"\"\"\n",
    "    개인정보 제거/마스킹 함수\n",
    "    \n",
    "    Args:\n",
    "        text: PII 제거할 텍스트\n",
    "        \n",
    "    Returns:\n",
    "        PII가 마스킹된 텍스트\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 전화번호 패턴 마스킹\n",
    "    text = re.sub(r'\\d{2,3}-\\d{3,4}-\\d{4}', '[전화번호]', text)\n",
    "    text = re.sub(r'\\d{3}\\d{4}\\d{4}', '[전화번호]', text)\n",
    "    \n",
    "    # 이메일 패턴 마스킹\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '[이메일]', text)\n",
    "    \n",
    "    # 주민등록번호 패턴 마스킹 (예: 123456-1234567)\n",
    "    text = re.sub(r'\\d{6}-[1-4]\\d{6}', '[주민등록번호]', text)\n",
    "    \n",
    "    # 계좌번호 패턴 마스킹 (예: 123-456-789012)\n",
    "    text = re.sub(r'\\d{3}-\\d{2,3}-\\d{6,8}', '[계좌번호]', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_rag_dataset(dataset: Dataset, \n",
    "                         min_length: int = 10, \n",
    "                         max_length: int = 2048) -> Dataset:\n",
    "    \"\"\"\n",
    "    RAG 데이터셋 전처리 함수\n",
    "    \n",
    "    Args:\n",
    "        dataset: 원본 RAG 데이터셋 (context, question, answer 구조)\n",
    "        min_length: 최소 텍스트 길이\n",
    "        max_length: 최대 텍스트 길이\n",
    "        \n",
    "    Returns:\n",
    "        전처리된 데이터셋\n",
    "    \"\"\"\n",
    "    print(\"🧹 RAG 데이터 전처리 시작...\")\n",
    "    \n",
    "    processed_data = []\n",
    "    seen_combinations = set()  # 중복 제거용\n",
    "    \n",
    "    for item in tqdm(dataset, desc=\"전처리 중\"):\n",
    "        # 텍스트 정리 (RAG 데이터 구조에 맞춤)\n",
    "        question = clean_text(item.get('question', ''))\n",
    "        context = clean_text(item.get('context', ''))\n",
    "        answer = clean_text(item.get('answer', ''))\n",
    "        \n",
    "        # PII 제거\n",
    "        question = remove_pii(question)\n",
    "        context = remove_pii(context)\n",
    "        answer = remove_pii(answer)\n",
    "        \n",
    "        # 길이 필터링\n",
    "        if (len(answer) < min_length or \n",
    "            len(context) > max_length or\n",
    "            len(question) < 5):\n",
    "            continue\n",
    "        \n",
    "        # 중복 확인 (question + context 조합)\n",
    "        combination = f\"{question}|{context}\"\n",
    "        if combination in seen_combinations:\n",
    "            continue\n",
    "        seen_combinations.add(combination)\n",
    "        \n",
    "        processed_data.append({\n",
    "            'question': question,\n",
    "            'context': context,\n",
    "            'answer': answer\n",
    "        })\n",
    "    \n",
    "    print(f\"✅ 전처리 완료: {len(dataset)} → {len(processed_data)}개 샘플\")\n",
    "    return Dataset.from_list(processed_data)\n",
    "\n",
    "# 전처리 실행\n",
    "processed_dataset = preprocess_rag_dataset(rag_dataset)\n",
    "print(f\"\\n📊 전처리 결과: {len(processed_dataset)}개 샘플\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAFT 방법론 적용\n",
    "\n",
    "### 🎯 RAFT (Retrieval Augmented Fine Tuning) 개념\n",
    "- **목적**: RAG 시스템의 성능을 향상시키기 위한 특수한 파인튜닝 방법\n",
    "- **핵심 아이디어**: 모델이 관련 context를 잘 활용하도록 학습\n",
    "- **방법**: Positive/Negative 샘플을 통한 대조 학습\n",
    "\n",
    "### 📋 RAFT 데이터 구조\n",
    "- **Positive 샘플** (60%): 정답 context + 3개 distractor context\n",
    "- **Negative 샘플** (40%): 4개 distractor context (정답 없음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raft_dataset_from_rag(dataset: Dataset, \n",
    "                                num_samples: int = 1000,\n",
    "                                positive_ratio: float = 0.6) -> Dataset:\n",
    "    \"\"\"\n",
    "    RAG 데이터셋으로부터 RAFT 스타일 데이터셋 생성 함수\n",
    "    \n",
    "    Args:\n",
    "        dataset: 원본 RAG 데이터셋 (question, context, answer 구조)\n",
    "        num_samples: 생성할 총 샘플 수\n",
    "        positive_ratio: positive 샘플 비율\n",
    "        \n",
    "    Returns:\n",
    "        RAFT 형태로 변환된 데이터셋\n",
    "    \"\"\"\n",
    "    print(\"🔄 RAFT 데이터셋 생성 중...\")\n",
    "    \n",
    "    # 샘플 수 제한\n",
    "    if len(dataset) > num_samples:\n",
    "        dataset = dataset.shuffle(seed=42).select(range(num_samples))\n",
    "    \n",
    "    num_positive = int(len(dataset) * positive_ratio)\n",
    "    num_negative = len(dataset) - num_positive\n",
    "    \n",
    "    # 모든 context를 context pool로 사용\n",
    "    all_contexts = [item['context'] for item in dataset]\n",
    "    \n",
    "    raft_data = []\n",
    "    used_indices = set()\n",
    "    \n",
    "    # Positive 샘플 생성\n",
    "    print(f\"✅ Positive 샘플 생성: {num_positive}개\")\n",
    "    positive_count = 0\n",
    "    \n",
    "    for i in tqdm(range(len(dataset)), desc=\"Positive 샘플 생성\"):\n",
    "        if positive_count >= num_positive:\n",
    "            break\n",
    "            \n",
    "        if i in used_indices:\n",
    "            continue\n",
    "            \n",
    "        item = dataset[i]\n",
    "        correct_context = item['context']\n",
    "        question = item['question']\n",
    "        answer = item['answer']\n",
    "        \n",
    "        # 3개의 distractor 선택 (정답이 아닌 다른 context들)\n",
    "        distractors = random.sample(\n",
    "            [ctx for j, ctx in enumerate(all_contexts) if j != i and ctx != correct_context], \n",
    "            min(3, len(all_contexts) - 1)\n",
    "        )\n",
    "        \n",
    "        # context 리스트 구성 (정답 context를 랜덤 위치에 배치)\n",
    "        contexts = [correct_context] + distractors\n",
    "        random.shuffle(contexts)\n",
    "        \n",
    "        raft_data.append({\n",
    "            'type': 'positive',\n",
    "            'question': question,\n",
    "            'contexts': contexts,\n",
    "            'answer': answer,\n",
    "            'instruction': \"주어진 컨텍스트를 바탕으로 질문에 답변해주세요.\"\n",
    "        })\n",
    "        \n",
    "        used_indices.add(i)\n",
    "        positive_count += 1\n",
    "    \n",
    "    # Negative 샘플 생성\n",
    "    print(f\"✅ Negative 샘플 생성: {num_negative}개\")\n",
    "    negative_count = 0\n",
    "    \n",
    "    for i in tqdm(range(len(dataset)), desc=\"Negative 샘플 생성\"):\n",
    "        if negative_count >= num_negative:\n",
    "            break\n",
    "            \n",
    "        if i in used_indices:\n",
    "            continue\n",
    "            \n",
    "        item = dataset[i]\n",
    "        question = item['question']\n",
    "        correct_answer = item['answer']\n",
    "        correct_context = item['context']\n",
    "        \n",
    "        # 4개의 distractor만 선택 (정답 context 제외)\n",
    "        distractors = random.sample(\n",
    "            [ctx for j, ctx in enumerate(all_contexts) if j != i and ctx != correct_context],\n",
    "            min(4, len(all_contexts) - 1)\n",
    "        )\n",
    "        \n",
    "        raft_data.append({\n",
    "            'type': 'negative', \n",
    "            'question': question,\n",
    "            'contexts': distractors,\n",
    "            'answer': correct_answer,  # 정답은 있지만 context에는 없음\n",
    "            'instruction': \"주어진 컨텍스트를 바탕으로 질문에 답변해주세요.\"\n",
    "        })\n",
    "        \n",
    "        used_indices.add(i)\n",
    "        negative_count += 1\n",
    "    \n",
    "    # 데이터 섞기\n",
    "    random.shuffle(raft_data)\n",
    "    \n",
    "    print(f\"✅ RAFT 데이터셋 생성 완료: {len(raft_data)}개 샘플\")\n",
    "    print(f\"   - Positive: {positive_count}개\")\n",
    "    print(f\"   - Negative: {negative_count}개\")\n",
    "    \n",
    "    return Dataset.from_list(raft_data)\n",
    "\n",
    "# RAFT 데이터셋 생성\n",
    "raft_dataset = create_raft_dataset_from_rag(processed_dataset, num_samples=500)\n",
    "\n",
    "# 샘플 확인\n",
    "print(\"\\n📋 RAFT 데이터 샘플:\")\n",
    "sample = raft_dataset[0]\n",
    "print(f\"Type: {sample['type']}\")\n",
    "print(f\"Question: {sample['question'][:100]}...\")\n",
    "print(f\"Contexts: {len(sample['contexts'])}개\")\n",
    "print(f\"Answer: {sample['answer'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 템플릿 설계 및 고정\n",
    "\n",
    "### 📝 EXAONE 모델용 Chat Template\n",
    "- **System Role**: 모델의 역할 정의\n",
    "- **User Role**: 사용자 입력 (질문 + contexts)\n",
    "- **Assistant Role**: 모델 응답 (답변)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_template(item: Dict[str, Any]) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    EXAONE 모델용 채팅 템플릿 생성\n",
    "    \n",
    "    Args:\n",
    "        item: RAFT 데이터 아이템\n",
    "        \n",
    "    Returns:\n",
    "        채팅 메시지 리스트\n",
    "    \"\"\"\n",
    "    # System 메시지\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"당신은 주어진 컨텍스트를 바탕으로 질문에 정확하고 도움이 되는 답변을 제공하는 AI 어시스턴트입니다. \"\n",
    "            \"컨텍스트에서 관련 정보를 찾아 답변하되, 정보가 없다면 모른다고 솔직히 답변하세요.\"\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # User 메시지 - 구조화된 입력\n",
    "    contexts_text = \"\\n\\n\".join([f\"컨텍스트 {i+1}: {ctx}\" for i, ctx in enumerate(item['contexts'])])\n",
    "    \n",
    "    user_content = f\"\"\"다음 컨텍스트들을 참고하여 질문에 답변해주세요.\n",
    "\n",
    "=== 컨텍스트 ===\n",
    "{contexts_text}\n",
    "\n",
    "=== 질문 ===\n",
    "{item['question']}\n",
    "\n",
    "=== 요청사항 ===\n",
    "{item['instruction']}\"\"\"\n",
    "    \n",
    "    user_message = {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": user_content\n",
    "    }\n",
    "    \n",
    "    # Assistant 메시지 (학습시에만 포함)\n",
    "    assistant_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": item['answer']\n",
    "    }\n",
    "    \n",
    "    return [system_message, user_message, assistant_message]\n",
    "\n",
    "def apply_chat_template_to_dataset(dataset: Dataset) -> Dataset:\n",
    "    \"\"\"\n",
    "    데이터셋 전체에 채팅 템플릿 적용\n",
    "    \n",
    "    Args:\n",
    "        dataset: RAFT 데이터셋\n",
    "        \n",
    "    Returns:\n",
    "        템플릿이 적용된 데이터셋\n",
    "    \"\"\"\n",
    "    print(\"🔄 채팅 템플릿 적용 중...\")\n",
    "    \n",
    "    templated_data = []\n",
    "    \n",
    "    for item in tqdm(dataset, desc=\"템플릿 적용\"):\n",
    "        chat_messages = create_chat_template(item)\n",
    "        \n",
    "        templated_item = {\n",
    "            'messages': chat_messages,\n",
    "            'type': item['type'],\n",
    "            'original_question': item['question'],\n",
    "            'original_answer': item['answer']\n",
    "        }\n",
    "        templated_data.append(templated_item)\n",
    "    \n",
    "    print(f\"✅ 템플릿 적용 완료: {len(templated_data)}개 샘플\")\n",
    "    return Dataset.from_list(templated_data)\n",
    "\n",
    "# 템플릿 적용\n",
    "templated_dataset = apply_chat_template_to_dataset(raft_dataset)\n",
    "\n",
    "# 템플릿 적용 결과 확인\n",
    "print(\"\\n📋 템플릿 적용 결과:\")\n",
    "sample = templated_dataset[0]\n",
    "print(f\"Type: {sample['type']}\")\n",
    "print(f\"Messages: {len(sample['messages'])}개\")\n",
    "print(f\"\\n첫 번째 메시지:\")\n",
    "print(f\"Role: {sample['messages'][0]['role']}\")\n",
    "print(f\"Content: {sample['messages'][0]['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 토크나이저 로드 및 길이 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAONE 토크나이저 로드\n",
    "print(\"🔄 EXAONE 토크나이저 로드 중...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\")\n",
    "\n",
    "def analyze_token_lengths(dataset: Dataset, tokenizer, max_length: int = 4096) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    토큰 길이 분석 함수\n",
    "    \n",
    "    Args:\n",
    "        dataset: 분석할 데이터셋\n",
    "        tokenizer: 사용할 토크나이저\n",
    "        max_length: 최대 토큰 길이\n",
    "        \n",
    "    Returns:\n",
    "        토큰 길이 분석 결과\n",
    "    \"\"\"\n",
    "    print(\"📊 토큰 길이 분석 중...\")\n",
    "    \n",
    "    token_lengths = []\n",
    "    overflow_count = 0\n",
    "    \n",
    "    for item in tqdm(dataset, desc=\"토큰 길이 계산\"):\n",
    "        # 전체 대화를 하나의 텍스트로 변환\n",
    "        full_text = tokenizer.apply_chat_template(\n",
    "            item['messages'], \n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        \n",
    "        # 토큰 개수 계산\n",
    "        tokens = tokenizer.encode(full_text)\n",
    "        token_length = len(tokens)\n",
    "        token_lengths.append(token_length)\n",
    "        \n",
    "        if token_length > max_length:\n",
    "            overflow_count += 1\n",
    "    \n",
    "    analysis_result = {\n",
    "        \"total_samples\": len(token_lengths),\n",
    "        \"mean_length\": np.mean(token_lengths),\n",
    "        \"median_length\": np.median(token_lengths),\n",
    "        \"min_length\": np.min(token_lengths),\n",
    "        \"max_length\": np.max(token_lengths),\n",
    "        \"std_length\": np.std(token_lengths),\n",
    "        \"overflow_count\": overflow_count,\n",
    "        \"overflow_rate\": overflow_count / len(token_lengths),\n",
    "        \"token_lengths\": token_lengths\n",
    "    }\n",
    "    \n",
    "    return analysis_result\n",
    "\n",
    "# 토큰 길이 분석 실행\n",
    "token_analysis = analyze_token_lengths(templated_dataset, tokenizer)\n",
    "\n",
    "# 분석 결과 출력\n",
    "print(f\"\\n📊 토큰 길이 분석 결과:\")\n",
    "print(f\"총 샘플 수: {token_analysis['total_samples']}개\")\n",
    "print(f\"평균 길이: {token_analysis['mean_length']:.1f} 토큰\")\n",
    "print(f\"중간값: {token_analysis['median_length']:.1f} 토큰\")\n",
    "print(f\"최소 길이: {token_analysis['min_length']} 토큰\")\n",
    "print(f\"최대 길이: {token_analysis['max_length']} 토큰\")\n",
    "print(f\"표준편차: {token_analysis['std_length']:.1f} 토큰\")\n",
    "print(f\"4096 토큰 초과: {token_analysis['overflow_count']}개 ({token_analysis['overflow_rate']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train/Valid 분할 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Valid 분할 (8:2 비율)\n",
    "print(\"🔄 Train/Valid 분할 중...\")\n",
    "\n",
    "train_indices, valid_indices = train_test_split(\n",
    "    list(range(len(templated_dataset))),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=[item['type'] for item in templated_dataset]  # positive/negative 비율 유지\n",
    ")\n",
    "\n",
    "train_dataset = templated_dataset.select(train_indices)\n",
    "valid_dataset = templated_dataset.select(valid_indices)\n",
    "\n",
    "print(f\"✅ 분할 완료:\")\n",
    "print(f\"  - Train: {len(train_dataset)}개 샘플\")\n",
    "print(f\"  - Valid: {len(valid_dataset)}개 샘플\")\n",
    "\n",
    "# 분할 결과 확인\n",
    "train_types = [item['type'] for item in train_dataset]\n",
    "valid_types = [item['type'] for item in valid_dataset]\n",
    "\n",
    "print(f\"\\n📊 분할 결과 확인:\")\n",
    "print(f\"Train - Positive: {train_types.count('positive')}개, Negative: {train_types.count('negative')}개\")\n",
    "print(f\"Valid - Positive: {valid_types.count('positive')}개, Negative: {valid_types.count('negative')}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장 디렉토리 생성\n",
    "import os\n",
    "os.makedirs('processed_data', exist_ok=True)\n",
    "\n",
    "# 데이터셋을 JSONL 형식으로 저장\n",
    "def save_dataset_jsonl(dataset: Dataset, file_path: str):\n",
    "    \"\"\"데이터셋을 JSONL 형식으로 저장\"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for item in dataset:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Train/Valid 데이터셋 저장\n",
    "print(\"💾 데이터셋 저장 중...\")\n",
    "save_dataset_jsonl(train_dataset, 'processed_data/train_raft_ko.jsonl')\n",
    "save_dataset_jsonl(valid_dataset, 'processed_data/valid_raft_ko.jsonl')\n",
    "\n",
    "# 메타데이터 저장\n",
    "metadata = {\n",
    "    \"dataset_info\": {\n",
    "        \"total_samples\": len(templated_dataset),\n",
    "        \"train_samples\": len(train_dataset),\n",
    "        \"valid_samples\": len(valid_dataset),\n",
    "        \"positive_samples\": len([item for item in templated_dataset if item['type'] == 'positive']),\n",
    "        \"negative_samples\": len([item for item in templated_dataset if item['type'] == 'negative'])\n",
    "    },\n",
    "    \"token_analysis\": {\n",
    "        \"mean_length\": float(token_analysis['mean_length']),\n",
    "        \"median_length\": float(token_analysis['median_length']),\n",
    "        \"max_length\": int(token_analysis['max_length']),\n",
    "        \"overflow_count\": int(token_analysis['overflow_count']),\n",
    "        \"overflow_rate\": float(token_analysis['overflow_rate'])\n",
    "    },\n",
    "    \"processing_info\": {\n",
    "        \"source_dataset\": \"neural-bridge/rag-dataset-12000\",\n",
    "        \"preprocessing\": \"RAFT + Chat Template\",\n",
    "        \"model_target\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('processed_data/metadata.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ 데이터셋 저장 완료:\")\n",
    "print(f\"  - processed_data/train_raft_ko.jsonl\")\n",
    "print(f\"  - processed_data/valid_raft_ko.jsonl\")\n",
    "print(f\"  - processed_data/metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 데이터 품질 검증 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 길이 분포 및 데이터 품질 종합 시각화\n",
    "# 이 차트들은 RAFT 데이터셋의 품질과 특성을 한눈에 파악할 수 있게 해줍니다\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# 2x2 서브플롯 구성 - 4개의 다른 관점에서 데이터를 분석\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. 토큰 길이 분포 히스토그램 \n",
    "# 📊 의미: 모델 입력 길이의 분포를 보여줌. 대부분의 데이터가 4096 토큰 이하인지 확인\n",
    "ax1.hist(token_analysis['token_lengths'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.axvline(token_analysis['mean_length'], color='red', linestyle='--', \n",
    "           label=f'평균 길이: {token_analysis[\"mean_length\"]:.1f} 토큰')\n",
    "ax1.axvline(4096, color='orange', linestyle='--', label='모델 최대 길이: 4096 토큰')\n",
    "ax1.set_xlabel('토큰 길이 (개)')\n",
    "ax1.set_ylabel('샘플 빈도수')\n",
    "ax1.set_title('📏 토큰 길이 분포\\n(대부분 샘플이 모델 제한 내에 있는지 확인)', fontsize=12, pad=15)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 분포 해석을 위한 추가 정보 표시\n",
    "ax1.text(0.7, 0.9, f'중간값: {token_analysis[\"median_length\"]:.0f}\\n'\n",
    "                   f'표준편차: {token_analysis[\"std_length\"]:.0f}\\n'\n",
    "                   f'오버플로우: {token_analysis[\"overflow_count\"]}개', \n",
    "         transform=ax1.transAxes, fontsize=9, \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"wheat\", alpha=0.8))\n",
    "\n",
    "# 2. RAFT Positive/Negative 샘플 비율 파이차트\n",
    "# 📊 의미: RAFT 방법론의 핵심인 positive/negative 샘플 균형 확인\n",
    "# - Positive (60%): 정답이 포함된 context로 학습 (정확한 정보 활용 학습)\n",
    "# - Negative (40%): 정답이 없는 context로 학습 (모른다고 답하는 법 학습)\n",
    "type_counts = [train_types.count('positive'), train_types.count('negative')]\n",
    "colors = ['lightcoral', 'lightblue']\n",
    "wedges, texts, autotexts = ax2.pie(type_counts, labels=['Positive 샘플', 'Negative 샘플'], \n",
    "                                  autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "ax2.set_title('🎯 RAFT 샘플 비율\\n(RAG 성능 향상을 위한 대조 학습)', fontsize=12, pad=15)\n",
    "\n",
    "# 파이차트 텍스트 크기 조정\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "# 3. 토큰 길이 상세 분포 (박스플롯)\n",
    "# 📊 의미: 토큰 길이의 사분위수, 이상값 등을 보여줌\n",
    "# - 중앙값, 1/3분위수로 데이터의 집중도 파악\n",
    "# - 이상값(outlier)으로 비정상적으로 긴 샘플 식별\n",
    "box_plot = ax3.boxplot(token_analysis['token_lengths'], patch_artist=True)\n",
    "box_plot['boxes'][0].set_facecolor('lightgreen')\n",
    "ax3.set_ylabel('토큰 길이 (개)')\n",
    "ax3.set_title('📦 토큰 길이 상세 분포\\n(사분위수와 이상값 분석)', fontsize=12, pad=15)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 박스플롯 해석을 위한 주석 추가\n",
    "q1 = np.percentile(token_analysis['token_lengths'], 25)\n",
    "q3 = np.percentile(token_analysis['token_lengths'], 75)\n",
    "ax3.text(1.1, 0.8, f'Q1 (25%): {q1:.0f}\\n'\n",
    "                   f'중앙값: {token_analysis[\"median_length\"]:.0f}\\n'\n",
    "                   f'Q3 (75%): {q3:.0f}', \n",
    "         transform=ax3.transAxes, fontsize=9,\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.3))\n",
    "\n",
    "# 4. Context 개수별 분포 막대그래프\n",
    "# 📊 의미: 각 샘플당 제공되는 context 개수 분포\n",
    "# - RAFT에서는 보통 4개 context 사용 (1개 정답 + 3개 distractor 또는 4개 distractor)\n",
    "context_counts = [len(item['contexts']) for item in raft_dataset]\n",
    "context_count_freq = pd.Series(context_counts).value_counts().sort_index()\n",
    "bars = ax4.bar(context_count_freq.index, context_count_freq.values, \n",
    "               alpha=0.7, color='mediumpurple', edgecolor='black')\n",
    "ax4.set_xlabel('샘플당 Context 개수')\n",
    "ax4.set_ylabel('샘플 수')\n",
    "ax4.set_title('📚 Context 개수별 분포\\n(RAFT 구조의 일관성 확인)', fontsize=12, pad=15)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 막대그래프 위에 값 표시\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{int(height)}개', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 전체 레이아웃 조정 및 저장\n",
    "plt.tight_layout(pad=3.0)  # 서브플롯 간격 조정\n",
    "plt.savefig('processed_data/data_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 데이터 품질 분석 차트 저장 완료: processed_data/data_analysis.png\")\n",
    "print(\"\\n🔍 차트 해석 가이드:\")\n",
    "print(\"  📏 토큰 길이 분포: 대부분 샘플이 모델 최대 길이(4096) 이내인지 확인\")\n",
    "print(\"  🎯 RAFT 샘플 비율: Positive(60%) vs Negative(40%) 균형으로 대조 학습 효과 극대화\")\n",
    "print(\"  📦 토큰 길이 박스플롯: 데이터 집중도와 이상값으로 품질 문제 파악\")  \n",
    "print(\"  📚 Context 개수 분포: RAFT 구조의 일관성 확인 (보통 3-4개)\")\n",
    "print(\"\\n💡 이 차트들을 통해 다음 실습에서 사용할 데이터의 품질을 사전 검증했습니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ 완료된 작업\n",
    "1. **RAG 데이터셋 로드**: neural-bridge/rag-dataset-12000 활용\n",
    "2. **데이터 전처리**: 공백/제어문자/중복/길이/PII 처리\n",
    "3. **RAFT 방법론 적용**: Positive/Negative 샘플 생성\n",
    "4. **템플릿 표준화**: EXAONE 모델용 Chat Template\n",
    "5. **토큰 길이 검증**: 4096 토큰 제한 준수 확인\n",
    "6. **Train/Valid 분할**: 8:2 비율, 균등 분할\n",
    "7. **데이터 저장**: JSONL 형태로 저장\n",
    "8. **품질 검증**: 시각화 및 통계 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 검증 및 요약\n",
    "print(\"🎯 Day 1 실습 1 완료 요약\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"📁 저장된 파일:\")\n",
    "print(f\"  - processed_data/train_raft_ko.jsonl ({len(train_dataset)}개 샘플)\")\n",
    "print(f\"  - processed_data/valid_raft_ko.jsonl ({len(valid_dataset)}개 샘플)\")\n",
    "print(f\"  - processed_data/metadata.json\")\n",
    "print(f\"  - processed_data/data_analysis.png\")\n",
    "\n",
    "print(f\"\\n📊 데이터 품질 지표:\")\n",
    "print(f\"  - 평균 토큰 길이: {token_analysis['mean_length']:.1f}\")\n",
    "print(f\"  - 최대 토큰 길이: {token_analysis['max_length']}\")\n",
    "print(f\"  - 4096 토큰 초과율: {token_analysis['overflow_rate']:.1%}\")\n",
    "print(f\"  - Positive 샘플: {train_types.count('positive')}개\")\n",
    "print(f\"  - Negative 샘플: {train_types.count('negative')}개\")\n",
    "\n",
    "print(f\"\\n✅ 다음 실습에서 사용할 데이터가 준비되었습니다!\")\n",
    "print(f\"   02_data_quality_check.ipynb에서 더 자세한 품질 검증을 진행합니다.\")\n",
    "\n",
    "# 샘플 데이터 미리보기\n",
    "print(f\"\\n📋 생성된 데이터 샘플:\")\n",
    "sample_item = train_dataset[0]\n",
    "print(f\"Type: {sample_item['type']}\")\n",
    "print(f\"System: {sample_item['messages'][0]['content'][:100]}...\")\n",
    "print(f\"User: {sample_item['messages'][1]['content'][:200]}...\")\n",
    "print(f\"Assistant: {sample_item['messages'][2]['content'][:100]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
