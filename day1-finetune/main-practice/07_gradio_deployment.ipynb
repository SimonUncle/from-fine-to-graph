{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 ì‹¤ìŠµ 7: Gradioë¡œ ëª¨ë¸ ë°°í¬í•˜ê¸°\n",
    "\n",
    "## ğŸ¯ ì´ ë…¸íŠ¸ë¶ì˜ ëª©ì \n",
    "\n",
    "6ë²ˆì—ì„œ í•™ìŠµí•œ LoRA ëª¨ë¸ì„ **Gradio UI**ë¡œ ë§Œë“¤ê³  **HuggingFace Spaces**ì— ë°°í¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ë°°í¬ ëŒ€ìƒ (ì˜ˆì‹œ)\n",
    "\n",
    "ê°•ì‚¬ ë°ëª¨:\n",
    "- âœ… **ê°ì • ë¶„ë¥˜ ëª¨ë¸** (6ë²ˆì—ì„œ í•™ìŠµ)\n",
    "- âœ… **ì˜ì–´ QA ëª¨ë¸** (6ë²ˆì—ì„œ í•™ìŠµ)\n",
    "\n",
    "í•™ìƒ ì—¬ëŸ¬ë¶„:\n",
    "- ğŸ”§ **TODOë§Œ ì±„ì›Œì„œ** ë³¸ì¸ ëª¨ë¸ ë°°í¬\n",
    "- ğŸš€ **ì—¬ëŸ¬ ëª¨ë¸ í…ŒìŠ¤íŠ¸** ê°€ëŠ¥\n",
    "\n",
    "### ì‹¤ìŠµ íë¦„\n",
    "\n",
    "1. Gradio ì¸í„°í˜ì´ìŠ¤ ë§Œë“¤ê¸° (ë¡œì»¬ í…ŒìŠ¤íŠ¸)\n",
    "2. 6ë²ˆì—ì„œ í•™ìŠµí•œ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸\n",
    "3. LoRA ì–´ëŒ‘í„°ë¥¼ HF Hubì— ì—…ë¡œë“œ\n",
    "4. HF Spaces ë°°í¬ìš© íŒŒì¼ ìƒì„±\n",
    "5. HF Spacesì— ì—…ë¡œë“œ\n",
    "6. ê³µê°œ ë§í¬ë¡œ ê³µìœ \n",
    "\n",
    "### ğŸ’¡ Gradioë€?\n",
    "\n",
    "- Pythonìœ¼ë¡œ **3ì¤„**ì´ë©´ UI ë§Œë“¤ ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "- ML ëª¨ë¸ ë°ëª¨ì— ìµœì í™”\n",
    "- HuggingFace Spacesì™€ ì™„ë²½ í˜¸í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "\n",
    "Gradioì™€ HuggingFace Hub ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "**âš ï¸ ì¤‘ìš”: Colabì—ì„œëŠ” ì„¤ì¹˜ í›„ ëŸ°íƒ€ì„ ì¬ì‹œì‘ í•„ìˆ˜!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gradio huggingface_hub bitsandbytes\n",
    "\n",
    "print(\"âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ\")\n",
    "print(\"\")\n",
    "print(\"âš ï¸  Colab ì‚¬ìš©ì:\")\n",
    "print(\"   ëŸ°íƒ€ì„ ì¬ì‹œì‘ì´ í•„ìš”í•©ë‹ˆë‹¤!\")\n",
    "print(\"   Runtime â†’ Restart runtime í´ë¦­ í›„\")\n",
    "print(\"   Cell 4ë¶€í„° ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(f\"âœ… Gradio ë²„ì „: {gr.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë°°í¬í•  ëª¨ë¸ ì„¤ì •\n",
    "\n",
    "### ğŸ“ í•™ìŠµí•œ ëª¨ë¸ë“¤ ëª©ë¡\n",
    "\n",
    "Day 1ì—ì„œ í•™ìŠµí•œ ëª¨ë“  ëª¨ë¸ë“¤ì„ ë°°í¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "| ë…¸íŠ¸ë¶ | ëª¨ë¸ | HF ê²½ë¡œ (ì˜ˆì‹œ) |\n",
    "|--------|------|----------------|\n",
    "| 03ë²ˆ | í•œêµ­ì–´ ìš”ì•½ (EXAONE-3.5) | `ryanu/exaone-summary-lora` |\n",
    "| 05ë²ˆ | Granite ìš”ì•½ | `ryanu/granite-summary-lora` |\n",
    "| 05ë²ˆ | Qwen3 ìš”ì•½ | `ryanu/qwen3-summary-lora` |\n",
    "| 06ë²ˆ | ê°ì • ë¶„ë¥˜ (IMDB) | `ryanu/lora-sentiment` |\n",
    "| 06ë²ˆ | ì˜ì–´ QA (SQuAD) | `ryanu/lora-qa` |\n",
    "\n",
    "### ğŸš€ ì‚¬ìš© ë°©ë²•\n",
    "\n",
    "1. **ëª¨ë¸ ì„ íƒ**: ì•„ë˜ `MODELS`ì—ì„œ ë°°í¬í•˜ê³  ì‹¶ì€ ëª¨ë¸ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì£¼ì„ ì²˜ë¦¬\n",
    "2. **ê²½ë¡œ ë³€ê²½**: `ryanu`ë¥¼ ë³¸ì¸ HF usernameìœ¼ë¡œ ë³€ê²½\n",
    "3. **í…ŒìŠ¤íŠ¸**: Cell 7 ì‹¤í–‰í•˜ì—¬ ë¡œì»¬ì—ì„œ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "**ê¸°ë³¸ê°’**: í•œêµ­ì–´ ìš”ì•½ + ê°ì • ë¶„ë¥˜ + ì˜ì–´ QA (3ê°œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”! ğŸ‘‡\n",
    "# ë°°í¬í•˜ê³  ì‹¶ì€ ëª¨ë¸ë§Œ ì£¼ì„ í•´ì œí•˜ì„¸ìš”!\n",
    "\n",
    "MODELS = {\n",
    "    # ========================================\n",
    "    # 03ë²ˆ: í•œêµ­ì–´ ìš”ì•½ (EXAONE-3.5)\n",
    "    # ========================================\n",
    "    \"í•œêµ­ì–´ ìš”ì•½ (03ë²ˆ)\": {\n",
    "        \"base_model\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\",\n",
    "        \"lora_path\": \"ryanu/exaone-summary-lora\",  # TODO: ë³¸ì¸ ê²½ë¡œë¡œ ë³€ê²½!\n",
    "        \"prompt_template\": \"{input}\\n\\nìš”ì•½:\",\n",
    "        \"max_new_tokens\": 60,\n",
    "        \"placeholder\": \"ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”...\",\n",
    "        \"example\": \"ì„œìš¸ì‹œê°€ ë‚´ë…„ë¶€í„° ì „ê¸°ì°¨ ì¶©ì „ì†Œë¥¼ ëŒ€í­ í™•ëŒ€í•œë‹¤. ì‹œëŠ” 2025ë…„ê¹Œì§€ ê³µê³µ ì¶©ì „ì†Œ 1ë§Œê°œë¥¼ ì„¤ì¹˜í•  ê³„íšì´ë‹¤.\",\n",
    "    },\n",
    "    \n",
    "    # ========================================\n",
    "    # 05ë²ˆ: ë‹¤ì¤‘ ëª¨ë¸ ë¹„êµ (ì„ íƒ)\n",
    "    # ========================================\n",
    "    # \"Granite ìš”ì•½ (05ë²ˆ)\": {\n",
    "    #     \"base_model\": \"ibm-granite/granite-4.0-micro\",\n",
    "    #     \"lora_path\": \"ryanu/granite-summary-lora\",  # TODO: ë³¸ì¸ ê²½ë¡œë¡œ ë³€ê²½!\n",
    "    #     \"prompt_template\": \"<|user|>\\n{input}\\n\\nìœ„ ê¸°ì‚¬ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.<|assistant|>\\n\",\n",
    "    #     \"max_new_tokens\": 60,\n",
    "    #     \"placeholder\": \"ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”...\",\n",
    "    #     \"example\": \"ì„œìš¸ì‹œê°€ ë‚´ë…„ë¶€í„° ì „ê¸°ì°¨ ì¶©ì „ì†Œë¥¼ ëŒ€í­ í™•ëŒ€í•œë‹¤.\",\n",
    "    # },\n",
    "    # \"Qwen3 ìš”ì•½ (05ë²ˆ)\": {\n",
    "    #     \"base_model\": \"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    #     \"lora_path\": \"ryanu/qwen3-summary-lora\",  # TODO: ë³¸ì¸ ê²½ë¡œë¡œ ë³€ê²½!\n",
    "    #     \"prompt_template\": \"<|im_start|>user\\n{input}\\n\\nìœ„ ê¸°ì‚¬ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "    #     \"max_new_tokens\": 60,\n",
    "    #     \"placeholder\": \"ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”...\",\n",
    "    #     \"example\": \"ì„œìš¸ì‹œê°€ ë‚´ë…„ë¶€í„° ì „ê¸°ì°¨ ì¶©ì „ì†Œë¥¼ ëŒ€í­ í™•ëŒ€í•œë‹¤.\",\n",
    "    # },\n",
    "    \n",
    "    # ========================================\n",
    "    # 06ë²ˆ: ê°ì • ë¶„ë¥˜ + ì˜ì–´ QA\n",
    "    # ========================================\n",
    "    \"ê°ì • ë¶„ë¥˜ (06ë²ˆ)\": {\n",
    "        \"base_model\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\",\n",
    "        \"lora_path\": \"ryanu/lora-sentiment\",  # TODO: ë³¸ì¸ ê²½ë¡œë¡œ ë³€ê²½!\n",
    "        \"prompt_template\": \"ë‹¤ìŒ ì˜í™” ë¦¬ë·°ì˜ ê°ì •ì„ ë¶„ë¥˜í•˜ì„¸ìš”.\\n\\në¦¬ë·°: {input}\\n\\nê°ì •:\",\n",
    "        \"max_new_tokens\": 10,\n",
    "        \"placeholder\": \"ì˜í™” ë¦¬ë·°ë¥¼ ì…ë ¥í•˜ì„¸ìš”...\",\n",
    "        \"example\": \"This movie was amazing! Great story and excellent acting.\",\n",
    "    },\n",
    "    \"ì˜ì–´ QA (06ë²ˆ)\": {\n",
    "        \"base_model\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\",\n",
    "        \"lora_path\": \"ryanu/lora-qa\",  # TODO: ë³¸ì¸ ê²½ë¡œë¡œ ë³€ê²½!\n",
    "        \"prompt_template\": \"Context: The Eiffel Tower is in Paris, France.\\n\\nQuestion: {input}\\n\\nAnswer:\",\n",
    "        \"max_new_tokens\": 30,\n",
    "        \"placeholder\": \"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...\",\n",
    "        \"example\": \"Where is the Eiffel Tower located?\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"âœ… {len(MODELS)}ê°œ ëª¨ë¸ ì„¤ì • ì™„ë£Œ\")\n",
    "for model_name in MODELS.keys():\n",
    "    print(f\"   - {model_name}\")\n",
    "    \n",
    "print(f\"\\nğŸ’¡ Tips:\")\n",
    "print(f\"   - 'ryanu'ë¥¼ ë³¸ì¸ HF usernameìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”!\")\n",
    "print(f\"   - 05ë²ˆ ëª¨ë¸ì€ ì£¼ì„ í•´ì œí•˜ì—¬ ì¶”ê°€ ê°€ëŠ¥\")\n",
    "print(f\"   - ë°°í¬í•˜ê³  ì‹¶ì€ ëª¨ë¸ë§Œ ì„ íƒí•˜ì„¸ìš”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜\n",
    "\n",
    "ì„ íƒí•œ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ ìºì‹±\n",
    "loaded_models = {}\n",
    "\n",
    "def load_model(model_name):\n",
    "    \"\"\"ëª¨ë¸ì„ ë¡œë“œí•˜ê±°ë‚˜ ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸°\"\"\"\n",
    "    if model_name in loaded_models:\n",
    "        print(f\"âœ… {model_name} ìºì‹œì—ì„œ ë¡œë“œ\")\n",
    "        return loaded_models[model_name]\n",
    "    \n",
    "    config = MODELS[model_name]\n",
    "    \n",
    "    print(f\"\\nğŸ“¥ {model_name} ë¡œë“œ ì¤‘...\")\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì €\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config[\"base_model\"],\n",
    "        use_fast=False\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Base ëª¨ë¸ (4-bit)\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16,\n",
    "    )\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config[\"base_model\"],\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=quant_config,\n",
    "    )\n",
    "    \n",
    "    # LoRA ì–´ëŒ‘í„° ë¡œë“œ\n",
    "    model = PeftModel.from_pretrained(base_model, config[\"lora_path\"])\n",
    "    \n",
    "    print(f\"   âœ… {model_name} ë¡œë“œ ì™„ë£Œ\")\n",
    "    \n",
    "    # ìºì‹œì— ì €ì¥\n",
    "    loaded_models[model_name] = (model, tokenizer, config)\n",
    "    \n",
    "    return model, tokenizer, config\n",
    "\n",
    "print(\"âœ… ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ì¶”ë¡  í•¨ìˆ˜\n",
    "\n",
    "ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ëª¨ë¸ë¡œ ì¶”ë¡ í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model_name, user_input):\n",
    "    \"\"\"ì„ íƒí•œ ëª¨ë¸ë¡œ ì¶”ë¡  ì‹¤í–‰\"\"\"\n",
    "    try:\n",
    "        # ëª¨ë¸ ë¡œë“œ\n",
    "        model, tokenizer, config = load_model(model_name)\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "        prompt = config[\"prompt_template\"].format(input=user_input)\n",
    "        \n",
    "        # í† í°í™”\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # ìƒì„±\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=config[\"max_new_tokens\"],\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # ë””ì½”ë”©\n",
    "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ ì œê±°\n",
    "        if \"ìš”ì•½:\" in result:\n",
    "            answer = result.split(\"ìš”ì•½:\")[-1].strip()\n",
    "        elif \"ê°ì •:\" in result:\n",
    "            answer = result.split(\"ê°ì •:\")[-1].strip()\n",
    "        elif \"Answer:\" in result:\n",
    "            answer = result.split(\"Answer:\")[-1].strip()\n",
    "        else:\n",
    "            answer = result[len(prompt):].strip()\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\"\n",
    "\n",
    "print(\"âœ… ì¶”ë¡  í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±\n",
    "\n",
    "UIë¥¼ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demo():\n",
    "    \"\"\"Gradio ë°ëª¨ ìƒì„±\"\"\"\n",
    "    \n",
    "    with gr.Blocks(title=\"LoRA ëª¨ë¸ ë°ëª¨\") as demo:\n",
    "        gr.Markdown(\"\"\"\n",
    "        # ğŸ¤– LoRA íŒŒì¸íŠœë‹ ëª¨ë¸ ë°ëª¨\n",
    "        \n",
    "        Day 1ì—ì„œ í•™ìŠµí•œ LoRA ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”!\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                # ëª¨ë¸ ì„ íƒ\n",
    "                model_dropdown = gr.Dropdown(\n",
    "                    choices=list(MODELS.keys()),\n",
    "                    value=list(MODELS.keys())[0],\n",
    "                    label=\"ğŸ“Œ ëª¨ë¸ ì„ íƒ\",\n",
    "                    interactive=True,\n",
    "                )\n",
    "                \n",
    "                # ì…ë ¥ í…ìŠ¤íŠ¸\n",
    "                input_text = gr.Textbox(\n",
    "                    label=\"ğŸ’¬ ì…ë ¥\",\n",
    "                    placeholder=MODELS[list(MODELS.keys())[0]][\"placeholder\"],\n",
    "                    lines=5,\n",
    "                )\n",
    "                \n",
    "                # ë²„íŠ¼\n",
    "                submit_btn = gr.Button(\"ğŸš€ ì‹¤í–‰\", variant=\"primary\")\n",
    "                clear_btn = gr.Button(\"ğŸ—‘ï¸ ì´ˆê¸°í™”\")\n",
    "            \n",
    "            with gr.Column(scale=1):\n",
    "                # ì¶œë ¥\n",
    "                output_text = gr.Textbox(\n",
    "                    label=\"âœ¨ ê²°ê³¼\",\n",
    "                    lines=10,\n",
    "                    interactive=False,\n",
    "                )\n",
    "        \n",
    "        # ì˜ˆì‹œ\n",
    "        gr.Markdown(\"### ğŸ“ ì˜ˆì‹œ\")\n",
    "        example_boxes = []\n",
    "        for model_name, config in MODELS.items():\n",
    "            example_boxes.append(\n",
    "                gr.Examples(\n",
    "                    examples=[[config[\"example\"]]],\n",
    "                    inputs=[input_text],\n",
    "                    label=f\"{model_name} ì˜ˆì‹œ\",\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬\n",
    "        def update_placeholder(model_name):\n",
    "            return gr.update(placeholder=MODELS[model_name][\"placeholder\"])\n",
    "        \n",
    "        model_dropdown.change(\n",
    "            fn=update_placeholder,\n",
    "            inputs=[model_dropdown],\n",
    "            outputs=[input_text],\n",
    "        )\n",
    "        \n",
    "        submit_btn.click(\n",
    "            fn=generate_response,\n",
    "            inputs=[model_dropdown, input_text],\n",
    "            outputs=[output_text],\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            fn=lambda: (\"\", \"\"),\n",
    "            inputs=None,\n",
    "            outputs=[input_text, output_text],\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "print(\"âœ… create_demo í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = create_demo()\n",
    "\n",
    "print(\"âœ… Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "# ë¯¸ë¦¬ ì²« ë²ˆì§¸ ëª¨ë¸ ë¡œë“œ (ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´)\n",
    "print(\"\\nğŸ“¥ ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ ì¤‘...\")\n",
    "first_model = list(MODELS.keys())[0]\n",
    "try:\n",
    "    load_model(first_model)\n",
    "    print(f\"   âœ… {first_model} ë¡œë“œ ì™„ë£Œ!\")\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(f\"   (ì²« ì‹¤í–‰ ì‹œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ë¡œì»¬ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ë©´ ë¡œì»¬ì—ì„œ UIë¥¼ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "**Colab**: ìë™ìœ¼ë¡œ ê³µê°œ ë§í¬ ìƒì„±ë¨\n",
    "**ë¡œì»¬**: http://localhost:7860 ì—ì„œ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "\n",
    "# âš ï¸ ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ Gradio ë°ëª¨ ì‹œì‘\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nğŸ’¡ ì²« ì‹¤í–‰ ì‹œ:\")\n",
    "print(\"   - ëª¨ë¸ ë‹¤ìš´ë¡œë“œì— 1-2ë¶„ ì†Œìš”\")\n",
    "print(\"   - 'ì‹¤í–‰' ë²„íŠ¼ í´ë¦­ í›„ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”\")\n",
    "print(\"   - ë¡œë”© ì¤‘ì—ëŠ” íƒ€ì„ì•„ì›ƒì²˜ëŸ¼ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "demo.launch(share=True, debug=True)  # debug=Trueë¡œ ì˜¤ë¥˜ í™•ì¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. HF Spaces ë°°í¬ìš© íŒŒì¼ ìƒì„±\n",
    "\n",
    "HuggingFace Spacesì— ë°°í¬í•˜ë ¤ë©´ `app.py`ì™€ `requirements.txt`ê°€ í•„ìš”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ë°°í¬ìš© ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "deploy_dir = \"./gradio_deploy\"\n",
    "os.makedirs(deploy_dir, exist_ok=True)\n",
    "\n",
    "# app.py ìƒì„±\n",
    "app_py_content = '''import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# ëª¨ë¸ ì„¤ì • (ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”!)\n",
    "MODELS = {\n",
    "    # ========================================\n",
    "    # 03ë²ˆ: í•œêµ­ì–´ ìš”ì•½ (EXAONE-3.5)\n",
    "    # ========================================\n",
    "    \"í•œêµ­ì–´ ìš”ì•½ (03ë²ˆ)\": {\n",
    "        \"base_model\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\",\n",
    "        \"lora_path\": \"your-username/exaone-summary-lora\",  # TODO: ë³¸ì¸ ê²½ë¡œë¡œ!\n",
    "        \"prompt_template\": \"{input}\\\\n\\\\nìš”ì•½:\",\n",
    "        \"max_new_tokens\": 60,\n",
    "        \"placeholder\": \"ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”...\",\n",
    "        \"example\": \"ì„œìš¸ì‹œê°€ ë‚´ë…„ë¶€í„° ì „ê¸°ì°¨ ì¶©ì „ì†Œë¥¼ ëŒ€í­ í™•ëŒ€í•œë‹¤.\",\n",
    "    },\n",
    "    \n",
    "    # ========================================\n",
    "    # 05ë²ˆ: ë‹¤ì¤‘ ëª¨ë¸ ë¹„êµ (ì„ íƒì‚¬í•­)\n",
    "    # ========================================\n",
    "    # \"Granite ìš”ì•½ (05ë²ˆ)\": {\n",
    "    #     \"base_model\": \"ibm-granite/granite-4.0-micro\",\n",
    "    #     \"lora_path\": \"your-username/granite-summary-lora\",\n",
    "    #     \"prompt_template\": \"<|user|>\\\\n{input}\\\\n\\\\nìœ„ ê¸°ì‚¬ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.<|assistant|>\\\\n\",\n",
    "    #     \"max_new_tokens\": 60,\n",
    "    #     \"placeholder\": \"ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”...\",\n",
    "    #     \"example\": \"ì„œìš¸ì‹œê°€ ë‚´ë…„ë¶€í„° ì „ê¸°ì°¨ ì¶©ì „ì†Œë¥¼ ëŒ€í­ í™•ëŒ€í•œë‹¤.\",\n",
    "    # },\n",
    "    # \"Qwen3 ìš”ì•½ (05ë²ˆ)\": {\n",
    "    #     \"base_model\": \"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    #     \"lora_path\": \"your-username/qwen3-summary-lora\",\n",
    "    #     \"prompt_template\": \"<|im_start|>user\\\\n{input}\\\\n\\\\nìœ„ ê¸°ì‚¬ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.<|im_end|>\\\\n<|im_start|>assistant\\\\n\",\n",
    "    #     \"max_new_tokens\": 60,\n",
    "    #     \"placeholder\": \"ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”...\",\n",
    "    #     \"example\": \"ì„œìš¸ì‹œê°€ ë‚´ë…„ë¶€í„° ì „ê¸°ì°¨ ì¶©ì „ì†Œë¥¼ ëŒ€í­ í™•ëŒ€í•œë‹¤.\",\n",
    "    # },\n",
    "    \n",
    "    # ========================================\n",
    "    # 06ë²ˆ: ê°ì • ë¶„ë¥˜ + ì˜ì–´ QA\n",
    "    # ========================================\n",
    "    \"ê°ì • ë¶„ë¥˜ (06ë²ˆ)\": {\n",
    "        \"base_model\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\",\n",
    "        \"lora_path\": \"your-username/lora-sentiment\",\n",
    "        \"prompt_template\": \"ë‹¤ìŒ ì˜í™” ë¦¬ë·°ì˜ ê°ì •ì„ ë¶„ë¥˜í•˜ì„¸ìš”.\\\\n\\\\në¦¬ë·°: {input}\\\\n\\\\nê°ì •:\",\n",
    "        \"max_new_tokens\": 10,\n",
    "        \"placeholder\": \"ì˜í™” ë¦¬ë·°ë¥¼ ì…ë ¥í•˜ì„¸ìš”...\",\n",
    "        \"example\": \"This movie was amazing! Great story and excellent acting.\",\n",
    "    },\n",
    "    \"ì˜ì–´ QA (06ë²ˆ)\": {\n",
    "        \"base_model\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\",\n",
    "        \"lora_path\": \"your-username/lora-qa\",\n",
    "        \"prompt_template\": \"Context: The Eiffel Tower is in Paris, France.\\\\n\\\\nQuestion: {input}\\\\n\\\\nAnswer:\",\n",
    "        \"max_new_tokens\": 30,\n",
    "        \"placeholder\": \"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...\",\n",
    "        \"example\": \"Where is the Eiffel Tower located?\",\n",
    "    },\n",
    "}\n",
    "\n",
    "loaded_models = {}\n",
    "\n",
    "def load_model(model_name):\n",
    "    if model_name in loaded_models:\n",
    "        return loaded_models[model_name]\n",
    "    \n",
    "    config = MODELS[model_name]\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(config[\"base_model\"], use_fast=False)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config[\"base_model\"],\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=quant_config,\n",
    "    )\n",
    "    \n",
    "    model = PeftModel.from_pretrained(base_model, config[\"lora_path\"])\n",
    "    \n",
    "    loaded_models[model_name] = (model, tokenizer, config)\n",
    "    return model, tokenizer, config\n",
    "\n",
    "def generate_response(model_name, user_input):\n",
    "    try:\n",
    "        model, tokenizer, config = load_model(model_name)\n",
    "        prompt = config[\"prompt_template\"].format(input=user_input)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=config[\"max_new_tokens\"],\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ ì œê±°\n",
    "        if \"ìš”ì•½:\" in result:\n",
    "            return result.split(\"ìš”ì•½:\")[-1].strip()\n",
    "        elif \"ê°ì •:\" in result:\n",
    "            return result.split(\"ê°ì •:\")[-1].strip()\n",
    "        elif \"Answer:\" in result:\n",
    "            return result.split(\"Answer:\")[-1].strip()\n",
    "        elif \"<|assistant|>\" in result:\n",
    "            return result.split(\"<|assistant|>\")[-1].strip()\n",
    "        elif \"<|im_start|>assistant\" in result:\n",
    "            return result.split(\"<|im_start|>assistant\")[-1].replace(\"<|im_end|>\", \"\").strip()\n",
    "        else:\n",
    "            return result[len(prompt):].strip()\n",
    "    except Exception as e:\n",
    "        return f\"âŒ ì˜¤ë¥˜: {str(e)}\"\n",
    "\n",
    "with gr.Blocks(title=\"LoRA ëª¨ë¸ ë°ëª¨\") as demo:\n",
    "    gr.Markdown(\"# ğŸ¤– LoRA íŒŒì¸íŠœë‹ ëª¨ë¸ ë°ëª¨\")\n",
    "    gr.Markdown(\"Day 1ì—ì„œ í•™ìŠµí•œ ì—¬ëŸ¬ LoRA ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”!\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            model_dropdown = gr.Dropdown(\n",
    "                choices=list(MODELS.keys()),\n",
    "                value=list(MODELS.keys())[0],\n",
    "                label=\"ğŸ“Œ ëª¨ë¸ ì„ íƒ\"\n",
    "            )\n",
    "            input_text = gr.Textbox(label=\"ğŸ’¬ ì…ë ¥\", lines=5)\n",
    "            submit_btn = gr.Button(\"ğŸš€ ì‹¤í–‰\", variant=\"primary\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            output_text = gr.Textbox(label=\"âœ¨ ê²°ê³¼\", lines=10)\n",
    "    \n",
    "    submit_btn.click(\n",
    "        fn=generate_response,\n",
    "        inputs=[model_dropdown, input_text],\n",
    "        outputs=[output_text]\n",
    "    )\n",
    "\n",
    "demo.launch()\n",
    "'''\n",
    "\n",
    "with open(f\"{deploy_dir}/app.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(app_py_content)\n",
    "\n",
    "# requirements.txt ìƒì„±\n",
    "requirements_content = '''gradio\n",
    "transformers\n",
    "torch\n",
    "peft\n",
    "bitsandbytes\n",
    "accelerate\n",
    "sentencepiece\n",
    "'''\n",
    "\n",
    "with open(f\"{deploy_dir}/requirements.txt\", \"w\") as f:\n",
    "    f.write(requirements_content)\n",
    "\n",
    "# README.md ìƒì„±\n",
    "readme_content = '''---\n",
    "title: LoRA Model Demo\n",
    "emoji: ğŸ¤–\n",
    "colorFrom: blue\n",
    "colorTo: green\n",
    "sdk: gradio\n",
    "sdk_version: 4.0.0\n",
    "app_file: app.py\n",
    "pinned: false\n",
    "---\n",
    "\n",
    "# LoRA íŒŒì¸íŠœë‹ ëª¨ë¸ ë°ëª¨\n",
    "\n",
    "Day 1ì—ì„œ í•™ìŠµí•œ LoRA ëª¨ë¸ ë°ëª¨ì…ë‹ˆë‹¤.\n",
    "\n",
    "## í¬í•¨ëœ ëª¨ë¸\n",
    "\n",
    "- í•œêµ­ì–´ ìš”ì•½ (03ë²ˆ) - EXAONE-3.5\n",
    "- ê°ì • ë¶„ë¥˜ (06ë²ˆ) - IMDB\n",
    "- ì˜ì–´ QA (06ë²ˆ) - SQuAD\n",
    "\n",
    "## ì„ íƒ ê°€ëŠ¥ (ì£¼ì„ í•´ì œ)\n",
    "\n",
    "- Granite ìš”ì•½ (05ë²ˆ)\n",
    "- Qwen3 ìš”ì•½ (05ë²ˆ)\n",
    "\n",
    "## ì‚¬ìš© ë°©ë²•\n",
    "\n",
    "1. ëª¨ë¸ ì„ íƒ\n",
    "2. ì…ë ¥ í…ìŠ¤íŠ¸ ì…ë ¥\n",
    "3. ì‹¤í–‰ ë²„íŠ¼ í´ë¦­\n",
    "'''\n",
    "\n",
    "with open(f\"{deploy_dir}/README.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(f\"\\nâœ… ë°°í¬ìš© íŒŒì¼ ìƒì„± ì™„ë£Œ!\")\n",
    "print(f\"   ğŸ“ {deploy_dir}/\")\n",
    "print(f\"      - app.py\")\n",
    "print(f\"      - requirements.txt\")\n",
    "print(f\"      - README.md\")\n",
    "print(f\"\\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(f\"   1. app.pyì—ì„œ 'your-username'ì„ ë³¸ì¸ HF usernameìœ¼ë¡œ ë³€ê²½\")\n",
    "print(f\"   2. ë°°í¬í•˜ì§€ ì•Šì„ ëª¨ë¸ì€ ì£¼ì„ ì²˜ë¦¬ ë˜ëŠ” ì‚­ì œ\")\n",
    "print(f\"   3. LoRA ì–´ëŒ‘í„°ë¥¼ HF Hubì— ì—…ë¡œë“œ\")\n",
    "print(f\"   4. HF Spacesì— ë°°í¬\")\n",
    "print(f\"\\nğŸ“Š ê¸°ë³¸ í™œì„±í™”:\")\n",
    "print(f\"   - í•œêµ­ì–´ ìš”ì•½ (03ë²ˆ)\")\n",
    "print(f\"   - ê°ì • ë¶„ë¥˜ (06ë²ˆ)\")\n",
    "print(f\"   - ì˜ì–´ QA (06ë²ˆ)\")\n",
    "print(f\"\\n   05ë²ˆ ëª¨ë¸ì€ ì£¼ì„ í•´ì œí•˜ì—¬ ì¶”ê°€ ê°€ëŠ¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. HF Spaces ë°°í¬ ê°€ì´ë“œ\n",
    "\n",
    "### ğŸ“‹ ë°°í¬ ì „ ì¤€ë¹„\n",
    "\n",
    "1. **LoRA ì–´ëŒ‘í„° ì—…ë¡œë“œ** (05ë²ˆ ë…¸íŠ¸ë¶ ì°¸ê³ )\n",
    "   ```python\n",
    "   # ì˜ˆì‹œ\n",
    "   api.upload_folder(\n",
    "       folder_path=\"./lora_summarization\",\n",
    "       repo_id=\"your-username/lora-summarization\",\n",
    "       repo_type=\"model\"\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **app.py ìˆ˜ì •**\n",
    "   - `lora_path`ë¥¼ HuggingFace ê²½ë¡œë¡œ ë³€ê²½\n",
    "   - ì˜ˆ: `\"lora_path\": \"your-username/lora-summarization\"`\n",
    "\n",
    "### ğŸš€ ë°°í¬ ë°©ë²•\n",
    "\n",
    "**ì˜µì…˜ 1: Web UIë¡œ ë°°í¬**\n",
    "1. https://huggingface.co/spaces ì ‘ì†\n",
    "2. `New Space` í´ë¦­\n",
    "3. Space name ì…ë ¥ (ì˜ˆ: `lora-demo`)\n",
    "4. SDK: `Gradio` ì„ íƒ\n",
    "5. `Create Space` í´ë¦­\n",
    "6. `gradio_deploy/` í´ë”ì˜ íŒŒì¼ë“¤ ì—…ë¡œë“œ:\n",
    "   - `app.py`\n",
    "   - `requirements.txt`\n",
    "   - `README.md`\n",
    "\n",
    "**ì˜µì…˜ 2: CLIë¡œ ë°°í¬**\n",
    "```bash\n",
    "# HuggingFace CLI ì„¤ì¹˜\n",
    "pip install huggingface_hub\n",
    "\n",
    "# ë¡œê·¸ì¸\n",
    "huggingface-cli login\n",
    "\n",
    "# Space ìƒì„± ë° ì—…ë¡œë“œ\n",
    "cd gradio_deploy\n",
    "git init\n",
    "git add .\n",
    "git commit -m \"Initial commit\"\n",
    "git remote add origin https://huggingface.co/spaces/your-username/lora-demo\n",
    "git push origin main\n",
    "```\n",
    "\n",
    "### âœ… ë°°í¬ í™•ì¸\n",
    "\n",
    "ë°°í¬ í›„ ì•½ 1-2ë¶„ ë’¤:\n",
    "- Space í˜ì´ì§€ì—ì„œ \"Running\" ìƒíƒœ í™•ì¸\n",
    "- ê³µê°œ URLë¡œ ì ‘ì† í…ŒìŠ¤íŠ¸\n",
    "- ë§í¬ ê³µìœ  ê°€ëŠ¥!\n",
    "\n",
    "### ğŸ’° ë¹„ìš©\n",
    "\n",
    "- **CPU ê¸°ë°˜**: ë¬´ë£Œ âœ…\n",
    "- **GPU í•„ìš”ì‹œ**: PRO ê³„ì • í•„ìš” ($9/ì›”)\n",
    "\n",
    "ì‘ì€ ëª¨ë¸(EXAONE-3.5-2.4B)ì€ CPUë¡œë„ ì¶©ë¶„í•©ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF Hub CLIë¡œ ë°°í¬í•˜ê¸° (ì„ íƒ)\n",
    "# ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”\n",
    "\n",
    "# from huggingface_hub import HfApi, login\n",
    "# \n",
    "# # ë¡œê·¸ì¸\n",
    "# login(token=\"YOUR_HF_TOKEN\")\n",
    "# \n",
    "# api = HfApi()\n",
    "# \n",
    "# # Space ìƒì„±\n",
    "# api.create_repo(\n",
    "#     repo_id=\"your-username/lora-demo\",  # ë³¸ì¸ usernameìœ¼ë¡œ ë³€ê²½!\n",
    "#     repo_type=\"space\",\n",
    "#     space_sdk=\"gradio\"\n",
    "# )\n",
    "# \n",
    "# # íŒŒì¼ ì—…ë¡œë“œ\n",
    "# api.upload_folder(\n",
    "#     folder_path=\"./gradio_deploy\",\n",
    "#     repo_id=\"your-username/lora-demo\",\n",
    "#     repo_type=\"space\"\n",
    "# )\n",
    "# \n",
    "# print(\"âœ… HF Spaces ë°°í¬ ì™„ë£Œ!\")\n",
    "# print(\"   ë§í¬: https://huggingface.co/spaces/your-username/lora-demo\")\n",
    "\n",
    "print(\"ğŸ’¡ ìœ„ ì£¼ì„ì„ í•´ì œí•˜ê³  usernameì„ ìˆ˜ì •í•œ í›„ ì‹¤í–‰í•˜ì„¸ìš”!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ë§ˆë¬´ë¦¬ ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤!\n",
    "\n",
    "Day 1ì˜ ëª¨ë“  ì‹¤ìŠµì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!\n",
    "\n",
    "**ë°°ìš´ ë‚´ìš© ì •ë¦¬:**\n",
    "- âœ… 01: RAFT ë°ì´í„° ì „ì²˜ë¦¬\n",
    "- âœ… 02: ë°ì´í„° í’ˆì§ˆ ê²€ì¦\n",
    "- âœ… 03: LoRA íŒŒì¸íŠœë‹\n",
    "- âœ… 04: Baseline vs LoRA ë¹„êµ\n",
    "- âœ… 05: ë‹¤ì¤‘ ëª¨ë¸ + Chat Template ì‹¤í—˜\n",
    "- âœ… 06: 3ê°œ íƒœìŠ¤í¬ ë¹„êµ ì‹¤í—˜\n",
    "- âœ… 07: Gradio ë°°í¬ + HF Spaces\n",
    "\n",
    "### ğŸ’¡ ì¶”ê°€ ì‹¤í—˜ ì•„ì´ë””ì–´\n",
    "\n",
    "1. **ë‹¤ë¥¸ íƒœìŠ¤í¬ ì‹œë„**:\n",
    "   - ë²ˆì—­, ì½”ë“œ ìƒì„±, ì±—ë´‡ ë“±\n",
    "\n",
    "2. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**:\n",
    "   - LoRA rank, alpha, dropout ì¡°í•© ì‹¤í—˜\n",
    "\n",
    "3. **ì•™ìƒë¸”**:\n",
    "   - ì—¬ëŸ¬ LoRA ëª¨ë¸ ê²°í•©\n",
    "\n",
    "4. **A/B í…ŒìŠ¤íŠ¸**:\n",
    "   - Gradioì—ì„œ ë‘ ëª¨ë¸ ë¹„êµ UI ë§Œë“¤ê¸°\n",
    "\n",
    "### ğŸ“š ì°¸ê³  ìë£Œ\n",
    "\n",
    "- [Gradio ë¬¸ì„œ](https://www.gradio.app/docs)\n",
    "- [HF Spaces ê°€ì´ë“œ](https://huggingface.co/docs/hub/spaces)\n",
    "- [PEFT ë¬¸ì„œ](https://huggingface.co/docs/peft)\n",
    "\n",
    "### ğŸš€ Day 2 ì˜ˆê³ \n",
    "\n",
    "ë‚´ì¼ì€ **RAG (Retrieval-Augmented Generation)**ë¥¼ ë°°ì›ë‹ˆë‹¤!\n",
    "- Naive RAG êµ¬í˜„\n",
    "- Advanced RAG ê¸°ë²•\n",
    "- Hybrid Search + Reranking\n",
    "\n",
    "ì˜ ì‰¬ê³  ë‚´ì¼ ë§Œë‚˜ìš”! ğŸ‘‹"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}