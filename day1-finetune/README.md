# Day 1: EXAONE 3.5 Fine-Tuning μ‹¤μµ

EXAONE 3.5 λ¨λΈμ„ ν™μ©ν• RAG μ„±λ¥ ν–¥μƒμ„ μ„ν• RAFT(Retrieval Augmented Fine Tuning) μ‹¤μµ μλ£μ…λ‹λ‹¤.

## π“ μ‹¤μµ κµ¬μ„± (μ΄ 4μ‹κ°„)

### 1οΈβƒ£ μ„Έμ… 1: λ°μ΄ν„° μ „μ²λ¦¬ λ° κ²€μ¦ (13:10-14:00, 50λ¶„)
- **01_data_preprocessing_and_validation.ipynb**: ν•κµ­μ–΄ RAG λ°μ΄ν„°μ…‹ λ΅λ“ λ° RAFT μ „μ²λ¦¬
- **02_data_quality_check.ipynb**: λ°μ΄ν„° ν’μ§ κ²€μ¦ λ° μ‹κ°ν™”

### 2οΈβƒ£ μ„Έμ… 2: λ¨λΈ νμΈνλ‹ (14:40-16:10, 90λ¶„) 
- **03_fine_tuning_with_lora.ipynb**: EXAONE 3.5 λ¨λΈ QLoRA νμΈνλ‹

### 3οΈβƒ£ μ„Έμ… 3: ν‰κ°€ λ° λΉ„κµ (16:20-17:00, 40λ¶„)
- **04_evaluation_and_comparison.ipynb**: λ‹¤μ–‘ν• λ©”νΈλ¦­μ„ ν™μ©ν• λ¨λΈ μ„±λ¥ ν‰κ°€

## π€ λΉ λ¥Έ μ‹μ‘

### Google Colabμ—μ„ μ‹¤ν–‰
1. κ° λ…ΈνΈλ¶μ„ Google Colabμ— μ—…λ΅λ“
2. GPU λ°νƒ€μ„ μ„¤μ • (λ°νƒ€μ„ > λ°νƒ€μ„ μ ν• λ³€κ²½ > GPU)
3. λ…ΈνΈλ¶μ„ μμ„λ€λ΅ μ‹¤ν–‰

### λ΅μ»¬ ν™κ²½ μ„¤μ •
```bash
# Python ν™κ²½ μƒμ„± (κ¶μ¥: Python 3.8+)
conda create -n exaone-finetune python=3.8
conda activate exaone-finetune

# ν•„μ ν¨ν‚¤μ§€ μ„¤μΉ
pip install torch==2.0.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers==4.36.0
pip install peft==0.7.1
pip install datasets==2.14.0
pip install accelerate==0.24.0
pip install bitsandbytes==0.41.1
pip install rouge-score==0.1.2
pip install nltk==3.8.1
pip install scikit-learn==1.3.0
pip install matplotlib==3.7.2
pip install seaborn==0.12.2
pip install pandas==2.0.3
pip install numpy==1.24.3
pip install tqdm==4.65.0

# Jupyter μ„¤μΉ
pip install jupyter ipywidgets
```

## π“– λ…ΈνΈλ¶λ³„ μƒμ„Έ κ°€μ΄λ“
### λ…ΈνΈλ¶ μ‹¤ν–‰ μ°Έκ³  (λ‹¤μ΄μ–΄κ·Έλ¨ λ λ”λ§)
- `main-practice/00-concepts.ipynb`μ λΉ„κµ λ‹¤μ΄μ–΄κ·Έλ¨μ€ HTML μΉ΄λ“λ΅ ν‘μ‹λμ–΄ λ³„λ„ μ„¤μΉ μ—†μ΄ μ¦‰μ‹ ν™•μΈν•  μ μμµλ‹λ‹¤.

- `main-practice/00-concepts.ipynb` λ‹¤μ΄μ–΄κ·Έλ¨μ€ Graphvizλ΅ μƒμ„±λ©λ‹λ‹¤.
  ```bash
  # Colab
  apt-get install -qq graphviz
  pip install graphviz
  ```
  μ„¤μΉ ν›„ λ…ΈνΈλ¶μ λ λ”λ§ μ…€μ„ μ‹¤ν–‰ν•λ©΄ SVGλ΅ μ¶λ ¥λ©λ‹λ‹¤.

- `main-practice/00-concepts.ipynb`μ—μ„ νμΈνλ‹ μ „λµ λΉ„κµ κ·Έλν”„λ¥Ό ν™•μΈν•λ ¤λ©΄ μ•„λ ν™•μ¥μ„ ν• λ²λ§ μ„¤μΉ/λ΅λ”©ν•μ„Έμ”.
  ```bash
  pip install mermaid-magic
  ```
  λ…ΈνΈλ¶ μ…€μ—μ„ `%load_ext mermaid_magic` μ‹¤ν–‰ ν›„ `%%mermaid` μ…€μ„ μ‹¤ν–‰ν•λ©΄ κ·Έλν”„κ°€ λ λ”λ§λ©λ‹λ‹¤.


### 01_data_preprocessing_and_validation.ipynb
**λ©ν‘**: ν•κµ­μ–΄ RAG λ°μ΄ν„°μ…‹ μ¤€λΉ„ λ° RAFT λ°©λ²•λ΅  μ μ©
- neural-bridge/rag-dataset-12000 λ°μ΄ν„°μ…‹ λ΅λ“
- RAFT(Retrieval Augmented Fine Tuning) μ „μ²λ¦¬ κµ¬ν„
- Positive/Negative μƒν” μƒμ„±
- λ°μ΄ν„° ν¬λ§· λ³€ν™ λ° κ²€μ¦

**ν•µμ‹¬ κ°λ…**:
- RAFT λ°©λ²•λ΅ : RAG μ„±λ¥ ν–¥μƒμ„ μ„ν• νμΈνλ‹ κΈ°λ²•
- Positive/Negative μƒν”λ§: κ΄€λ ¨/λΉ„κ΄€λ ¨ λ¬Έμ„ κµ¬λ¶„ ν•™μµ

### 02_data_quality_check.ipynb  
**λ©ν‘**: μ „μ²λ¦¬λ λ°μ΄ν„°μ ν’μ§ κ²€μ¦ λ° μ‹κ°ν™”
- ν† ν° λ¶„ν¬ λ¶„μ„
- ν…μ¤νΈ κΈΈμ΄ ν†µκ³„
- RAFT κµ¬μ΅° κ²€μ¦
- λ°μ΄ν„° ν’μ§ μ§€ν‘ κ³„μ‚°

**ν•µμ‹¬ κ°λ…**:
- λ°μ΄ν„° ν’μ§ λ©”νΈλ¦­
- ν† ν°ν™” λ° κΈΈμ΄ λ¶„μ„
- μ‹κ°ν™”λ¥Ό ν†µν• λ°μ΄ν„° μ΄ν•΄

### 03_fine_tuning_with_lora.ipynb
**λ©ν‘**: EXAONE 3.5 λ¨λΈ QLoRA νμΈνλ‹
- 4λΉ„νΈ μ–‘μν™”λ¥Ό ν†µν• λ©”λ¨λ¦¬ μµμ ν™”
- LoRA(Low-Rank Adaptation) μ μ©
- ν¨μ¨μ μΈ νμΈνλ‹ μ‹¤ν–‰
- GPU λ©”λ¨λ¦¬ λ¨λ‹ν„°λ§

**ν•µμ‹¬ κ°λ…**:
- QLoRA: 4λΉ„νΈ μ–‘μν™” + LoRAμ κ²°ν•©
- Parameter-Efficient Fine-Tuning
- λ©”λ¨λ¦¬ ν¨μ¨μ  ν•™μµ μ „λµ

### 04_evaluation_and_comparison.ipynb
**λ©ν‘**: νμΈνλ‹ μ „ν›„ λ¨λΈ μ„±λ¥ λΉ„κµ ν‰κ°€
- ROUGE, BLEU, μ½”μ‚¬μΈ μ μ‚¬λ„, Perplexity κ³„μ‚°
- μ •λ‰μ /μ •μ„±μ  ν‰κ°€
- μ¤λ¥ μ‚¬λ΅€ λ¶„μ„
- λ¨λΈ ν¨ν‚¤μ§• λ° λ°°ν¬ μ¤€λΉ„

**ν•µμ‹¬ κ°λ…**:
- λ‹¤μ–‘ν• NLG ν‰κ°€ λ©”νΈλ¦­
- λ¨λΈ μ„±λ¥ λ²¤μΉλ§ν‚Ή
- λ°°ν¬λ¥Ό μ„ν• λ¨λΈ μµμ ν™”

## π”§ μ£Όμ” κΈ°μ  μ¤νƒ

- **λ¨λΈ**: LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct
- **λ°μ΄ν„°μ…‹**: neural-bridge/rag-dataset-12000 
- **νμΈνλ‹**: QLoRA (4-bit + LoRA)
- **ν‰κ°€**: ROUGE, BLEU, μ½”μ‚¬μΈ μ μ‚¬λ„, Perplexity
- **ν™κ²½**: Google Colab (λ¬΄λ£ GPU μ§€μ›)

## π― ν•™μµ λ©ν‘

μ΄ μ‹¤μµμ„ ν†µν•΄ λ‹¤μμ„ ν•™μµν•©λ‹λ‹¤:

1. **RAFT λ°©λ²•λ΅  μ΄ν•΄**: RAG μ‹μ¤ν… μ„±λ¥ ν–¥μƒμ„ μ„ν• νμΈνλ‹ μ „λµ
2. **ν¨μ¨μ  νμΈνλ‹**: QLoRAλ¥Ό ν™μ©ν• λ©”λ¨λ¦¬ ν¨μ¨μ  ν•™μµ
3. **ν¬κ΄„μ  ν‰κ°€**: λ‹¤μ–‘ν• λ©”νΈλ¦­μ„ ν†µν• λ¨λΈ μ„±λ¥ μΈ΅μ •
4. **μ‹¤λ¬΄ μ μ©**: νμΈνλ‹λ λ¨λΈμ ν¨ν‚¤μ§• λ° λ°°ν¬ μ¤€λΉ„

## π“ μμƒ μ„±λ¥

RAFT νμΈνλ‹ ν›„ μμƒλλ” κ°μ„  ν¨κ³Ό:
- **ROUGE-L**: 15-25% ν–¥μƒ
- **BLEU μ μ**: 10-20% ν–¥μƒ  
- **μ½”μ‚¬μΈ μ μ‚¬λ„**: 5-15% ν–¥μƒ
- **μ‘λ‹µ μΌκ΄€μ„±**: ν„μ €ν• κ°μ„ 

## β οΈ μ£Όμμ‚¬ν•­

### Google Colab μ‚¬μ© μ‹
- GPU λ°νƒ€μ„ ν•„μ μ„¤μ •
- μ„Έμ… νƒ€μ„μ•„μ›ƒμ— μ£Όμ (12μ‹κ°„ μ ν•)
- μ¤‘κ°„ κ²°κ³Όλ¬Ό μ •κΈ°μ μΌλ΅ Driveμ— μ €μ¥
- λ©”λ¨λ¦¬ μ‚¬μ©λ‰ λ¨λ‹ν„°λ§ ν•„μ

### λ΅μ»¬ ν™κ²½ μ‚¬μ© μ‹  
- CUDA 11.8+ ν•„μ” (GPU μ‚¬μ© μ‹)
- μµμ† 16GB RAM, 8GB VRAM κ¶μ¥
- PyTorch 2.0+ νΈν™μ„± ν™•μΈ

## π” νΈλ¬λΈ”μν…

### μμ£Ό λ°μƒν•λ” λ¬Έμ λ“¤

**1. GPU λ©”λ¨λ¦¬ λ¶€μ΅±**
```python
# ν•΄κ²°μ±…: λ°°μΉ ν¬κΈ° μ¤„μ΄κΈ°
per_device_train_batch_size=1
gradient_accumulation_steps=8
```

**2. ν† ν¬λ‚μ΄μ € μ¤λ¥**
```python
# ν•΄κ²°μ±…: pad_token μ„¤μ •
tokenizer.pad_token = tokenizer.eos_token
```

**3. λ¨λΈ λ΅λ“ μ‹¤ν¨**
```python
# ν•΄κ²°μ±…: device_map μ„¤μ •
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto"
)
```

## π¤ κΈ°μ—¬ν•κΈ°

μ΄ μ‹¤μµ μλ£μ— λ€ν• κ°μ„  μ‚¬ν•­μ΄λ‚ μ¤λ¥ λ°κ²¬ μ‹:
1. μ΄μ λ“±λ΅μ„ ν†µν• λ¬Έμ  λ³΄κ³ 
2. Pull Requestλ¥Ό ν†µν• κ°μ„  μ‚¬ν•­ μ μ•
3. ν”Όλ“λ°± λ° μ μ•μ‚¬ν•­ κ³µμ 

## π“ λ¬Έμμ‚¬ν•­

μ‹¤μµ μ¤‘ λ¬Έμ κ°€ λ°μƒν•κ±°λ‚ μ¶”κ°€ μ§λ¬Έμ΄ μμΌμ‹λ©΄:
- κ°•μ μ¤‘: μ§μ ‘ μ§λ¬Έ
- κ°•μ ν›„: μ΄λ©”μΌ λλ” GitHub μ΄μ ν™μ©

## π“ λΌμ΄μ„ μ¤

μ΄ μ‹¤μµ μλ£λ” κµμ΅ λ©μ μΌλ΅ μ μ‘λμ—μΌλ©°, μμ λ΅­κ² ν™μ©ν•μ‹¤ μ μμµλ‹λ‹¤.

---

**Happy Fine-tuning! π€**

*λ³Έ μ‹¤μµμ„ ν†µν•΄ μ—¬λ¬λ¶„λ§μ κ³ μ„±λ¥ RAG μ‹μ¤ν…μ„ κµ¬μ¶•ν•΄λ³΄μ„Έμ”!*