{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 📊 Day 1-00.06: 모델 평가하기 (초보자용)\n",
        "\n",
        "## 🎯 이번 노트북에서 할 일\n",
        "- **학습된 모델** 로드하기\n",
        "- **성능 평가**하기 (매우 간단하게!)\n",
        "- **원본 모델과 비교**하기\n",
        "- **결과 분석**하기\n",
        "\n",
        "## 💡 모델 평가란?\n",
        "**학습된 모델이 얼마나 잘 작동하는지 확인하는 것**\n",
        "\n",
        "### 🔧 평가 방법들\n",
        "1. **정성적 평가**: 사람이 직접 답변 품질 확인\n",
        "2. **정량적 평가**: 수치로 성능 측정\n",
        "3. **비교 평가**: 원본 모델과 성능 비교\n",
        "4. **실제 사용**: 실제 질문에 답변해보기\n",
        "\n",
        "### 📊 오늘 할 평가\n",
        "- **질문 답변 품질**: 답변이 정확하고 유용한지\n",
        "- **RAFT 성능**: 문서를 참고해서 답변하는지\n",
        "- **한국어 이해**: 한국어 질문을 잘 이해하는지\n",
        "- **일관성**: 같은 질문에 일관된 답변을 하는지\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 필요한 라이브러리 불러오기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모델 평가에 필요한 라이브러리들을 불러옵니다\n",
        "import torch\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "from transformers import (\n",
        "    AutoTokenizer,           # 토크나이저\n",
        "    AutoModelForCausalLM,    # 언어 모델\n",
        "    BitsAndBytesConfig       # 4비트 양자화\n",
        ")\n",
        "from peft import (\n",
        "    PeftModel,               # LoRA 모델 로드\n",
        "    LoraConfig,              # LoRA 설정\n",
        "    TaskType                 # 태스크 타입\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✅ 모델 평가 라이브러리가 준비되었습니다!\")\n",
        "print(f\"🔥 PyTorch 버전: {torch.__version__}\")\n",
        "print(f\"🚀 CUDA 사용 가능: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 학습된 모델 로드하기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 학습된 모델을 로드합니다\n",
        "print(\"📥 학습된 모델 로드 중...\")\n",
        "\n",
        "# 학습 설정 불러오기\n",
        "with open(\"models/training_config.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    training_config = json.load(f)\n",
        "\n",
        "model_name = training_config['model_name']\n",
        "print(f\"   - 기본 모델: {model_name}\")\n",
        "\n",
        "# 4비트 양자화 설정\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# 기본 모델 로드\n",
        "print(\"🤖 기본 모델 로드 중...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# 토크나이저 로드\n",
        "print(\"🔤 토크나이저 로드 중...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    padding_side=\"right\"\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# LoRA 모델 로드\n",
        "print(\"🔧 LoRA 모델 로드 중...\")\n",
        "model = PeftModel.from_pretrained(base_model, \"models/fine_tuned_lora\")\n",
        "\n",
        "print(\"✅ 학습된 모델 로드 완료!\")\n",
        "print(f\"   - 기본 모델: {model_name}\")\n",
        "print(f\"   - LoRA 어댑터: models/fine_tuned_lora/\")\n",
        "print(f\"   - 디바이스: {next(model.parameters()).device}\")\n",
        "\n",
        "# 모델 정보 확인\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "all_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"   - 학습 가능한 파라미터: {trainable_params:,}개\")\n",
        "print(f\"   - 전체 파라미터: {all_params:,}개\")\n",
        "print(f\"   - 학습 비율: {trainable_params/all_params*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 텍스트 생성 함수 만들기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모델이 텍스트를 생성하는 함수를 만듭니다 (매우 간단하게!)\n",
        "def generate_text(model, tokenizer, prompt, max_length=100, temperature=0.7):\n",
        "    \"\"\"\n",
        "    모델이 텍스트를 생성하는 함수\n",
        "    \n",
        "    Args:\n",
        "        model: 사용할 모델\n",
        "        tokenizer: 토크나이저\n",
        "        prompt: 입력 텍스트\n",
        "        max_length: 최대 생성 길이\n",
        "        temperature: 창의성 조절 (0.1=보수적, 1.0=창의적)\n",
        "    \n",
        "    Returns:\n",
        "        생성된 텍스트\n",
        "    \"\"\"\n",
        "    # 입력 토큰화\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # 텍스트 생성\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_length,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # 생성된 텍스트 디코딩\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # 입력 부분 제거하고 생성된 부분만 반환\n",
        "    generated_part = generated_text[len(prompt):].strip()\n",
        "    \n",
        "    return generated_part\n",
        "\n",
        "# 함수 테스트\n",
        "print(\"🧪 텍스트 생성 함수 테스트:\")\n",
        "test_prompt = \"질문: 인공지능이란 무엇인가요?\\n답변:\"\n",
        "generated = generate_text(model, tokenizer, test_prompt, max_length=50)\n",
        "print(f\"   - 입력: {test_prompt}\")\n",
        "print(f\"   - 생성: {generated}\")\n",
        "print(\"✅ 텍스트 생성 함수 준비 완료!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
