{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“Š Day 1-00.06: ëª¨ë¸ í‰ê°€í•˜ê¸° (ì´ˆë³´ììš©)\n",
        "\n",
        "## ğŸ¯ ì´ë²ˆ ë…¸íŠ¸ë¶ì—ì„œ í•  ì¼\n",
        "- **í•™ìŠµëœ ëª¨ë¸** ë¡œë“œí•˜ê¸°\n",
        "- **ì„±ëŠ¥ í‰ê°€**í•˜ê¸° (ë§¤ìš° ê°„ë‹¨í•˜ê²Œ!)\n",
        "- **ì›ë³¸ ëª¨ë¸ê³¼ ë¹„êµ**í•˜ê¸°\n",
        "- **ê²°ê³¼ ë¶„ì„**í•˜ê¸°\n",
        "\n",
        "## ğŸ’¡ ëª¨ë¸ í‰ê°€ë€?\n",
        "**í•™ìŠµëœ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì˜ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ê²ƒ**\n",
        "\n",
        "### ğŸ”§ í‰ê°€ ë°©ë²•ë“¤\n",
        "1. **ì •ì„±ì  í‰ê°€**: ì‚¬ëŒì´ ì§ì ‘ ë‹µë³€ í’ˆì§ˆ í™•ì¸\n",
        "2. **ì •ëŸ‰ì  í‰ê°€**: ìˆ˜ì¹˜ë¡œ ì„±ëŠ¥ ì¸¡ì •\n",
        "3. **ë¹„êµ í‰ê°€**: ì›ë³¸ ëª¨ë¸ê³¼ ì„±ëŠ¥ ë¹„êµ\n",
        "4. **ì‹¤ì œ ì‚¬ìš©**: ì‹¤ì œ ì§ˆë¬¸ì— ë‹µë³€í•´ë³´ê¸°\n",
        "\n",
        "### ğŸ“Š ì˜¤ëŠ˜ í•  í‰ê°€\n",
        "- **ì§ˆë¬¸ ë‹µë³€ í’ˆì§ˆ**: ë‹µë³€ì´ ì •í™•í•˜ê³  ìœ ìš©í•œì§€\n",
        "- **RAFT ì„±ëŠ¥**: ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ë‹µë³€í•˜ëŠ”ì§€\n",
        "- **í•œêµ­ì–´ ì´í•´**: í•œêµ­ì–´ ì§ˆë¬¸ì„ ì˜ ì´í•´í•˜ëŠ”ì§€\n",
        "- **ì¼ê´€ì„±**: ê°™ì€ ì§ˆë¬¸ì— ì¼ê´€ëœ ë‹µë³€ì„ í•˜ëŠ”ì§€\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ëª¨ë¸ í‰ê°€ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤\n",
        "import torch\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "from transformers import (\n",
        "    AutoTokenizer,           # í† í¬ë‚˜ì´ì €\n",
        "    AutoModelForCausalLM,    # ì–¸ì–´ ëª¨ë¸\n",
        "    BitsAndBytesConfig       # 4ë¹„íŠ¸ ì–‘ìí™”\n",
        ")\n",
        "from peft import (\n",
        "    PeftModel,               # LoRA ëª¨ë¸ ë¡œë“œ\n",
        "    LoraConfig,              # LoRA ì„¤ì •\n",
        "    TaskType                 # íƒœìŠ¤í¬ íƒ€ì…\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ… ëª¨ë¸ í‰ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "print(f\"ğŸ”¥ PyTorch ë²„ì „: {torch.__version__}\")\n",
        "print(f\"ğŸš€ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. í•™ìŠµëœ ëª¨ë¸ ë¡œë“œí•˜ê¸°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•™ìŠµëœ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤\n",
        "print(\"ğŸ“¥ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
        "\n",
        "# í•™ìŠµ ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "with open(\"models/training_config.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    training_config = json.load(f)\n",
        "\n",
        "model_name = training_config['model_name']\n",
        "print(f\"   - ê¸°ë³¸ ëª¨ë¸: {model_name}\")\n",
        "\n",
        "# 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ\n",
        "print(\"ğŸ¤– ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "print(\"ğŸ”¤ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    padding_side=\"right\"\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# LoRA ëª¨ë¸ ë¡œë“œ\n",
        "print(\"ğŸ”§ LoRA ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
        "model = PeftModel.from_pretrained(base_model, \"models/fine_tuned_lora\")\n",
        "\n",
        "print(\"âœ… í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
        "print(f\"   - ê¸°ë³¸ ëª¨ë¸: {model_name}\")\n",
        "print(f\"   - LoRA ì–´ëŒ‘í„°: models/fine_tuned_lora/\")\n",
        "print(f\"   - ë””ë°”ì´ìŠ¤: {next(model.parameters()).device}\")\n",
        "\n",
        "# ëª¨ë¸ ì •ë³´ í™•ì¸\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "all_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"   - í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,}ê°œ\")\n",
        "print(f\"   - ì „ì²´ íŒŒë¼ë¯¸í„°: {all_params:,}ê°œ\")\n",
        "print(f\"   - í•™ìŠµ ë¹„ìœ¨: {trainable_params/all_params*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜ ë§Œë“¤ê¸°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤ (ë§¤ìš° ê°„ë‹¨í•˜ê²Œ!)\n",
        "def generate_text(model, tokenizer, prompt, max_length=100, temperature=0.7):\n",
        "    \"\"\"\n",
        "    ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
        "    \n",
        "    Args:\n",
        "        model: ì‚¬ìš©í•  ëª¨ë¸\n",
        "        tokenizer: í† í¬ë‚˜ì´ì €\n",
        "        prompt: ì…ë ¥ í…ìŠ¤íŠ¸\n",
        "        max_length: ìµœëŒ€ ìƒì„± ê¸¸ì´\n",
        "        temperature: ì°½ì˜ì„± ì¡°ì ˆ (0.1=ë³´ìˆ˜ì , 1.0=ì°½ì˜ì )\n",
        "    \n",
        "    Returns:\n",
        "        ìƒì„±ëœ í…ìŠ¤íŠ¸\n",
        "    \"\"\"\n",
        "    # ì…ë ¥ í† í°í™”\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # í…ìŠ¤íŠ¸ ìƒì„±\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_length,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # ì…ë ¥ ë¶€ë¶„ ì œê±°í•˜ê³  ìƒì„±ëœ ë¶€ë¶„ë§Œ ë°˜í™˜\n",
        "    generated_part = generated_text[len(prompt):].strip()\n",
        "    \n",
        "    return generated_part\n",
        "\n",
        "# í•¨ìˆ˜ í…ŒìŠ¤íŠ¸\n",
        "print(\"ğŸ§ª í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜ í…ŒìŠ¤íŠ¸:\")\n",
        "test_prompt = \"ì§ˆë¬¸: ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\\në‹µë³€:\"\n",
        "generated = generate_text(model, tokenizer, test_prompt, max_length=50)\n",
        "print(f\"   - ì…ë ¥: {test_prompt}\")\n",
        "print(f\"   - ìƒì„±: {generated}\")\n",
        "print(\"âœ… í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
