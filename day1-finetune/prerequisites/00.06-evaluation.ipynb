{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š Day 1-00.06: ëª¨ë¸ í‰ê°€í•˜ê¸° (ì´ˆë³´ììš©)\n",
    "\n",
    "## ğŸ¯ ì´ë²ˆ ë…¸íŠ¸ë¶ì—ì„œ í•  ì¼\n",
    "- **í•™ìŠµëœ ëª¨ë¸** ë¡œë“œí•˜ê¸°\n",
    "- **ì„±ëŠ¥ í‰ê°€**í•˜ê¸° (ë§¤ìš° ê°„ë‹¨í•˜ê²Œ!)\n",
    "- **ì›ë³¸ ëª¨ë¸ê³¼ ë¹„êµ**í•˜ê¸°\n",
    "- **ê²°ê³¼ ë¶„ì„**í•˜ê¸°\n",
    "\n",
    "## ğŸ’¡ ëª¨ë¸ í‰ê°€ë€?\n",
    "**í•™ìŠµëœ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì˜ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ê²ƒ**\n",
    "\n",
    "### ğŸ”§ í‰ê°€ ë°©ë²•ë“¤\n",
    "1. **ì •ì„±ì  í‰ê°€**: ì‚¬ëŒì´ ì§ì ‘ ë‹µë³€ í’ˆì§ˆ í™•ì¸\n",
    "2. **ì •ëŸ‰ì  í‰ê°€**: ìˆ˜ì¹˜ë¡œ ì„±ëŠ¥ ì¸¡ì •\n",
    "3. **ë¹„êµ í‰ê°€**: ì›ë³¸ ëª¨ë¸ê³¼ ì„±ëŠ¥ ë¹„êµ\n",
    "4. **ì‹¤ì œ ì‚¬ìš©**: ì‹¤ì œ ì§ˆë¬¸ì— ë‹µë³€í•´ë³´ê¸°\n",
    "\n",
    "### ğŸ“Š ì˜¤ëŠ˜ í•  í‰ê°€\n",
    "- **ì§ˆë¬¸ ë‹µë³€ í’ˆì§ˆ**: ë‹µë³€ì´ ì •í™•í•˜ê³  ìœ ìš©í•œì§€\n",
    "- **RAFT ì„±ëŠ¥**: ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ë‹µë³€í•˜ëŠ”ì§€\n",
    "- **í•œêµ­ì–´ ì´í•´**: í•œêµ­ì–´ ì§ˆë¬¸ì„ ì˜ ì´í•´í•˜ëŠ”ì§€\n",
    "- **ì¼ê´€ì„±**: ê°™ì€ ì§ˆë¬¸ì— ì¼ê´€ëœ ë‹µë³€ì„ í•˜ëŠ”ì§€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ í‰ê°€ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from transformers import (\n",
    "    AutoTokenizer,           # í† í¬ë‚˜ì´ì €\n",
    "    AutoModelForCausalLM,    # ì–¸ì–´ ëª¨ë¸\n",
    "    BitsAndBytesConfig       # 4ë¹„íŠ¸ ì–‘ìí™”\n",
    ")\n",
    "from peft import (\n",
    "    PeftModel,               # LoRA ëª¨ë¸ ë¡œë“œ\n",
    "    LoraConfig,              # LoRA ì„¤ì •\n",
    "    TaskType                 # íƒœìŠ¤í¬ íƒ€ì…\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… ëª¨ë¸ í‰ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(f\"ğŸ”¥ PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"ğŸš€ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í•™ìŠµëœ ëª¨ë¸ ë¡œë“œí•˜ê¸°\n",
    "\n",
    "> ğŸ’¡ ì´ ë…¸íŠ¸ë¶ì€ ê°œë… í•™ìŠµìš© ì˜ˆì‹œì…ë‹ˆë‹¤. ì‹¤ì œë¡œëŠ” `main-practice/03_fine_tuning_with_lora.ipynb`ì—ì„œ í•™ìŠµê³¼ ì €ì¥ì´ ëë‚¬ë‹¤ëŠ” ê°€ì •í•˜ì— ê²°ê³¼ë§Œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "> ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ì´ ì—†ë‹¤ë©´ ì•„ë˜ ì…€ì€ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ íë¦„ë§Œ ë³´ì—¬ì¤ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ê³¼ì •ì„ ê°œë…ì ìœ¼ë¡œ ì‚´í´ë´…ë‹ˆë‹¤ (ì‹¤ì œ íŒŒì¼ ì ‘ê·¼ ì—†ìŒ)\n",
    "print(\"ğŸ“¥ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì‹œë‚˜ë¦¬ì˜¤\")\n",
    "print(\"   - ì´ ë…¸íŠ¸ë¶ì€ main-practice/03_fine_tuning_with_lora.ipynbì—ì„œ í•™ìŠµê³¼ ì €ì¥ì´ ëë‚¬ë‹¤ê³  ê°€ì •í•˜ê³  ì ˆì°¨ë§Œ ì„¤ëª…í•©ë‹ˆë‹¤.\")\n",
    "print(\"   - Colab í™˜ê²½ì—ì„œë„ í° íŒŒì¼ì„ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¬ í•„ìš” ì—†ì´, ì–´ë–¤ ì •ë³´ê°€ ì €ì¥ë˜ì—ˆëŠ”ì§€ë§Œ í™•ì¸í•˜ë©´ ë©ë‹ˆë‹¤.\")\n",
    "\n",
    "MODEL_READY = False\n",
    "model = None\n",
    "base_model = None\n",
    "tokenizer = None\n",
    "training_config = {\n",
    "    \"model_name\": \"LG/EXAONE-3.0-7.8B-Instruct\",\n",
    "    \"lora_config\": {\n",
    "        \"r\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "    \"training_args\": {\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"per_device_train_batch_size\": 4,\n",
    "        \"learning_rate\": 2e-4\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ğŸ—‚ï¸ training_config.json ì˜ˆì‹œ\")\n",
    "for key, value in training_config.items():\n",
    "    print(f\"   - {key}: {value}\")\n",
    "\n",
    "print(\"ğŸ”§ LoRA ì–´ëŒ‘í„° êµ¬ì¡° ìš”ì•½ (models/fine_tuned_lora/)\")\n",
    "print(\"   - adapter_config.json: ìœ„ LoRA ì„¤ì •ì´ ì €ì¥\")\n",
    "print(\"   - adapter_model.bin: í•™ìŠµëœ LoRA ê°€ì¤‘ì¹˜\")\n",
    "print(\"   - tokenizer/ : í† í¬ë‚˜ì´ì € ì„¤ì •\")\n",
    "\n",
    "print(\"âœ… ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” ìœ„ íŒŒì¼ë“¤ì„ ë©”ì¸ ë…¸íŠ¸ë¶ì—ì„œ ì €ì¥í•œ ë’¤ ì—¬ê¸°ì„œ ë¶ˆëŸ¬ì™€ í‰ê°€í•©ë‹ˆë‹¤.\")\n",
    "print(\"ğŸ“ MODEL_READY:\", MODEL_READY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜ ê±´ë„ˆë›°ê¸°\n",
    "\n",
    "ëª©ì—… í‰ê°€ì—ì„œëŠ” ì‹¤ì œ ëª¨ë¸ì„ í˜¸ì¶œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë°”ë¡œ ì•„ë˜ ìƒ˜í”Œ ë°ì´í„°(`sample_data`)ë¥¼ ì‚¬ìš©í•´ í‰ê°€ ì§€í‘œ ê³„ì‚° íë¦„ì„ ì—°ìŠµí•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. í‰ê°€ ì§€í‘œ ì´í•´í•˜ê¸°\n",
    "\n",
    "í•™ìŠµí•œ ëª¨ë¸ì„ ì‹¤ì œë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” ëŒ€ì‹ , ì˜ˆì‹œ ë‹µë³€ì„ ê°€ì§€ê³  ëŒ€í‘œì ì¸ ìë™ í‰ê°€ ì§€í‘œë¥¼ ê³„ì‚°í•´ ë´…ë‹ˆë‹¤.\n",
    "- **ROUGE-1**: ì •ë‹µê³¼ ì˜ˆì¸¡ ì‚¬ì´ì˜ ë‹¨ì–´ ì¤‘ë³µ ë¹„ìœ¨(ë¦¬ì½œ).\n",
    "- **BLEU**: ì •ë‹µ ë¬¸ì¥ì„ ì–¼ë§ˆë‚˜ ì˜ ë”°ë¼ ì¼ëŠ”ì§€ ë³´ëŠ” ì •ë°€ë„ ê¸°ë°˜ ì ìˆ˜.\n",
    "- **ì½”ì‚¬ì¸ ìœ ì‚¬ë„**: ë‘ ë¬¸ì¥ì„ ë²¡í„°ë¡œ ë°”ê¿” ê°ë„ë¥¼ ë¹„êµí•˜ëŠ” ì§€í‘œ(1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬).\n",
    "\n",
    "> ğŸ” ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” `generate_text`ë¡œ ì–»ì€ ì‘ë‹µê³¼ ì •ë‹µì„ ì´ ì§€í‘œì— ë„£ì–´ ë¹„êµí•˜ë©´ ë©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í‰ê°€ìš© ìƒ˜í”Œ ë°ì´í„° (ì˜ˆì‹œ)\n",
    "import pandas as pd\n",
    "\n",
    "sample_data = [\n",
    "    {\n",
    "        \"question\": \"ì§ˆë¬¸: ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "        \"reference\": \"ì¸ê³µì§€ëŠ¥ì€ ë°ì´í„°ë¥¼ í•™ìŠµí•´ íŒ¨í„´ì„ ì°¾ê³  ì˜ì‚¬ê²°ì •ì„ ìë™í™”í•˜ëŠ” ì»´í“¨í„° ê¸°ìˆ ì…ë‹ˆë‹¤.\",\n",
    "        \"baseline_answer\": \"ì¸ê³µì§€ëŠ¥ì€ ì‚¬ëŒì´ ë§Œë“  ê·œì¹™ì„ ì‹¤í–‰í•˜ëŠ” í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤.\",\n",
    "        \"lora_answer\": \"ì¸ê³µì§€ëŠ¥ì€ ë°ì´í„°ë¥¼ í•™ìŠµí•´ ìŠ¤ìŠ¤ë¡œ íŒ¨í„´ì„ ì°¾ì•„ ì˜ì‚¬ê²°ì •ì„ ë•ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"ì§ˆë¬¸: ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì€?\",\n",
    "        \"reference\": \"ë¨¸ì‹ ëŸ¬ë‹ì€ ì‚¬ëŒì´ íŠ¹ì§•ì„ ì •ì˜í•œ ë’¤ í•™ìŠµì‹œí‚¤ê³ , ë”¥ëŸ¬ë‹ì€ ì‹ ê²½ë§ì´ íŠ¹ì§• ì¶”ì¶œê¹Œì§€ í•¨ê»˜ ìˆ˜í–‰í•©ë‹ˆë‹¤.\",\n",
    "        \"baseline_answer\": \"ë¨¸ì‹ ëŸ¬ë‹ì€ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³  ë”¥ëŸ¬ë‹ì€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\",\n",
    "        \"lora_answer\": \"ë¨¸ì‹ ëŸ¬ë‹ì€ ì‚¬ëŒì´ íŠ¹ì§•ì„ ì„¤ê³„í•˜ê³ , ë”¥ëŸ¬ë‹ì€ ì‹ ê²½ë§ì´ íŠ¹ì§• ì¶”ì¶œê¹Œì§€ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"ì§ˆë¬¸: ìì—°ì–´ ì²˜ë¦¬ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "        \"reference\": \"ìì—°ì–´ ì²˜ë¦¬ëŠ” ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•˜ê³  ìƒì„±í•˜ë„ë¡ ë§Œë“œëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\",\n",
    "        \"baseline_answer\": \"ìì—°ì–´ ì²˜ë¦¬ëŠ” ì–¸ì–´ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\",\n",
    "        \"lora_answer\": \"ìì—°ì–´ ì²˜ë¦¬ëŠ” ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•˜ê³  ìƒì„±í•˜ë„ë¡ ë•ëŠ” ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì…ë‹ˆë‹¤.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "df_samples = pd.DataFrame(sample_data)\n",
    "print(\"ğŸ—‚ï¸ ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\")\n",
    "display(df_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í‰ê°€ ì§€í‘œ ê³„ì‚° í•¨ìˆ˜ ì •ì˜\n",
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "TOKEN_PATTERN = re.compile(r\"[\\wê°€-í£]+\", re.UNICODE)\n",
    "\n",
    "def tokenize(text):\n",
    "    return TOKEN_PATTERN.findall(text.lower())\n",
    "\n",
    "def compute_rouge1(reference, candidate):\n",
    "    ref_tokens = tokenize(reference)\n",
    "    cand_tokens = tokenize(candidate)\n",
    "    ref_counts = Counter(ref_tokens)\n",
    "    overlap = 0\n",
    "    for token in cand_tokens:\n",
    "        if ref_counts[token] > 0:\n",
    "            overlap += 1\n",
    "            ref_counts[token] -= 1\n",
    "    return overlap / len(ref_tokens) if ref_tokens else 0.0\n",
    "\n",
    "def compute_bleu(reference, candidate):\n",
    "    ref_tokens = tokenize(reference)\n",
    "    cand_tokens = tokenize(candidate)\n",
    "    if not cand_tokens:\n",
    "        return 0.0\n",
    "\n",
    "    precisions = []\n",
    "    for n in range(1, 5):\n",
    "        ref_ngrams = Counter(tuple(ref_tokens[i:i+n]) for i in range(len(ref_tokens) - n + 1))\n",
    "        cand_ngrams = Counter(tuple(cand_tokens[i:i+n]) for i in range(len(cand_tokens) - n + 1))\n",
    "        if len(cand_ngrams) == 0:\n",
    "            precisions.append(0.0)\n",
    "            continue\n",
    "        overlap = 0\n",
    "        for ngram, count in cand_ngrams.items():\n",
    "            overlap += min(count, ref_ngrams.get(ngram, 0))\n",
    "        precisions.append(overlap / sum(cand_ngrams.values()))\n",
    "\n",
    "    if any(p == 0 for p in precisions):\n",
    "        geo_mean = 0.0\n",
    "    else:\n",
    "        geo_mean = math.exp(sum(math.log(p) for p in precisions) / 4)\n",
    "\n",
    "    ref_len = len(ref_tokens)\n",
    "    cand_len = len(cand_tokens)\n",
    "    if cand_len == 0:\n",
    "        bp = 0.0\n",
    "    elif cand_len > ref_len:\n",
    "        bp = 1.0\n",
    "    else:\n",
    "        bp = math.exp(1 - ref_len / cand_len)\n",
    "\n",
    "    return bp * geo_mean\n",
    "\n",
    "def compute_cosine_similarity(reference, candidate):\n",
    "    ref_tokens = tokenize(reference)\n",
    "    cand_tokens = tokenize(candidate)\n",
    "    vocab = set(ref_tokens) | set(cand_tokens)\n",
    "    if not vocab:\n",
    "        return 0.0\n",
    "\n",
    "    ref_counts = Counter(ref_tokens)\n",
    "    cand_counts = Counter(cand_tokens)\n",
    "    dot = sum(ref_counts[token] * cand_counts[token] for token in vocab)\n",
    "    ref_norm = math.sqrt(sum(count * count for count in ref_counts.values()))\n",
    "    cand_norm = math.sqrt(sum(count * count for count in cand_counts.values()))\n",
    "    if ref_norm == 0 or cand_norm == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return dot / (ref_norm * cand_norm)\n",
    "\n",
    "print(\"ğŸ› ï¸ í‰ê°€ ì§€í‘œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒ˜í”Œ ë°ì´í„°ë¡œ í‰ê°€ ì§€í‘œ ê³„ì‚°\n",
    "results = []\n",
    "\n",
    "for row in sample_data:\n",
    "    reference = row['reference']\n",
    "    baseline = row['baseline_answer']\n",
    "    lora = row['lora_answer']\n",
    "\n",
    "    baseline_scores = {\n",
    "        'rouge1': compute_rouge1(reference, baseline),\n",
    "        'bleu': compute_bleu(reference, baseline),\n",
    "        'cosine': compute_cosine_similarity(reference, baseline)\n",
    "    }\n",
    "    lora_scores = {\n",
    "        'rouge1': compute_rouge1(reference, lora),\n",
    "        'bleu': compute_bleu(reference, lora),\n",
    "        'cosine': compute_cosine_similarity(reference, lora)\n",
    "    }\n",
    "\n",
    "    results.append({\n",
    "        'ì§ˆë¬¸': row['question'],\n",
    "        'ê¸°ì¤€ ëª¨ë¸ ROUGE-1': baseline_scores['rouge1'],\n",
    "        'LoRA ROUGE-1': lora_scores['rouge1'],\n",
    "        'ê¸°ì¤€ ëª¨ë¸ BLEU': baseline_scores['bleu'],\n",
    "        'LoRA BLEU': lora_scores['bleu'],\n",
    "        'ê¸°ì¤€ ëª¨ë¸ ì½”ì‚¬ì¸': baseline_scores['cosine'],\n",
    "        'LoRA ì½”ì‚¬ì¸': lora_scores['cosine']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.round(3)\n",
    "print(\"ğŸ“ˆ ìƒ˜í”Œ í‰ê°€ ê²°ê³¼\")\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í‰ê·  ì ìˆ˜ ë¹„êµ\n",
    "metric_summary = pd.DataFrame({\n",
    "    'Metric': ['ROUGE-1', 'BLEU', 'ì½”ì‚¬ì¸ ìœ ì‚¬ë„'],\n",
    "    'Baseline': [\n",
    "        results_df['ê¸°ì¤€ ëª¨ë¸ ROUGE-1'].mean(),\n",
    "        results_df['ê¸°ì¤€ ëª¨ë¸ BLEU'].mean(),\n",
    "        results_df['ê¸°ì¤€ ëª¨ë¸ ì½”ì‚¬ì¸'].mean()\n",
    "    ],\n",
    "    'LoRA': [\n",
    "        results_df['LoRA ROUGE-1'].mean(),\n",
    "        results_df['LoRA BLEU'].mean(),\n",
    "        results_df['LoRA ì½”ì‚¬ì¸'].mean()\n",
    "    ]\n",
    "}).round(3)\n",
    "\n",
    "print(\"ğŸ“Š í‰ê·  ì„±ëŠ¥ ë¹„êµ\")\n",
    "display(metric_summary)\n",
    "\n",
    "improvements = metric_summary['LoRA'] - metric_summary['Baseline']\n",
    "for metric, delta in zip(metric_summary['Metric'], improvements):\n",
    "    arrow = 'ğŸ”¼' if delta > 0 else ('ğŸ”½' if delta < 0 else 'â–')\n",
    "    print(f\"{arrow} {metric}: {delta:+.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ê²°ê³¼ í•´ì„í•˜ê¸°\n",
    "\n",
    "- í‘œë¥¼ ë³´ë©´ LoRA ë‹µë³€ì´ ê¸°ì¤€ ëª¨ë¸ë³´ë‹¤ ëª¨ë“  ì§€í‘œì—ì„œ ë†’ì€ ê°’ì„ ê°–ë„ë¡ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.\n",
    "- **ROUGE-1**ì´ ë†’ìœ¼ë©´ ì •ë‹µê³¼ ë‹¨ì–´ê°€ ë” ë§ì´ ê²¹ì¹˜ë¯€ë¡œ ë‚´ìš©ì´ ìœ ì‚¬í•˜ë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.\n",
    "- **BLEU**ëŠ” n-ê·¸ë¨ ì •ë°€ë„ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë¬¸ì¥ ì–´ìˆœê³¼ í‘œí˜„ë ¥ì´ ì •ë‹µê³¼ ë¹„ìŠ·í• ìˆ˜ë¡ ì ìˆ˜ê°€ ì˜¤ë¦…ë‹ˆë‹¤.\n",
    "- **ì½”ì‚¬ì¸ ìœ ì‚¬ë„**ëŠ” ë¬¸ì¥ ë²¡í„°ì˜ ê°ë„ë¥¼ ë¹„êµí•˜ë¯€ë¡œ ì „ì²´ì ì¸ ì˜ë¯¸ê°€ ë¹„ìŠ·í• ìˆ˜ë¡ 1ì— ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë§ˆë¬´ë¦¬ ì •ë¦¬\n",
    "- ì§€ê¸ˆì€ ì˜ˆì‹œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ ROUGE, BLEU, ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í–ˆìŠµë‹ˆë‹¤.\n",
    "- ì‹¤ì œ í‰ê°€ì—ì„œëŠ” `generate_text`ë¡œ ì–»ì€ ëª¨ë¸ ì‘ë‹µì„ `sample_data` ëŒ€ì‹  ë„£ì–´ ë™ì¼í•œ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë©´ ë©ë‹ˆë‹¤.\n",
    "- ë©”íŠ¸ë¦­ì´ ëª¨ë‘ í–¥ìƒë˜ì—ˆë‹¤ë©´ LoRA íŒŒì¸íŠœë‹ì´ íš¨ê³¼ì ì´ì—ˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "- í•„ìš”ì— ë”°ë¼ METEOR, BERTScore ë“± ë‹¤ë¥¸ ì§€í‘œë„ ê°™ì€ êµ¬ì¡°ë¡œ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
