{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# π“ Day 1-00.05: LoRA νμΈνλ‹ κ°λ… μ΄ν•΄ν•κΈ° (μ΄λ³΄μμ©)\n",
        "\n",
        "## π― μ΄λ² λ…ΈνΈλ¶μ—μ„ ν•  μΌ\n",
        "- **LoRAμ ν•µμ‹¬ κ°λ…** μ΄ν•΄ν•κΈ°\n",
        "- **νμΈνλ‹ κ³Όμ •** μ‹κ°ν™”λ΅ λ³΄κΈ°\n",
        "- **μ‹¤μ  μ½”λ“** λ§›λ³΄κΈ° (μ‹¤ν–‰ν•μ§€ μ•μ)\n",
        "- **λ©”λ¨λ¦¬ μ μ•½** μ›λ¦¬ μ΄ν•΄ν•κΈ°\n",
        "\n",
        "## π’΅ LoRAλ€ λ¬΄μ—‡μΈκ°€?\n",
        "\n",
        "### π”§ LoRA (Low-Rank Adaptation)\n",
        "**μ „μ²΄ λ¨λΈμ„ λ‹¤μ‹ ν•™μµν•μ§€ μ•κ³ , μ‘μ€ μ–΄λ‘ν„°λ§ μ¶”κ°€ν•΄μ„ ν•™μµν•λ” λ°©λ²•**\n",
        "\n",
        "### π― μ™ LoRAλ¥Ό μ‚¬μ©ν• κΉ?\n",
        "\n",
        "#### β μΌλ° νμΈνλ‹μ λ¬Έμ μ \n",
        "- **λ©”λ¨λ¦¬ λ¶€μ΅±**: 7B λ¨λΈ = 28GB+ λ©”λ¨λ¦¬ ν•„μ”\n",
        "- **μ‹κ°„ μ¤λ κ±Έλ¦Ό**: μ „μ²΄ λ¨λΈμ„ λ‹¤μ‹ ν•™μµ\n",
        "- **λΉ„μ© λ†’μ**: GPU λ¦¬μ†μ¤ λ§μ΄ μ‚¬μ©\n",
        "- **κ³Όμ ν•© μ„ν—**: μ‘μ€ λ°μ΄ν„°λ΅ ν° λ¨λΈ ν•™μµ\n",
        "\n",
        "#### β… LoRAμ μ¥μ \n",
        "- **λ©”λ¨λ¦¬ μ μ•½**: 1%λ§ ν•™μµ (28GB β†’ 2GB)\n",
        "- **λΉ λ¥Έ ν•™μµ**: μ‘μ€ μ–΄λ‘ν„°λ§ μ—…λ°μ΄νΈ\n",
        "- **μ•μ •μ **: μ›λ³Έ λ¨λΈ μ„±λ¥ μ μ§€\n",
        "- **ν¨μ¨μ **: μ—¬λ¬ νƒμ¤ν¬μ— μ¬μ‚¬μ© κ°€λ¥\n",
        "\n",
        "### π§  LoRA μ‘λ™ μ›λ¦¬\n",
        "\n",
        "#### 1οΈβƒ£ κΈ°μ΅΄ λ°©μ‹ (Full Fine-tuning)\n",
        "```\n",
        "μ›λ³Έ λ¨λΈ (7B νλΌλ―Έν„°)\n",
        "    β†“ μ „μ²΄ ν•™μµ\n",
        "μƒλ΅μ΄ λ¨λΈ (7B νλΌλ―Έν„°)\n",
        "```\n",
        "\n",
        "#### 2οΈβƒ£ LoRA λ°©μ‹\n",
        "```\n",
        "μ›λ³Έ λ¨λΈ (7B νλΌλ―Έν„°) + LoRA μ–΄λ‘ν„° (0.1B νλΌλ―Έν„°)\n",
        "    β†“ μ–΄λ‘ν„°λ§ ν•™μµ\n",
        "μ›λ³Έ λ¨λΈ + ν•™μµλ μ–΄λ‘ν„°\n",
        "```\n",
        "\n",
        "### π” LoRA μ–΄λ‘ν„° κµ¬μ΅°\n",
        "```\n",
        "μ…λ ¥ β†’ [μ›λ³Έ λ μ΄μ–΄] β†’ [LoRA A] β†’ [LoRA B] β†’ μ¶λ ¥\n",
        "           (κ³ μ •)        (ν•™μµ)     (ν•™μµ)\n",
        "```\n",
        "\n",
        "- **μ›λ³Έ λ μ΄μ–΄**: ν•™μµν•μ§€ μ•μ (κ³ μ •)\n",
        "- **LoRA A, B**: μ‘μ€ ν–‰λ ¬λ“¤λ§ ν•™μµ\n",
        "- **κ²°κ³Ό**: μ›λ³Έ + μ–΄λ‘ν„°μ μ΅°ν•©\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. LoRA νλΌλ―Έν„° μ΄ν•΄ν•κΈ°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LoRAμ ν•µμ‹¬ νλΌλ―Έν„°λ“¤μ„ μ΄ν•΄ν•΄λ΄…μ‹λ‹¤\n",
        "print(\"π”§ LoRA νλΌλ―Έν„° μ„¤λ…\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 1. r (rank) - LoRAμ ν•µμ‹¬ νλΌλ―Έν„°\n",
        "print(\"1οΈβƒ£ r (rank): LoRAμ 'ν¬κΈ°'\")\n",
        "print(\"   - μ‘μ„μλ΅: λΉ λ¥΄κ³  λ©”λ¨λ¦¬ μ κ² μ‚¬μ©, μ •ν™•λ„ λ‚®μ\")\n",
        "print(\"   - ν΄μλ΅: λλ¦¬κ³  λ©”λ¨λ¦¬ λ§μ΄ μ‚¬μ©, μ •ν™•λ„ λ†’μ\")\n",
        "print(\"   - μΌλ°μ  λ²”μ„: 4, 8, 16, 32, 64\")\n",
        "print(\"   - μμ‹: r=16 β†’ 16μ°¨μ› μ–΄λ‘ν„°\")\n",
        "\n",
        "print(\"\\n2οΈβƒ£ lora_alpha: ν•™μµ κ°•λ„ μ΅°μ \")\n",
        "print(\"   - λ†’μ„μλ΅: λ” κ°•ν•κ² ν•™μµ\")\n",
        "print(\"   - λ‚®μ„μλ΅: λ” λ¶€λ“λ½κ² ν•™μµ\")\n",
        "print(\"   - μΌλ°μ  κ°’: rμ 2λ°° (r=16 β†’ alpha=32)\")\n",
        "\n",
        "print(\"\\n3οΈβƒ£ target_modules: μ–΄λ–¤ λ μ΄μ–΄λ¥Ό ν•™μµν• μ§€\")\n",
        "print(\"   - 'q_proj': Query ν”„λ΅μ μ… (μ–΄ν…μ…μ μ§λ¬Έ λ¶€λ¶„)\")\n",
        "print(\"   - 'v_proj': Value ν”„λ΅μ μ… (μ–΄ν…μ…μ κ°’ λ¶€λ¶„)\")\n",
        "print(\"   - 'k_proj': Key ν”„λ΅μ μ… (μ–΄ν…μ…μ ν‚¤ λ¶€λ¶„)\")\n",
        "print(\"   - 'o_proj': Output ν”„λ΅μ μ… (μ–΄ν…μ… μ¶λ ¥)\")\n",
        "\n",
        "print(\"\\n4οΈβƒ£ lora_dropout: κ³Όμ ν•© λ°©μ§€\")\n",
        "print(\"   - 0.05~0.1: μΌλ°μ  λ²”μ„\")\n",
        "print(\"   - λ†’μ„μλ΅: κ³Όμ ν•© λ°©μ§€, ν•™μµ μ–΄λ ¤μ›€\")\n",
        "print(\"   - λ‚®μ„μλ΅: ν•™μµ μ‰¬μ›€, κ³Όμ ν•© μ„ν—\")\n",
        "\n",
        "# μ‹¤μ  LoRA μ„¤μ • μμ‹\n",
        "print(\"\\nπ“‹ μ‹¤μ  LoRA μ„¤μ • μμ‹:\")\n",
        "print(\"   r=16                    # μ λ‹Ήν• ν¬κΈ°\")\n",
        "print(\"   lora_alpha=32           # rμ 2λ°°\")\n",
        "print(\"   target_modules=['q_proj', 'v_proj']  # μ–΄ν…μ… λ μ΄μ–΄λ“¤\")\n",
        "print(\"   lora_dropout=0.05       # κ³Όμ ν•© λ°©μ§€\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. λ©”λ¨λ¦¬ μ μ•½ ν¨κ³Ό μ‹κ°ν™”\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LoRAμ λ©”λ¨λ¦¬ μ μ•½ ν¨κ³Όλ¥Ό μ‹κ°ν™”ν•΄λ΄…μ‹λ‹¤\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(\"π“ LoRA λ©”λ¨λ¦¬ μ μ•½ ν¨κ³Ό λΉ„κµ\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# λ¨λΈ ν¬κΈ°λ³„ λ©”λ¨λ¦¬ μ‚¬μ©λ‰ (GB)\n",
        "models = ['7B λ¨λΈ', '13B λ¨λΈ', '30B λ¨λΈ', '70B λ¨λΈ']\n",
        "full_tuning = [28, 52, 120, 280]  # Full fine-tuning λ©”λ¨λ¦¬\n",
        "lora_tuning = [2, 4, 8, 16]       # LoRA λ©”λ¨λ¦¬\n",
        "\n",
        "# κ·Έλν”„ μƒμ„±\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# 1. λ©”λ¨λ¦¬ μ‚¬μ©λ‰ λΉ„κµ\n",
        "x = np.arange(len(models))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax1.bar(x - width/2, full_tuning, width, label='Full Fine-tuning', color='red', alpha=0.7)\n",
        "bars2 = ax1.bar(x + width/2, lora_tuning, width, label='LoRA', color='green', alpha=0.7)\n",
        "\n",
        "ax1.set_xlabel('λ¨λΈ ν¬κΈ°')\n",
        "ax1.set_ylabel('λ©”λ¨λ¦¬ μ‚¬μ©λ‰ (GB)')\n",
        "ax1.set_title('λ©”λ¨λ¦¬ μ‚¬μ©λ‰ λΉ„κµ')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(models)\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# κ°’ ν‘μ‹\n",
        "for bar in bars1:\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "             f'{int(height)}GB', ha='center', va='bottom')\n",
        "\n",
        "for bar in bars2:\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "             f'{int(height)}GB', ha='center', va='bottom')\n",
        "\n",
        "# 2. μ μ•½λ¥  κ³„μ‚°\n",
        "savings = [(f-l)/f*100 for f, l in zip(full_tuning, lora_tuning)]\n",
        "bars3 = ax2.bar(models, savings, color='blue', alpha=0.7)\n",
        "\n",
        "ax2.set_xlabel('λ¨λΈ ν¬κΈ°')\n",
        "ax2.set_ylabel('λ©”λ¨λ¦¬ μ μ•½λ¥  (%)')\n",
        "ax2.set_title('LoRA λ©”λ¨λ¦¬ μ μ•½λ¥ ')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# κ°’ ν‘μ‹\n",
        "for bar in bars3:\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "             f'{height:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# μ μ•½ ν¨κ³Ό μ”μ•½\n",
        "print(\"\\nπ’΅ LoRA λ©”λ¨λ¦¬ μ μ•½ ν¨κ³Ό:\")\n",
        "for i, model in enumerate(models):\n",
        "    print(f\"   {model}: {full_tuning[i]}GB β†’ {lora_tuning[i]}GB ({savings[i]:.1f}% μ μ•½)\")\n",
        "\n",
        "print(f\"\\nπ― ν•µμ‹¬ ν¬μΈνΈ:\")\n",
        "print(f\"   - 7B λ¨λΈ: 28GB β†’ 2GB (93% μ μ•½!)\")\n",
        "print(f\"   - 70B λ¨λΈ: 280GB β†’ 16GB (94% μ μ•½!)\")\n",
        "print(f\"   - μΌλ°μ μΈ GPUλ΅λ„ λ€ν• λ¨λΈ νμΈνλ‹ κ°€λ¥\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. μ‹¤μ  νμΈνλ‹ μ½”λ“ λ§›λ³΄κΈ° (μ‹¤ν–‰ν•μ§€ μ•μ)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# μ‹¤μ  νμΈνλ‹ μ½”λ“λ¥Ό λ§›λ³΄κΈ°λ΅ μ‚΄ν΄λ΄…μ‹λ‹¤ (μ‹¤ν–‰ν•μ§€ μ•μ!)\n",
        "print(\"π“ μ‹¤μ  νμΈνλ‹ μ½”λ“ κµ¬μ΅°\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"1οΈβƒ£ λ°μ΄ν„° μ¤€λΉ„ λ‹¨κ³„:\")\n",
        "print(\"\"\"\n",
        "# RAFT λ°μ΄ν„° λ΅λ“\n",
        "raft_dataset = load_from_disk(\"data/raft_dataset\")\n",
        "\n",
        "# ν† ν°ν™” ν•¨μ\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "# λ°μ΄ν„°μ…‹ ν† ν°ν™”\n",
        "tokenized_dataset = raft_dataset.map(tokenize_function, batched=True)\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n2οΈβƒ£ ν•™μµ μ„¤μ • λ‹¨κ³„:\")\n",
        "print(\"\"\"\n",
        "# ν•™μµ μ„¤μ •\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,           # 3λ² λ°λ³µ ν•™μµ\n",
        "    per_device_train_batch_size=4, # λ°°μΉ ν¬κΈ°\n",
        "    gradient_accumulation_steps=4, # κ·Έλλ””μ–ΈνΈ λ„μ \n",
        "    warmup_steps=100,             # μ›λ°μ—…\n",
        "    learning_rate=2e-4,           # ν•™μµλ¥ \n",
        "    fp16=True,                    # 16λΉ„νΈ ν•™μµ\n",
        "    logging_steps=10,             # λ΅κΉ… μ£ΌκΈ°\n",
        "    save_steps=500,               # μ €μ¥ μ£ΌκΈ°\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n3οΈβƒ£ ν•™μµ μ‹¤ν–‰ λ‹¨κ³„:\")\n",
        "print(\"\"\"\n",
        "# λ°μ΄ν„° μ •λ¦¬κΈ°\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # μ–Έμ–΄ λ¨λΈλ§\n",
        ")\n",
        "\n",
        "# νΈλ μ΄λ„ μƒμ„±\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# μ‹¤μ  ν•™μµ μ‹¤ν–‰ (μ΄ λ¶€λ¶„μ΄ μ‹κ°„μ΄ μ¤λ κ±Έλ¦Ό!)\n",
        "print(\"π€ νμΈνλ‹ μ‹μ‘...\")\n",
        "trainer.train()\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n4οΈβƒ£ λ¨λΈ μ €μ¥ λ‹¨κ³„:\")\n",
        "print(\"\"\"\n",
        "# ν•™μµλ λ¨λΈ μ €μ¥\n",
        "model.save_pretrained(\"./fine_tuned_model\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
        "\n",
        "print(\"β… νμΈνλ‹ μ™„λ£!\")\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nπ’΅ μ‹¤μ  μ‹¤ν–‰ μ‹ μ£Όμμ‚¬ν•­:\")\n",
        "print(\"   - GPU λ©”λ¨λ¦¬: μµμ† 8GB μ΄μƒ ν•„μ”\")\n",
        "print(\"   - ν•™μµ μ‹κ°„: λ°μ΄ν„° ν¬κΈ°μ— λ”°λΌ 1-10μ‹κ°„\")\n",
        "print(\"   - λ¨λ‹ν„°λ§: ν•™μµ κ³Όμ •μ„ μ‹¤μ‹κ°„μΌλ΅ ν™•μΈ\")\n",
        "print(\"   - μ €μ¥: μ •κΈ°μ μΌλ΅ μ²΄ν¬ν¬μΈνΈ μ €μ¥\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. LoRA vs Full Fine-tuning λΉ„κµ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LoRAμ™€ Full Fine-tuningμ„ λΉ„κµν•΄λ΄…μ‹λ‹¤\n",
        "print(\"β–οΈ LoRA vs Full Fine-tuning λΉ„κµ\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# λΉ„κµν‘ μƒμ„±\n",
        "import pandas as pd\n",
        "\n",
        "comparison_data = {\n",
        "    'ν•­λ©': [\n",
        "        'λ©”λ¨λ¦¬ μ‚¬μ©λ‰',\n",
        "        'ν•™μµ μ‹κ°„',\n",
        "        'λ¨λΈ ν¬κΈ°',\n",
        "        'ν•™μµ νλΌλ―Έν„°',\n",
        "        'μ •ν™•λ„',\n",
        "        'μ•μ •μ„±',\n",
        "        'λΉ„μ©',\n",
        "        'μ¬μ‚¬μ©μ„±'\n",
        "    ],\n",
        "    'Full Fine-tuning': [\n",
        "        '28GB+ (7B λ¨λΈ)',\n",
        "        'λ§¤μ° μ¤λ (10+ μ‹κ°„)',\n",
        "        '7B νλΌλ―Έν„°',\n",
        "        '7B νλΌλ―Έν„°',\n",
        "        'λ§¤μ° λ†’μ',\n",
        "        'λ‚®μ (κ³Όμ ν•© μ„ν—)',\n",
        "        'λ§¤μ° λ†’μ',\n",
        "        'μ–΄λ ¤μ›€'\n",
        "    ],\n",
        "    'LoRA': [\n",
        "        '2GB (7B λ¨λΈ)',\n",
        "        'λΉ λ¦„ (1-3 μ‹κ°„)',\n",
        "        '7B + 0.1B μ–΄λ‘ν„°',\n",
        "        '0.1B νλΌλ―Έν„°',\n",
        "        'λ†’μ (95% μμ¤€)',\n",
        "        'λ†’μ (μ›λ³Έ μ μ§€)',\n",
        "        'λ‚®μ',\n",
        "        'μ‰¬μ›€ (μ–΄λ‘ν„°λ§ κµμ²΄)'\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(comparison_data)\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "print(\"\\nπ― ν•µμ‹¬ μ°¨μ΄μ :\")\n",
        "print(\"1οΈβƒ£ λ©”λ¨λ¦¬: LoRAκ°€ 93% μ μ•½\")\n",
        "print(\"2οΈβƒ£ μ‹κ°„: LoRAκ°€ 3-5λ°° λΉ λ¦„\")\n",
        "print(\"3οΈβƒ£ μ •ν™•λ„: LoRAκ°€ 95% μμ¤€ μ μ§€\")\n",
        "print(\"4οΈβƒ£ λΉ„μ©: LoRAκ°€ 10λ°° μ €λ ΄\")\n",
        "\n",
        "print(\"\\nπ’΅ μ–Έμ  λ¬΄μ—‡μ„ μ‚¬μ©ν• κΉ?\")\n",
        "print(\"β… LoRA μ‚¬μ© μ‹κΈ°:\")\n",
        "print(\"   - λ©”λ¨λ¦¬κ°€ λ¶€μ΅±ν•  λ•\")\n",
        "print(\"   - λΉ λ¥Έ μ‹¤ν—μ΄ ν•„μ”ν•  λ•\")\n",
        "print(\"   - μ—¬λ¬ νƒμ¤ν¬λ¥Ό μ‹λ„ν•  λ•\")\n",
        "print(\"   - λΉ„μ©μ„ μ μ•½ν•κ³  μ‹¶μ„ λ•\")\n",
        "\n",
        "print(\"\\nβ… Full Fine-tuning μ‚¬μ© μ‹κΈ°:\")\n",
        "print(\"   - μµκ³  μ •ν™•λ„κ°€ ν•„μ”ν•  λ•\")\n",
        "print(\"   - μ¶©λ¶„ν• λ©”λ¨λ¦¬κ°€ μμ„ λ•\")\n",
        "print(\"   - ν• λ²λ§ ν•™μµν•  λ•\")\n",
        "print(\"   - μ—°κµ¬ λ©μ μΌ λ•\")\n",
        "\n",
        "# λ°μ΄ν„°μ…‹ ν† ν°ν™”\n",
        "print(\"π“ ν† ν°ν™” μ‹¤ν–‰ μ¤‘...\")\n",
        "tokenized_dataset = raft_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,                   # λ°°μΉλ΅ μ²λ¦¬ (λΉ λ¦„)\n",
        "    remove_columns=raft_dataset.column_names  # μ›λ³Έ μ»¬λΌ μ κ±°\n",
        ")\n",
        "\n",
        "print(\"β… ν† ν°ν™” μ™„λ£!\")\n",
        "print(f\"   - ν† ν°ν™”λ λ°μ΄ν„° κ°μ: {len(tokenized_dataset):,}κ°\")\n",
        "print(f\"   - ν† ν° κΈΈμ΄: {tokenized_dataset[0]['input_ids'].shape[1]}κ°\")\n",
        "\n",
        "# μƒν” ν™•μΈ\n",
        "print(f\"\\nπ” ν† ν°ν™” κ²°κ³Ό μƒν”:\")\n",
        "sample_tokens = tokenized_dataset[0]['input_ids'][:20]  # μ²μ 20κ° ν† ν°\n",
        "sample_text = tokenizer.decode(sample_tokens, skip_special_tokens=True)\n",
        "print(f\"   - ν† ν° ID: {sample_tokens.tolist()}\")\n",
        "print(f\"   - λ””μ½”λ”©λ ν…μ¤νΈ: {sample_text[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. λ‹¤μ λ‹¨κ³„ μ•λ‚΄ (λ§¤μ° κ°„λ‹¨ν•κ²!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# λ‹¤μ λ‹¨κ³„ μ•λ‚΄ λ° μ”μ•½\n",
        "print(\"π― λ‹¤μ λ‹¨κ³„ μ•λ‚΄\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"π“ μ΄μ  λ‹¤μ λ…ΈνΈλ¶λ“¤λ΅ λ„μ–΄κ°€μ„Έμ”:\")\n",
        "print(\"1οΈβƒ£ 00.06-evaluation.ipynb: λ¨λΈ ν‰κ°€ν•κΈ°\")\n",
        "print(\"2οΈβƒ£ main-practice/03_fine_tuning_with_lora.ipynb: μ‹¤μ  νμΈνλ‹ μ‹¤ν–‰\")\n",
        "\n",
        "print(\"\\nπ’΅ μ§€κΈκΉμ§€ λ°°μ΄ LoRA ν•µμ‹¬ κ°λ…:\")\n",
        "print(\"β… LoRAλ€: μ‘μ€ μ–΄λ‘ν„°λ§ μ¶”κ°€ν•΄μ„ ν•™μµν•λ” λ°©λ²•\")\n",
        "print(\"β… λ©”λ¨λ¦¬ μ μ•½: 93% μ μ•½ (28GB β†’ 2GB)\")\n",
        "print(\"β… μ‹κ°„ λ‹¨μ¶•: 3-5λ°° λΉ λ¦„\")\n",
        "print(\"β… μ •ν™•λ„: 95% μμ¤€ μ μ§€\")\n",
        "print(\"β… λΉ„μ© μ μ•½: 10λ°° μ €λ ΄\")\n",
        "\n",
        "print(\"\\nπ”§ LoRA ν•µμ‹¬ νλΌλ―Έν„°:\")\n",
        "print(\"β… r (rank): 16 (μ λ‹Ήν• ν¬κΈ°)\")\n",
        "print(\"β… lora_alpha: 32 (rμ 2λ°°)\")\n",
        "print(\"β… target_modules: μ–΄ν…μ… λ μ΄μ–΄λ“¤\")\n",
        "print(\"β… lora_dropout: 0.05 (κ³Όμ ν•© λ°©μ§€)\")\n",
        "\n",
        "print(\"\\nπ“ RAFT + LoRA μ΅°ν•©μ μ¥μ :\")\n",
        "print(\"β… RAG μ„±λ¥ ν–¥μƒ: λ¬Έμ„ ν•„ν„°λ§ + μΈμ© μ •ν™•λ„\")\n",
        "print(\"β… ν¨μ¨μ  ν•™μµ: LoRAλ΅ λΉ λ¥΄κ³  μ €λ ΄ν•κ²\")\n",
        "print(\"β… λ„λ©”μΈ νΉν™”: νΉμ • λ°μ΄ν„°μ— νΉν™”λ μ„±λ¥\")\n",
        "print(\"β… μ‹¤μ©μ : μ‹¤μ  μ„λΉ„μ¤μ— λ°”λ΅ μ μ© κ°€λ¥\")\n",
        "\n",
        "print(\"\\nπ€ μ¤€λΉ„ μ™„λ£!\")\n",
        "print(\"μ΄μ  μ‹¤μ  νμΈνλ‹μ„ μ‹¤ν–‰ν•΄λ³΄μ„Έμ”!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. λ°μ΄ν„° μ½λ μ΄ν„° μ„¤μ •ν•κΈ°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# λ°μ΄ν„° μ½λ μ΄ν„°λ¥Ό μ„¤μ •ν•©λ‹λ‹¤ (λ°°μΉ λ°μ΄ν„°λ¥Ό μ •λ¦¬ν•λ” λ„κµ¬)\n",
        "print(\"π“¦ λ°μ΄ν„° μ½λ μ΄ν„° μ„¤μ • μ¤‘...\")\n",
        "\n",
        "# λ°μ΄ν„° μ½λ μ΄ν„° μƒμ„±\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,        # ν† ν¬λ‚μ΄μ €\n",
        "    mlm=False,                  # λ§μ¤ν¬ μ–Έμ–΄ λ¨λΈλ§ μ‚¬μ© μ•ν•¨ (GPT μ¤νƒ€μΌ)\n",
        "    pad_to_multiple_of=8        # 8μ λ°°μλ΅ ν¨λ”© (GPU ν¨μ¨μ„±)\n",
        ")\n",
        "\n",
        "print(\"β… λ°μ΄ν„° μ½λ μ΄ν„° μ„¤μ • μ™„λ£!\")\n",
        "print(\"   - μ–Έμ–΄ λ¨λΈλ§: GPT μ¤νƒ€μΌ (λ‹¤μ ν† ν° μμΈ΅)\")\n",
        "print(\"   - ν¨λ”©: 8μ λ°°μλ΅ μ •λ ¬\")\n",
        "print(\"   - λ§μ¤ν‚Ή: μ‚¬μ© μ•ν•¨\")\n",
        "\n",
        "# λ°μ΄ν„° μ½λ μ΄ν„° ν…μ¤νΈ\n",
        "print(f\"\\nπ§ λ°μ΄ν„° μ½λ μ΄ν„° ν…μ¤νΈ:\")\n",
        "test_batch = [tokenized_dataset[i] for i in range(2)]  # 2κ° μƒν”λ΅ ν…μ¤νΈ\n",
        "collated = data_collator(test_batch)\n",
        "print(f\"   - λ°°μΉ ν¬κΈ°: {collated['input_ids'].shape[0]}\")\n",
        "print(f\"   - μ‹ν€€μ¤ κΈΈμ΄: {collated['input_ids'].shape[1]}\")\n",
        "print(f\"   - λΌλ²¨ ν¬κΈ°: {collated['labels'].shape}\")\n",
        "print(f\"   - ν¨λ”© ν† ν° ID: {tokenizer.pad_token_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. νΈλ μ΄λ„ μƒμ„±ν•κΈ°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# νΈλ μ΄λ„λ¥Ό μƒμ„±ν•©λ‹λ‹¤ (μ‹¤μ  ν•™μµμ„ μ‹¤ν–‰ν•λ” λ„κµ¬)\n",
        "print(\"πƒ νΈλ μ΄λ„ μƒμ„± μ¤‘...\")\n",
        "\n",
        "# νΈλ μ΄λ„ μƒμ„±\n",
        "trainer = Trainer(\n",
        "    model=model,                    # ν•™μµν•  λ¨λΈ\n",
        "    args=training_args,             # ν•™μµ μ„¤μ •\n",
        "    train_dataset=tokenized_dataset, # ν•™μµ λ°μ΄ν„°\n",
        "    data_collator=data_collator,    # λ°μ΄ν„° μ½λ μ΄ν„°\n",
        "    tokenizer=tokenizer             # ν† ν¬λ‚μ΄μ €\n",
        ")\n",
        "\n",
        "print(\"β… νΈλ μ΄λ„ μƒμ„± μ™„λ£!\")\n",
        "print(f\"   - λ¨λΈ: LoRA νμΈνλ‹ λ¨λΈ\")\n",
        "print(f\"   - λ°μ΄ν„°: {len(tokenized_dataset):,}κ° μƒν”\")\n",
        "print(f\"   - μ—ν¬ν¬: {training_args.num_train_epochs}ν\")\n",
        "print(f\"   - μ΄ μ¤ν…: {len(tokenized_dataset) * training_args.num_train_epochs // training_args.gradient_accumulation_steps:,}κ°\")\n",
        "\n",
        "# ν•™μµ μ „ λ¨λΈ ν…μ¤νΈ\n",
        "print(f\"\\nπ§ ν•™μµ μ „ λ¨λΈ ν…μ¤νΈ:\")\n",
        "test_prompt = \"μ§λ¬Έ: μΈκ³µμ§€λ¥μ΄λ€ λ¬΄μ—‡μΈκ°€μ”?\\nλ‹µλ³€:\"\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=30,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"   - μ…λ ¥: {test_prompt}\")\n",
        "print(f\"   - μ¶λ ¥: {generated_text}\")\n",
        "print(f\"   - μƒμ„±λ λ¶€λ¶„: {generated_text[len(test_prompt):]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. μ‹¤μ  νμΈνλ‹ μ‹¤ν–‰ν•κΈ°! π€\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# λ“λ””μ–΄ μ‹¤μ  νμΈνλ‹μ„ μ‹¤ν–‰ν•©λ‹λ‹¤!\n",
        "print(\"π€ νμΈνλ‹ μ‹μ‘!\")\n",
        "print(\"=\" * 50)\n",
        "print(\"π’΅ μ΄ κ³Όμ •μ€ μ‹κ°„μ΄ κ±Έλ¦΄ μ μμµλ‹λ‹¤.\")\n",
        "print(\"π’΅ GPUλ¥Ό μ‚¬μ©ν•λ©΄ λ” λΉ λ¥΄κ² ν•™μµλ©λ‹λ‹¤.\")\n",
        "print(\"π’΅ ν•™μµ κ³Όμ •μ„ μ‹¤μ‹κ°„μΌλ΅ λ¨λ‹ν„°λ§ν•  μ μμµλ‹λ‹¤.\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# νμΈνλ‹ μ‹¤ν–‰\n",
        "try:\n",
        "    # μ‹¤μ  ν•™μµ μ‹μ‘!\n",
        "    trainer.train()\n",
        "    \n",
        "    print(\"\\nπ‰ νμΈνλ‹ μ™„λ£!\")\n",
        "    print(\"β… λ¨λΈμ΄ μ„±κ³µμ μΌλ΅ ν•™μµλμ—μµλ‹λ‹¤!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\nβ νμΈνλ‹ μ¤‘ μ¤λ¥ λ°μƒ: {e}\")\n",
        "    print(\"π’΅ GPU λ©”λ¨λ¦¬κ°€ λ¶€μ΅±ν•  μ μμµλ‹λ‹¤. λ°°μΉ ν¬κΈ°λ¥Ό μ¤„μ—¬λ³΄μ„Έμ”.\")\n",
        "    print(\"π’΅ λλ” Google Colabμ GPUλ¥Ό μ‚¬μ©ν•΄λ³΄μ„Έμ”.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. ν•™μµλ λ¨λΈ ν…μ¤νΈν•κΈ°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ν•™μµλ λ¨λΈμ΄ μ–Όλ§λ‚ κ°μ„ λμ—λ”μ§€ ν…μ¤νΈν•΄λ΄…μ‹λ‹¤!\n",
        "print(\"π§ ν•™μµλ λ¨λΈ ν…μ¤νΈ μ¤‘...\")\n",
        "\n",
        "# ν…μ¤νΈ μ§λ¬Έλ“¤\n",
        "test_questions = [\n",
        "    \"μ§λ¬Έ: μΈκ³µμ§€λ¥μ΄λ€ λ¬΄μ—‡μΈκ°€μ”?\\nλ‹µλ³€:\",\n",
        "    \"μ§λ¬Έ: λ¨Έμ‹ λ¬λ‹κ³Ό λ”¥λ¬λ‹μ μ°¨μ΄μ μ€ λ¬΄μ—‡μΈκ°€μ”?\\nλ‹µλ³€:\",\n",
        "    \"μ§λ¬Έ: μμ—°μ–΄ μ²λ¦¬λ” λ¬΄μ—‡μΈκ°€μ”?\\nλ‹µλ³€:\"\n",
        "]\n",
        "\n",
        "print(\"π“ ν•™μµ μ „ν›„ λΉ„κµ:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\nπ” ν…μ¤νΈ {i}: {question.split('μ§λ¬Έ: ')[1].split('\\\\n')[0]}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # μ…λ ¥ ν† ν°ν™”\n",
        "    inputs = tokenizer(question, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # ν…μ¤νΈ μƒμ„±\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,        # μµλ€ 50κ° ν† ν° μƒμ„±\n",
        "            temperature=0.7,          # μ°½μμ„± μ΅°μ \n",
        "            do_sample=True,           # μƒν”λ§ μ‚¬μ©\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # μƒμ„±λ ν…μ¤νΈ λ””μ½”λ”©\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = generated_text[len(question):].strip()\n",
        "    \n",
        "    print(f\"λ‹µλ³€: {answer}\")\n",
        "    print(f\"μ „μ²΄: {generated_text}\")\n",
        "\n",
        "print(\"\\nβ… λ¨λΈ ν…μ¤νΈ μ™„λ£!\")\n",
        "print(\"π’΅ ν•™μµλ λ¨λΈμ΄ λ” λ‚μ€ λ‹µλ³€μ„ μƒμ„±ν•λ”μ§€ ν™•μΈν•΄λ³΄μ„Έμ”!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. ν•™μµλ λ¨λΈ μ €μ¥ν•κΈ°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ν•™μµλ λ¨λΈμ„ μ €μ¥ν•©λ‹λ‹¤ (λ‚μ¤‘μ— μ‚¬μ©ν•  μ μκ²!)\n",
        "print(\"π’Ύ ν•™μµλ λ¨λΈ μ €μ¥ μ¤‘...\")\n",
        "\n",
        "# 1. LoRA μ–΄λ‘ν„°λ§ μ €μ¥ (κ°€λ²Όμ›€)\n",
        "model.save_pretrained(\"models/fine_tuned_lora\")\n",
        "\n",
        "# 2. ν† ν¬λ‚μ΄μ € μ €μ¥\n",
        "tokenizer.save_pretrained(\"models/fine_tuned_lora\")\n",
        "\n",
        "# 3. ν•™μµ μ„¤μ • μ €μ¥\n",
        "training_config = {\n",
        "    \"model_name\": model_name,\n",
        "    \"lora_config\": model_config['lora_config'],\n",
        "    \"training_args\": {\n",
        "        \"num_train_epochs\": training_args.num_train_epochs,\n",
        "        \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
        "        \"learning_rate\": training_args.learning_rate,\n",
        "        \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n",
        "        \"warmup_steps\": training_args.warmup_steps,\n",
        "        \"weight_decay\": training_args.weight_decay\n",
        "    },\n",
        "    \"dataset_info\": {\n",
        "        \"total_samples\": len(tokenized_dataset),\n",
        "        \"max_length\": 512\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(\"models/training_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(training_config, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"β… λ¨λΈ μ €μ¥ μ™„λ£!\")\n",
        "print(\"   - models/fine_tuned_lora/: LoRA μ–΄λ‘ν„°\")\n",
        "print(\"   - models/training_config.json: ν•™μµ μ„¤μ •\")\n",
        "print(\"   - λ‹¤μ λ‹¨κ³„μ—μ„ μ΄ λ¨λΈμ„ μ‚¬μ©ν•  μ μμµλ‹λ‹¤!\")\n",
        "\n",
        "# μ €μ¥λ νμΌ ν™•μΈ\n",
        "import os\n",
        "print(f\"\\nπ“ μ €μ¥λ νμΌλ“¤:\")\n",
        "for root, dirs, files in os.walk(\"models/fine_tuned_lora\"):\n",
        "    for file in files:\n",
        "        file_path = os.path.join(root, file)\n",
        "        file_size = os.path.getsize(file_path) / (1024*1024)  # MB\n",
        "        print(f\"   - {file_path}: {file_size:.1f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. λ‹¤μ λ‹¨κ³„ μ•λ‚΄\n",
        "\n",
        "### π― λ‹¤μ λ…ΈνΈλ¶μ—μ„ ν•  μΌ\n",
        "**00.06-evaluation.ipynb**μ—μ„:\n",
        "1. **ν•™μµλ λ¨λΈ** λ΅λ“ν•κΈ°\n",
        "2. **μ„±λ¥ ν‰κ°€**ν•κΈ°\n",
        "3. **μ›λ³Έ λ¨λΈκ³Ό λΉ„κµ**ν•κΈ°\n",
        "4. **κ²°κ³Ό λ¶„μ„**ν•κΈ°\n",
        "\n",
        "### π’΅ μ§€κΈκΉμ§€ λ°°μ΄ κ²ƒ\n",
        "- β… RAFT λ°μ΄ν„° λ΅λ“ λ° ν† ν°ν™”\n",
        "- β… LoRA νμΈνλ‹ μ„¤μ •\n",
        "- β… μ‹¤μ  νμΈνλ‹ μ‹¤ν–‰\n",
        "- β… ν•™μµ κ³Όμ • λ¨λ‹ν„°λ§\n",
        "- β… ν•™μµλ λ¨λΈ μ €μ¥\n",
        "\n",
        "### π”§ νμΈνλ‹μ ν•µμ‹¬\n",
        "- **λ°μ΄ν„°**: RAFT ν•μ‹μ μ§λ¬Έ-λ‹µλ³€ λ°μ΄ν„°\n",
        "- **λ¨λΈ**: EXAONE + LoRA μ–΄λ‘ν„°\n",
        "- **ν•™μµ**: 3 μ—ν¬ν¬, λ°°μΉ ν¬κΈ° 1, κ·Έλλ””μ–ΈνΈ λ„μ  4\n",
        "- **κ²°κ³Ό**: RAG μ„±λ¥ ν–¥μƒλ λ¨λΈ\n",
        "\n",
        "### π€ μ¤€λΉ„ μ™„λ£!\n",
        "μ΄μ  λ‹¤μ λ…ΈνΈλ¶μΌλ΅ λ„μ–΄κ°€μ„ ν•™μµλ λ¨λΈμ μ„±λ¥μ„ ν‰κ°€ν•΄λ³΄κ² μµλ‹λ‹¤!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
