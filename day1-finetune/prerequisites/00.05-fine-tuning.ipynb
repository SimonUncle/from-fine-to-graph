{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎓 Day 1-00.05: LoRA 파인튜닝 개념 이해하기 (초보자용)\n",
    "\n",
    "## 🎯 이번 노트북에서 할 일\n",
    "- **LoRA의 핵심 개념** 이해하기\n",
    "- **파인튜닝 과정** 시각화로 보기\n",
    "- **실제 코드** 맛보기 (실행하지 않음)\n",
    "- **메모리 절약** 원리 이해하기\n",
    "\n",
    "## 💡 LoRA란 무엇인가?\n",
    "\n",
    "### 🔧 LoRA (Low-Rank Adaptation)\n",
    "**전체 모델을 다시 학습하지 않고, 작은 어댑터만 추가해서 학습하는 방법**\n",
    "\n",
    "### 🎯 왜 LoRA를 사용할까?\n",
    "\n",
    "#### ❌ 일반 파인튜닝의 문제점\n",
    "- **메모리 부족**: 7B 모델 = 28GB+ 메모리 필요\n",
    "- **시간 오래 걸림**: 전체 모델을 다시 학습\n",
    "- **비용 높음**: GPU 리소스 많이 사용\n",
    "- **과적합 위험**: 작은 데이터로 큰 모델 학습\n",
    "\n",
    "#### ✅ LoRA의 장점\n",
    "- **메모리 절약**: 1%만 학습 (28GB → 2GB)\n",
    "- **빠른 학습**: 작은 어댑터만 업데이트\n",
    "- **안정적**: 원본 모델 성능 유지\n",
    "- **효율적**: 여러 태스크에 재사용 가능\n",
    "\n",
    "### 🧠 LoRA 작동 원리\n",
    "\n",
    "#### 1️⃣ 기존 방식 (Full Fine-tuning)\n",
    "```\n",
    "원본 모델 (7B 파라미터)\n",
    "    ↓ 전체 학습\n",
    "새로운 모델 (7B 파라미터)\n",
    "```\n",
    "\n",
    "#### 2️⃣ LoRA 방식\n",
    "```\n",
    "원본 모델 (7B 파라미터) + LoRA 어댑터 (0.1B 파라미터)\n",
    "    ↓ 어댑터만 학습\n",
    "원본 모델 + 학습된 어댑터\n",
    "```\n",
    "\n",
    "### 🔍 LoRA 어댑터 구조\n",
    "```\n",
    "입력 → [원본 레이어] → [LoRA A] → [LoRA B] → 출력\n",
    "           (고정)        (학습)     (학습)\n",
    "```\n",
    "\n",
    "- **원본 레이어**: 학습하지 않음 (고정)\n",
    "- **LoRA A, B**: 작은 행렬들만 학습\n",
    "- **결과**: 원본 + 어댑터의 조합\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LoRA 파라미터 이해하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA의 핵심 파라미터들을 이해해봅시다\n",
    "print(\"🔧 LoRA 파라미터 설명\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. r (rank) - LoRA의 핵심 파라미터\n",
    "print(\"1️⃣ r (rank): LoRA의 '크기'\")\n",
    "print(\"   - 작을수록: 빠르고 메모리 적게 사용, 정확도 낮음\")\n",
    "print(\"   - 클수록: 느리고 메모리 많이 사용, 정확도 높음\")\n",
    "print(\"   - 일반적 범위: 4, 8, 16, 32, 64\")\n",
    "print(\"   - 예시: r=16 → 16차원 어댑터\")\n",
    "\n",
    "print(\"\\n2️⃣ lora_alpha: 학습 강도 조절\")\n",
    "print(\"   - 높을수록: 더 강하게 학습\")\n",
    "print(\"   - 낮을수록: 더 부드럽게 학습\")\n",
    "print(\"   - 일반적 값: r의 2배 (r=16 → alpha=32)\")\n",
    "\n",
    "print(\"\\n3️⃣ target_modules: 어떤 레이어를 학습할지\")\n",
    "print(\"   - 'q_proj': Query 프로젝션 (어텐션의 질문 부분)\")\n",
    "print(\"   - 'v_proj': Value 프로젝션 (어텐션의 값 부분)\")\n",
    "print(\"   - 'k_proj': Key 프로젝션 (어텐션의 키 부분)\")\n",
    "print(\"   - 'o_proj': Output 프로젝션 (어텐션 출력)\")\n",
    "\n",
    "print(\"\\n4️⃣ lora_dropout: 과적합 방지\")\n",
    "print(\"   - 0.05~0.1: 일반적 범위\")\n",
    "print(\"   - 높을수록: 과적합 방지, 학습 어려움\")\n",
    "print(\"   - 낮을수록: 학습 쉬움, 과적합 위험\")\n",
    "\n",
    "# 실제 LoRA 설정 예시\n",
    "print(\"\\n📋 실제 LoRA 설정 예시:\")\n",
    "print(\"   r=16                    # 적당한 크기\")\n",
    "print(\"   lora_alpha=32           # r의 2배\")\n",
    "print(\"   target_modules=['q_proj', 'v_proj']  # 어텐션 레이어들\")\n",
    "print(\"   lora_dropout=0.05       # 과적합 방지\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 메모리 절약 효과 시각화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA의 메모리 절약 효과를 시각화해봅시다\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"📊 LoRA 메모리 절약 효과 비교\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 모델 크기별 메모리 사용량 (GB)\n",
    "models = ['7B 모델', '13B 모델', '30B 모델', '70B 모델']\n",
    "full_tuning = [28, 52, 120, 280]  # Full fine-tuning 메모리\n",
    "lora_tuning = [2, 4, 8, 16]       # LoRA 메모리\n",
    "\n",
    "# 그래프 생성\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# 1. 메모리 사용량 비교\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, full_tuning, width, label='Full Fine-tuning', color='red', alpha=0.7)\n",
    "bars2 = ax1.bar(x + width/2, lora_tuning, width, label='LoRA', color='green', alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('모델 크기')\n",
    "ax1.set_ylabel('메모리 사용량 (GB)')\n",
    "ax1.set_title('메모리 사용량 비교')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 값 표시\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "             f'{int(height)}GB', ha='center', va='bottom')\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "             f'{int(height)}GB', ha='center', va='bottom')\n",
    "\n",
    "# 2. 절약률 계산\n",
    "savings = [(f-l)/f*100 for f, l in zip(full_tuning, lora_tuning)]\n",
    "bars3 = ax2.bar(models, savings, color='blue', alpha=0.7)\n",
    "\n",
    "ax2.set_xlabel('모델 크기')\n",
    "ax2.set_ylabel('메모리 절약률 (%)')\n",
    "ax2.set_title('LoRA 메모리 절약률')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 값 표시\n",
    "for bar in bars3:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "             f'{height:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 절약 효과 요약\n",
    "print(\"\\n💡 LoRA 메모리 절약 효과:\")\n",
    "for i, model in enumerate(models):\n",
    "    print(f\"   {model}: {full_tuning[i]}GB → {lora_tuning[i]}GB ({savings[i]:.1f}% 절약)\")\n",
    "\n",
    "print(f\"\\n🎯 핵심 포인트:\")\n",
    "print(f\"   - 7B 모델: 28GB → 2GB (93% 절약!)\")\n",
    "print(f\"   - 70B 모델: 280GB → 16GB (94% 절약!)\")\n",
    "print(f\"   - 일반적인 GPU로도 대형 모델 파인튜닝 가능\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 실제 파인튜닝 코드 맛보기 (실행하지 않음)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 파인튜닝 코드를 맛보기로 살펴봅시다 (실행하지 않음!)\n",
    "print(\"📝 실제 파인튜닝 코드 구조\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"1️⃣ 데이터 준비 단계:\")\n",
    "print(\"\"\"\n",
    "# RAFT 데이터 로드\n",
    "raft_dataset = load_from_disk(\"data/raft_dataset\")\n",
    "\n",
    "# 토큰화 함수\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# 데이터셋 토큰화\n",
    "tokenized_dataset = raft_dataset.map(tokenize_function, batched=True)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n2️⃣ 학습 설정 단계:\")\n",
    "print(\"\"\"\n",
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,           # 3번 반복 학습\n",
    "    per_device_train_batch_size=4, # 배치 크기\n",
    "    gradient_accumulation_steps=4, # 그래디언트 누적\n",
    "    warmup_steps=100,             # 워밍업\n",
    "    learning_rate=2e-4,           # 학습률\n",
    "    fp16=True,                    # 16비트 학습\n",
    "    logging_steps=10,             # 로깅 주기\n",
    "    save_steps=500,               # 저장 주기\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n3️⃣ 학습 실행 단계:\")\n",
    "print(\"\"\"\n",
    "# 데이터 정리기\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # 언어 모델링\n",
    ")\n",
    "\n",
    "# 트레이너 생성\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 실제 학습 실행 (이 부분이 시간이 오래 걸림!)\n",
    "print(\"🚀 파인튜닝 시작...\")\n",
    "trainer.train()\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n4️⃣ 모델 저장 단계:\")\n",
    "print(\"\"\"\n",
    "# 학습된 모델 저장\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "print(\"✅ 파인튜닝 완료!\")\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n💡 실제 실행 시 주의사항:\")\n",
    "print(\"   - GPU 메모리: 최소 8GB 이상 필요\")\n",
    "print(\"   - 학습 시간: 데이터 크기에 따라 1-10시간\")\n",
    "print(\"   - 모니터링: 학습 과정을 실시간으로 확인\")\n",
    "print(\"   - 저장: 정기적으로 체크포인트 저장\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LoRA vs Full Fine-tuning 비교\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 💡 **참고**: 이 노트북은 개념 학습용입니다. 아래 셀은 토큰화를 실제로 실행해보고 싶을 때만 돌리세요.\n",
    "> 저장된 RAFT 데이터가 없으면 작은 샘플을 자동으로 만들어 설명만 진행합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA와 Full Fine-tuning을 비교해봅시다\n",
    "print(\"⚖️ LoRA vs Full Fine-tuning 비교\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# RAFT 데이터와 토큰화 함수 준비 (필요 시 생성)\n",
    "try:\n",
    "    from datasets import Dataset, load_from_disk\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"datasets 라이브러리가 필요합니다. `pip install datasets`로 설치해주세요.\") from exc\n",
    "\n",
    "if 'raft_dataset' not in globals():\n",
    "    print(\"📥 RAFT 데이터 불러오기...\")\n",
    "    try:\n",
    "        raft_dataset = load_from_disk(\"data/raft_dataset\")\n",
    "        print(\"✅ RAFT 데이터 로드 완료!\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"⚠️ 저장된 RAFT 데이터가 없어 미니 샘플로 대체합니다.\")\n",
    "        raft_dataset = Dataset.from_dict({\n",
    "            \"text\": [\n",
    "                \"질문: 인공지능이란 무엇인가요?\",\n",
    "                \"질문: 머신러닝과 딥러닝의 차이는?\"\n",
    "            ]\n",
    "        })\n",
    "        print(\"✅ 2개 문장으로 구성된 샘플 데이터셋 생성\")\n",
    "\n",
    "if 'tokenizer' not in globals():\n",
    "    raise NameError(\"tokenizer가 아직 정의되지 않았어요. 00.04 모델 설정 노트북을 먼저 실행해 토크나이저를 불러와 주세요.\")\n",
    "\n",
    "if 'tokenize_function' not in globals():\n",
    "    print(\"🛠️ 토큰화 함수를 새로 정의합니다.\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        )\n",
    "else:\n",
    "    print(\"ℹ️ 기존 tokenize_function을 재사용합니다.\")\n",
    "\n",
    "# 비교표 생성\n",
    "data = {\n",
    "    '항목': [\n",
    "        '메모리 사용량',\n",
    "        '학습 시간',\n",
    "        '모델 크기',\n",
    "        '학습 파라미터',\n",
    "        '정확도',\n",
    "        '안정성',\n",
    "        '비용',\n",
    "        '재사용성'\n",
    "    ],\n",
    "    'Full Fine-tuning': [\n",
    "        '28GB+ (7B 모델)',\n",
    "        '매우 오래 (10+ 시간)',\n",
    "        '7B 파라미터',\n",
    "        '7B 파라미터',\n",
    "        '매우 높음',\n",
    "        '낮음 (과적합 위험)',\n",
    "        '매우 높음',\n",
    "        '어려움'\n",
    "    ],\n",
    "    'LoRA': [\n",
    "        '2GB (7B 모델)',\n",
    "        '빠름 (1-3 시간)',\n",
    "        '7B + 0.1B 어댑터',\n",
    "        '0.1B 파라미터',\n",
    "        '높음 (95% 수준)',\n",
    "        '높음 (원본 유지)',\n",
    "        '낮음',\n",
    "        '쉬움 (어댑터만 교체)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(\"🎯 핵심 차이점:\")\n",
    "print(\"1️⃣ 메모리: LoRA가 93% 절약\")\n",
    "print(\"2️⃣ 시간: LoRA가 3-5배 빠름\")\n",
    "print(\"3️⃣ 정확도: LoRA가 95% 수준 유지\")\n",
    "print(\"4️⃣ 비용: LoRA가 10배 저렴\")\n",
    "\n",
    "print(\"💡 언제 무엇을 사용할까?\")\n",
    "print(\"✅ LoRA 사용 시기:\")\n",
    "print(\"   - 메모리가 부족할 때\")\n",
    "print(\"   - 빠른 실험이 필요할 때\")\n",
    "print(\"   - 여러 태스크를 시도할 때\")\n",
    "print(\"   - 비용을 절약하고 싶을 때\")\n",
    "\n",
    "print(\"✅ Full Fine-tuning 사용 시기:\")\n",
    "print(\"   - 최고 정확도가 필요할 때\")\n",
    "print(\"   - 충분한 메모리가 있을 때\")\n",
    "print(\"   - 한 번만 학습할 때\")\n",
    "print(\"   - 연구 목적일 때\")\n",
    "\n",
    "# 데이터셋 토큰화 (필요 시 실행)\n",
    "print(\"📝 토큰화 실행 중...\")\n",
    "tokenized_dataset = raft_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=raft_dataset.column_names if hasattr(raft_dataset, 'column_names') else []\n",
    ")\n",
    "\n",
    "print(\"✅ 토큰화 완료!\")\n",
    "print(f\"   - 토큰화된 데이터 개수: {len(tokenized_dataset):,}개\")\n",
    "\n",
    "if 'input_ids' in tokenized_dataset[0]:\n",
    "    sequence_length = len(tokenized_dataset[0]['input_ids'])\n",
    "else:\n",
    "    sequence_length = '알 수 없음'\n",
    "print(f\"   - 토큰 길이: {sequence_length}개\")\n",
    "\n",
    "# 샘플 확인\n",
    "print(\"🔍 토큰화 결과 샘플:\")\n",
    "sample_tokens = tokenized_dataset[0]['input_ids'][:20]\n",
    "if hasattr(sample_tokens, 'tolist'):\n",
    "    sample_token_list = sample_tokens.tolist()\n",
    "else:\n",
    "    sample_token_list = list(sample_tokens)\n",
    "\n",
    "sample_text = tokenizer.decode(sample_tokens, skip_special_tokens=True) if hasattr(tokenizer, 'decode') else '디코딩 불가'\n",
    "print(f\"   - 토큰 ID: {sample_token_list}\")\n",
    "print(f\"   - 디코딩된 텍스트: {sample_text[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 다음 단계 안내 (매우 간단하게!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 단계 안내 및 요약\n",
    "print(\"🎯 다음 단계 안내\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"📚 이제 다음 노트북들로 넘어가세요:\")\n",
    "print(\"1️⃣ 00.06-evaluation.ipynb: 모델 평가하기\")\n",
    "print(\"2️⃣ main-practice/03_fine_tuning_with_lora.ipynb: 실제 파인튜닝 실행\")\n",
    "\n",
    "print(\"\\n💡 지금까지 배운 LoRA 핵심 개념:\")\n",
    "print(\"✅ LoRA란: 작은 어댑터만 추가해서 학습하는 방법\")\n",
    "print(\"✅ 메모리 절약: 93% 절약 (28GB → 2GB)\")\n",
    "print(\"✅ 시간 단축: 3-5배 빠름\")\n",
    "print(\"✅ 정확도: 95% 수준 유지\")\n",
    "print(\"✅ 비용 절약: 10배 저렴\")\n",
    "\n",
    "print(\"\\n🔧 LoRA 핵심 파라미터:\")\n",
    "print(\"✅ r (rank): 16 (적당한 크기)\")\n",
    "print(\"✅ lora_alpha: 32 (r의 2배)\")\n",
    "print(\"✅ target_modules: 어텐션 레이어들\")\n",
    "print(\"✅ lora_dropout: 0.05 (과적합 방지)\")\n",
    "\n",
    "print(\"\\n🎓 RAFT + LoRA 조합의 장점:\")\n",
    "print(\"✅ RAG 성능 향상: 문서 필터링 + 인용 정확도\")\n",
    "print(\"✅ 효율적 학습: LoRA로 빠르고 저렴하게\")\n",
    "print(\"✅ 도메인 특화: 특정 데이터에 특화된 성능\")\n",
    "print(\"✅ 실용적: 실제 서비스에 바로 적용 가능\")\n",
    "\n",
    "print(\"\\n🚀 준비 완료!\")\n",
    "print(\"이제 실제 파인튜닝을 실행해보세요!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 데이터 콜레이터 설정하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 콜레이터를 설정합니다 (배치 데이터를 정리하는 도구)\n",
    "print(\"📦 데이터 콜레이터 설정 중...\")\n",
    "\n",
    "# 데이터 콜레이터 생성\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,        # 토크나이저\n",
    "    mlm=False,                  # 마스크 언어 모델링 사용 안함 (GPT 스타일)\n",
    "    pad_to_multiple_of=8        # 8의 배수로 패딩 (GPU 효율성)\n",
    ")\n",
    "\n",
    "print(\"✅ 데이터 콜레이터 설정 완료!\")\n",
    "print(\"   - 언어 모델링: GPT 스타일 (다음 토큰 예측)\")\n",
    "print(\"   - 패딩: 8의 배수로 정렬\")\n",
    "print(\"   - 마스킹: 사용 안함\")\n",
    "\n",
    "# 데이터 콜레이터 테스트\n",
    "print(f\"\\n🧪 데이터 콜레이터 테스트:\")\n",
    "test_batch = [tokenized_dataset[i] for i in range(2)]  # 2개 샘플로 테스트\n",
    "collated = data_collator(test_batch)\n",
    "print(f\"   - 배치 크기: {collated['input_ids'].shape[0]}\")\n",
    "print(f\"   - 시퀀스 길이: {collated['input_ids'].shape[1]}\")\n",
    "print(f\"   - 라벨 크기: {collated['labels'].shape}\")\n",
    "print(f\"   - 패딩 토큰 ID: {tokenizer.pad_token_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 트레이너 생성하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트레이너를 생성합니다 (실제 학습을 실행하는 도구)\n",
    "print(\"🏃 트레이너 생성 중...\")\n",
    "\n",
    "# 트레이너 생성\n",
    "trainer = Trainer(\n",
    "    model=model,                    # 학습할 모델\n",
    "    args=training_args,             # 학습 설정\n",
    "    train_dataset=tokenized_dataset, # 학습 데이터\n",
    "    data_collator=data_collator,    # 데이터 콜레이터\n",
    "    tokenizer=tokenizer             # 토크나이저\n",
    ")\n",
    "\n",
    "print(\"✅ 트레이너 생성 완료!\")\n",
    "print(f\"   - 모델: LoRA 파인튜닝 모델\")\n",
    "print(f\"   - 데이터: {len(tokenized_dataset):,}개 샘플\")\n",
    "print(f\"   - 에포크: {training_args.num_train_epochs}회\")\n",
    "print(f\"   - 총 스텝: {len(tokenized_dataset) * training_args.num_train_epochs // training_args.gradient_accumulation_steps:,}개\")\n",
    "\n",
    "# 학습 전 모델 테스트\n",
    "print(f\"\\n🧪 학습 전 모델 테스트:\")\n",
    "test_prompt = \"질문: 인공지능이란 무엇인가요?\\n답변:\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=30,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"   - 입력: {test_prompt}\")\n",
    "print(f\"   - 출력: {generated_text}\")\n",
    "print(f\"   - 생성된 부분: {generated_text[len(test_prompt):]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 실제 파인튜닝 실행하기! 🚀\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드디어 실제 파인튜닝을 실행합니다!\n",
    "print(\"🚀 파인튜닝 시작!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"💡 이 과정은 시간이 걸릴 수 있습니다.\")\n",
    "print(\"💡 GPU를 사용하면 더 빠르게 학습됩니다.\")\n",
    "print(\"💡 학습 과정을 실시간으로 모니터링할 수 있습니다.\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 파인튜닝 실행\n",
    "try:\n",
    "    # 실제 학습 시작!\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"\\n🎉 파인튜닝 완료!\")\n",
    "    print(\"✅ 모델이 성공적으로 학습되었습니다!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ 파인튜닝 중 오류 발생: {e}\")\n",
    "    print(\"💡 GPU 메모리가 부족할 수 있습니다. 배치 크기를 줄여보세요.\")\n",
    "    print(\"💡 또는 Google Colab의 GPU를 사용해보세요.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 학습된 모델 테스트하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델이 얼마나 개선되었는지 테스트해봅시다!\n",
    "print(\"🧪 학습된 모델 테스트 중...\")\n",
    "\n",
    "# 테스트 질문들\n",
    "test_questions = [\n",
    "    \"질문: 인공지능이란 무엇인가요?\\n답변:\",\n",
    "    \"질문: 머신러닝과 딥러닝의 차이점은 무엇인가요?\\n답변:\",\n",
    "    \"질문: 자연어 처리는 무엇인가요?\\n답변:\"\n",
    "]\n",
    "\n",
    "print(\"📝 학습 전후 비교:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n🔍 테스트 {i}: {question.split('질문: ')[1].split('\\\\n')[0]}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 입력 토큰화\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # 텍스트 생성\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,        # 최대 50개 토큰 생성\n",
    "            temperature=0.7,          # 창의성 조절\n",
    "            do_sample=True,           # 샘플링 사용\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # 생성된 텍스트 디코딩\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = generated_text[len(question):].strip()\n",
    "    \n",
    "    print(f\"답변: {answer}\")\n",
    "    print(f\"전체: {generated_text}\")\n",
    "\n",
    "print(\"\\n✅ 모델 테스트 완료!\")\n",
    "print(\"💡 학습된 모델이 더 나은 답변을 생성하는지 확인해보세요!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 학습된 모델 저장하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델을 저장합니다 (나중에 사용할 수 있게!)\n",
    "print(\"💾 학습된 모델 저장 중...\")\n",
    "\n",
    "# 1. LoRA 어댑터만 저장 (가벼움)\n",
    "model.save_pretrained(\"models/fine_tuned_lora\")\n",
    "\n",
    "# 2. 토크나이저 저장\n",
    "tokenizer.save_pretrained(\"models/fine_tuned_lora\")\n",
    "\n",
    "# 3. 학습 설정 저장\n",
    "training_config = {\n",
    "    \"model_name\": model_name,\n",
    "    \"lora_config\": model_config['lora_config'],\n",
    "    \"training_args\": {\n",
    "        \"num_train_epochs\": training_args.num_train_epochs,\n",
    "        \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n",
    "        \"warmup_steps\": training_args.warmup_steps,\n",
    "        \"weight_decay\": training_args.weight_decay\n",
    "    },\n",
    "    \"dataset_info\": {\n",
    "        \"total_samples\": len(tokenized_dataset),\n",
    "        \"max_length\": 512\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"models/training_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(training_config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ 모델 저장 완료!\")\n",
    "print(\"   - models/fine_tuned_lora/: LoRA 어댑터\")\n",
    "print(\"   - models/training_config.json: 학습 설정\")\n",
    "print(\"   - 다음 단계에서 이 모델을 사용할 수 있습니다!\")\n",
    "\n",
    "# 저장된 파일 확인\n",
    "import os\n",
    "print(f\"\\n📁 저장된 파일들:\")\n",
    "for root, dirs, files in os.walk(\"models/fine_tuned_lora\"):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_size = os.path.getsize(file_path) / (1024*1024)  # MB\n",
    "        print(f\"   - {file_path}: {file_size:.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 다음 단계 안내\n",
    "\n",
    "### 🎯 다음 노트북에서 할 일\n",
    "**00.06-evaluation.ipynb**에서:\n",
    "1. **학습된 모델** 로드하기\n",
    "2. **성능 평가**하기\n",
    "3. **원본 모델과 비교**하기\n",
    "4. **결과 분석**하기\n",
    "\n",
    "### 💡 지금까지 배운 것\n",
    "- ✅ RAFT 데이터 로드 및 토큰화\n",
    "- ✅ LoRA 파인튜닝 설정\n",
    "- ✅ 실제 파인튜닝 실행\n",
    "- ✅ 학습 과정 모니터링\n",
    "- ✅ 학습된 모델 저장\n",
    "\n",
    "### 🔧 파인튜닝의 핵심\n",
    "- **데이터**: RAFT 형식의 질문-답변 데이터\n",
    "- **모델**: EXAONE + LoRA 어댑터\n",
    "- **학습**: 3 에포크, 배치 크기 1, 그래디언트 누적 4\n",
    "- **결과**: RAG 성능 향상된 모델\n",
    "\n",
    "### 🚀 준비 완료!\n",
    "이제 다음 노트북으로 넘어가서 학습된 모델의 성능을 평가해보겠습니다!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
