{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ¤– Day 1-00.04: ëª¨ë¸ ì„¤ì •í•˜ê¸° (ì´ˆë³´ììš©)\n",
        "\n",
        "## ğŸ¯ ì´ë²ˆ ë…¸íŠ¸ë¶ì—ì„œ í•  ì¼\n",
        "- **EXAONE ëª¨ë¸** ë¡œë“œí•˜ê¸° (Hugging Face í™œìš©)\n",
        "- **í† í¬ë‚˜ì´ì €** ì„¤ì •í•˜ê¸°\n",
        "- **LoRA ì„¤ì •**í•˜ê¸° (ë§¤ìš° ê°„ë‹¨í•˜ê²Œ!)\n",
        "- **ëª¨ë¸ ì¤€ë¹„** ì™„ë£Œí•˜ê¸°\n",
        "\n",
        "## ğŸ’¡ ì‚¬ìš©í•  ëª¨ë¸\n",
        "- **ëª¨ë¸ëª…**: `LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct`\n",
        "- **ì–¸ì–´**: í•œêµ­ì–´ì— íŠ¹í™”\n",
        "- **í¬ê¸°**: 7.8B íŒŒë¼ë¯¸í„° (ì ë‹¹í•œ í¬ê¸°)\n",
        "- **ìš©ë„**: ì§ˆë¬¸ë‹µë³€, ëŒ€í™”, í…ìŠ¤íŠ¸ ìƒì„±\n",
        "\n",
        "## ğŸ”§ LoRAë€?\n",
        "**Low-Rank Adaptation**ì˜ ì¤„ì„ë§ë¡œ, ì ì€ ë©”ëª¨ë¦¬ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ íŒŒì¸íŠœë‹í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n",
        "\n",
        "### ğŸ¯ LoRAì˜ ì¥ì \n",
        "- **ë©”ëª¨ë¦¬ ì ˆì•½**: ì „ì²´ ëª¨ë¸ì„ í•™ìŠµí•˜ì§€ ì•Šê³  ì¼ë¶€ë§Œ í•™ìŠµ\n",
        "- **ë¹ ë¥¸ í•™ìŠµ**: ì ì€ íŒŒë¼ë¯¸í„°ë§Œ ì—…ë°ì´íŠ¸\n",
        "- **ì•ˆì •ì **: ê¸°ì¡´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œ íŠ¹í™”\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ëª¨ë¸ ì„¤ì •ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤\n",
        "import torch\n",
        "import json\n",
        "from transformers import (\n",
        "    AutoTokenizer,           # í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜\n",
        "    AutoModelForCausalLM,    # ì–¸ì–´ ëª¨ë¸\n",
        "    BitsAndBytesConfig       # 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,              # LoRA ì„¤ì •\n",
        "    get_peft_model,          # LoRA ëª¨ë¸ ìƒì„±\n",
        "    TaskType,                # íƒœìŠ¤í¬ íƒ€ì…\n",
        "    prepare_model_for_kbit_training  # 4ë¹„íŠ¸ í•™ìŠµ ì¤€ë¹„\n",
        ")\n",
        "from datasets import load_from_disk\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ… ëª¨ë¸ ì„¤ì • ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "print(f\"ğŸ”¥ PyTorch ë²„ì „: {torch.__version__}\")\n",
        "print(f\"ğŸš€ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ëª¨ë¸ ì„¤ì • (ë§¤ìš° ê°„ë‹¨í•˜ê²Œ!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXAONE ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤ (í•˜ë“œì½”ë”© ì—†ì´!)\n",
        "print(\"ğŸ¤– EXAONE ëª¨ë¸ ì„¤ì • ì¤‘...\")\n",
        "\n",
        "# ëª¨ë¸ ì´ë¦„ (Hugging Faceì—ì„œ ê°€ì ¸ì˜¤ê¸°) - ê³µê°œ ëª¨ë¸ ì‚¬ìš©\n",
        "model_name = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n",
        "\n",
        "print(f\"ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {model_name}\")\n",
        "print(\"ğŸ’¡ ì´ ê³¼ì •ì€ ì²˜ìŒì—ë§Œ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤. ë‹¤ìŒì—ëŠ” ìºì‹œì—ì„œ ë¹ ë¥´ê²Œ ë¡œë“œë©ë‹ˆë‹¤.\")\n",
        "\n",
        "# 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´)\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,                    # 4ë¹„íŠ¸ë¡œ ë¡œë“œ\n",
        "    bnb_4bit_compute_dtype=torch.float16, # ê³„ì‚°ì€ 16ë¹„íŠ¸ë¡œ\n",
        "    bnb_4bit_use_double_quant=True,       # ì´ì¤‘ ì–‘ìí™” ì‚¬ìš©\n",
        "    bnb_4bit_quant_type=\"nf4\"             # 4ë¹„íŠ¸ ì–‘ìí™” íƒ€ì…\n",
        ")\n",
        "\n",
        "print(\"âœ… ëª¨ë¸ ì„¤ì • ì™„ë£Œ!\")\n",
        "print(\"   - 4ë¹„íŠ¸ ì–‘ìí™”: ë©”ëª¨ë¦¬ ì ˆì•½\")\n",
        "print(\"   - 16ë¹„íŠ¸ ê³„ì‚°: ì •í™•ë„ ìœ ì§€\")\n",
        "print(\"   - ì´ì¤‘ ì–‘ìí™”: ë” ë‚˜ì€ ì••ì¶•\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. í† í¬ë‚˜ì´ì € ë¡œë“œí•˜ê¸°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXAONE-3.5-2.4B-Instruct ëª¨ë¸ ì‚¬ìš© (ê³µê°œ ëª¨ë¸)\n",
        "print(\"âœ… EXAONE-3.5-2.4B-Instruct ëª¨ë¸ ì‚¬ìš©\")\n",
        "print(\"   - ê³µê°œ ëª¨ë¸: ì¸ì¦ ë¶ˆí•„ìš”\")\n",
        "print(\"   - í¬ê¸°: 2.4B íŒŒë¼ë¯¸í„° (7.8Bë³´ë‹¤ ì‘ê³  ë¹ ë¦„)\")\n",
        "print(\"   - í•œêµ­ì–´ íŠ¹í™”: EXAONE ì‹œë¦¬ì¦ˆ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤ (í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë„êµ¬)\n",
        "print(\"ğŸ”¤ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...\")\n",
        "\n",
        "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,  # EXAONE ëª¨ë¸ì˜ íŠ¹ë³„í•œ ì½”ë“œ ì‚¬ìš©\n",
        "    padding_side=\"right\"     # íŒ¨ë”©ì„ ì˜¤ë¥¸ìª½ì— ì¶”ê°€\n",
        ")\n",
        "\n",
        "# íŒ¨ë”© í† í° ì„¤ì • (ë°°ì¹˜ ì²˜ë¦¬ì— í•„ìš”)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # EOS í† í°ì„ íŒ¨ë”© í† í°ìœ¼ë¡œ ì‚¬ìš©\n",
        "\n",
        "print(\"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!\")\n",
        "print(f\"   - ì–´íœ˜ í¬ê¸°: {tokenizer.vocab_size:,}ê°œ\")\n",
        "print(f\"   - íŒ¨ë”© í† í°: {tokenizer.pad_token}\")\n",
        "print(f\"   - EOS í† í°: {tokenizer.eos_token}\")\n",
        "\n",
        "# í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸\n",
        "test_text = \"ì•ˆë…•í•˜ì„¸ìš”! ì¸ê³µì§€ëŠ¥ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n",
        "tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
        "print(f\"\\nğŸ§ª í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸:\")\n",
        "print(f\"   ì…ë ¥: {test_text}\")\n",
        "print(f\"   í† í° ìˆ˜: {tokens['input_ids'].shape[1]}ê°œ\")\n",
        "print(f\"   í† í° ID: {tokens['input_ids'][0][:10].tolist()}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ëª¨ë¸ ë¡œë“œí•˜ê¸°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXAONE ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤ (4ë¹„íŠ¸ ì–‘ìí™” ì ìš©)\n",
        "print(\"ğŸ¤– EXAONE ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
        "print(\"ğŸ’¡ ì´ ê³¼ì •ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”!\")\n",
        "\n",
        "# ëª¨ë¸ ë¡œë“œ (4ë¹„íŠ¸ ì–‘ìí™” ì ìš©)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,  # 4ë¹„íŠ¸ ì–‘ìí™” ì ìš©\n",
        "    device_map=\"auto\",                       # ìë™ìœ¼ë¡œ GPU/CPU ë°°ì¹˜\n",
        "    trust_remote_code=True,                  # EXAONE ëª¨ë¸ì˜ íŠ¹ë³„í•œ ì½”ë“œ ì‚¬ìš©\n",
        "    torch_dtype=torch.float16                # 16ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì  ì‚¬ìš©\n",
        ")\n",
        "\n",
        "print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
        "print(f\"   - ëª¨ë¸ í¬ê¸°: 7.8B íŒŒë¼ë¯¸í„°\")\n",
        "print(f\"   - ì–‘ìí™”: 4ë¹„íŠ¸ (ë©”ëª¨ë¦¬ ì ˆì•½)\")\n",
        "print(f\"   - ë””ë°”ì´ìŠ¤: {next(model.parameters()).device}\")\n",
        "\n",
        "# ëª¨ë¸ ì •ë³´ ì¶œë ¥\n",
        "print(f\"\\nğŸ“Š ëª¨ë¸ ì •ë³´:\")\n",
        "print(f\"   - ëª¨ë¸ íƒ€ì…: {type(model).__name__}\")\n",
        "print(f\"   - í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}ê°œ\")\n",
        "print(f\"   - ì „ì²´ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}ê°œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. LoRA ì„¤ì •í•˜ê¸° (ë§¤ìš° ê°„ë‹¨í•˜ê²Œ!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LoRA ì„¤ì •ì„ ë§Œë“­ë‹ˆë‹¤ (ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ëª…!)\n",
        "print(\"ğŸ”§ LoRA ì„¤ì • ì¤‘...\")\n",
        "\n",
        "# LoRA ì„¤ì • (ê° íŒŒë¼ë¯¸í„°ì˜ ì˜ë¯¸ë¥¼ ëª…í™•íˆ ì„¤ëª…)\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                           # LoRA rank (ë‚®ì„ìˆ˜ë¡ ë¹ ë¦„, ë†’ì„ìˆ˜ë¡ ì •í™•)\n",
        "    lora_alpha=32,                  # LoRA scaling (í•™ìŠµë¥  ì¡°ì ˆ)\n",
        "    target_modules=[                # ì–´ë–¤ ë ˆì´ì–´ë¥¼ í•™ìŠµí• ì§€\n",
        "        \"q_proj\",                   # Query í”„ë¡œì ì…˜ (ì–´í…ì…˜ì˜ ì§ˆë¬¸ ë¶€ë¶„)\n",
        "        \"v_proj\",                   # Value í”„ë¡œì ì…˜ (ì–´í…ì…˜ì˜ ê°’ ë¶€ë¶„)\n",
        "        \"k_proj\",                   # Key í”„ë¡œì ì…˜ (ì–´í…ì…˜ì˜ í‚¤ ë¶€ë¶„)\n",
        "        \"o_proj\"                    # Output í”„ë¡œì ì…˜ (ì–´í…ì…˜ ì¶œë ¥)\n",
        "    ],\n",
        "    lora_dropout=0.05,              # ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ë“œë¡­ì•„ì›ƒ\n",
        "    bias=\"none\",                    # ë°”ì´ì–´ìŠ¤ í•™ìŠµ ì•ˆí•¨\n",
        "    task_type=TaskType.CAUSAL_LM,   # ì–¸ì–´ ëª¨ë¸ë§ íƒœìŠ¤í¬\n",
        ")\n",
        "\n",
        "print(\"âœ… LoRA ì„¤ì • ì™„ë£Œ!\")\n",
        "print(\"   - r=16: LoRA rank (ì ë‹¹í•œ í¬ê¸°)\")\n",
        "print(\"   - alpha=32: ìŠ¤ì¼€ì¼ë§ íŒ©í„°\")\n",
        "print(\"   - target_modules: ì–´í…ì…˜ ë ˆì´ì–´ë“¤\")\n",
        "print(\"   - dropout=0.05: ê³¼ì í•© ë°©ì§€\")\n",
        "print(\"   - task_type: ì–¸ì–´ ëª¨ë¸ë§\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. LoRA ëª¨ë¸ ìƒì„±í•˜ê¸°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4ë¹„íŠ¸ í•™ìŠµì„ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„\n",
        "print(\"ğŸ”„ 4ë¹„íŠ¸ í•™ìŠµ ì¤€ë¹„ ì¤‘...\")\n",
        "\n",
        "# 4ë¹„íŠ¸ ì–‘ìí™”ëœ ëª¨ë¸ì„ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì¤€ë¹„\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA ì–´ëŒ‘í„°ë¥¼ ëª¨ë¸ì— ì¶”ê°€\n",
        "print(\"ğŸ”§ LoRA ì–´ëŒ‘í„° ì¶”ê°€ ì¤‘...\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "print(\"âœ… LoRA ëª¨ë¸ ìƒì„± ì™„ë£Œ!\")\n",
        "\n",
        "# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í™•ì¸\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "all_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f\"\\nğŸ“Š íŒŒë¼ë¯¸í„° ì •ë³´:\")\n",
        "print(f\"   - í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,}ê°œ\")\n",
        "print(f\"   - ì „ì²´ íŒŒë¼ë¯¸í„°: {all_params:,}ê°œ\")\n",
        "print(f\"   - í•™ìŠµ ë¹„ìœ¨: {trainable_params/all_params*100:.2f}%\")\n",
        "print(f\"   - ë©”ëª¨ë¦¬ ì ˆì•½: {(1-trainable_params/all_params)*100:.1f}%\")\n",
        "\n",
        "# ëª¨ë¸ êµ¬ì¡° í™•ì¸\n",
        "print(f\"\\nğŸ” LoRA ì–´ëŒ‘í„° ì •ë³´:\")\n",
        "for name, module in model.named_modules():\n",
        "    if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):\n",
        "        print(f\"   - {name}: LoRA ì–´ëŒ‘í„° ì ìš©ë¨\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. ëª¨ë¸ í…ŒìŠ¤íŠ¸í•˜ê¸°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì„¤ì •ëœ ëª¨ë¸ì´ ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸í•´ë´…ì‹œë‹¤\n",
        "print(\"ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸\n",
        "test_prompt = \"ì§ˆë¬¸: ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\\në‹µë³€:\"\n",
        "\n",
        "# í† í¬ë‚˜ì´ì§•\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# ìƒì„± ì„¤ì •\n",
        "generation_config = {\n",
        "    \"max_new_tokens\": 50,        # ìµœëŒ€ 50ê°œ í† í° ìƒì„±\n",
        "    \"temperature\": 0.7,          # ì°½ì˜ì„± ì¡°ì ˆ (0.7 = ì ë‹¹íˆ ì°½ì˜ì )\n",
        "    \"do_sample\": True,           # ìƒ˜í”Œë§ ì‚¬ìš©\n",
        "    \"pad_token_id\": tokenizer.eos_token_id  # íŒ¨ë”© í† í° ID\n",
        "}\n",
        "\n",
        "# í…ìŠ¤íŠ¸ ìƒì„±\n",
        "with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì•ˆí•¨ (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        **generation_config\n",
        "    )\n",
        "\n",
        "# ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"âœ… ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
        "print(f\"\\nğŸ“ í…ŒìŠ¤íŠ¸ ê²°ê³¼:\")\n",
        "print(f\"ì…ë ¥: {test_prompt}\")\n",
        "print(f\"ì¶œë ¥: {generated_text}\")\n",
        "print(f\"\\nğŸ¯ ìƒì„±ëœ ë¶€ë¶„ë§Œ:\")\n",
        "print(generated_text[len(test_prompt):])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. ì„¤ì • ì €ì¥í•˜ê¸°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ëª¨ë¸ ì„¤ì •ì„ ì €ì¥í•©ë‹ˆë‹¤ (ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì‚¬ìš©)\n",
        "print(\"ğŸ’¾ ëª¨ë¸ ì„¤ì • ì €ì¥ ì¤‘...\")\n",
        "\n",
        "# ëª¨ë¸ ì„¤ì • ì •ë³´\n",
        "model_config = {\n",
        "    \"model_name\": model_name,\n",
        "    \"lora_config\": {\n",
        "        \"r\": lora_config.r,\n",
        "        \"lora_alpha\": lora_config.lora_alpha,\n",
        "        \"target_modules\": lora_config.target_modules,\n",
        "        \"lora_dropout\": lora_config.lora_dropout,\n",
        "        \"bias\": lora_config.bias,\n",
        "        \"task_type\": str(lora_config.task_type)\n",
        "    },\n",
        "    \"quantization_config\": {\n",
        "        \"load_in_4bit\": quantization_config.load_in_4bit,\n",
        "        \"bnb_4bit_compute_dtype\": str(quantization_config.bnb_4bit_compute_dtype),\n",
        "        \"bnb_4bit_use_double_quant\": quantization_config.bnb_4bit_use_double_quant,\n",
        "        \"bnb_4bit_quant_type\": quantization_config.bnb_4bit_quant_type\n",
        "    },\n",
        "    \"model_info\": {\n",
        "        \"trainable_params\": int(trainable_params),\n",
        "        \"all_params\": int(all_params),\n",
        "        \"trainable_ratio\": float(trainable_params/all_params)\n",
        "    }\n",
        "}\n",
        "\n",
        "# ì„¤ì • ì €ì¥\n",
        "with open(\"models/model_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(model_config, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# LoRA ì„¤ì •ë„ ë³„ë„ë¡œ ì €ì¥\n",
        "model.save_pretrained(\"models/lora_config\")\n",
        "\n",
        "print(\"âœ… ì„¤ì • ì €ì¥ ì™„ë£Œ!\")\n",
        "print(\"   - models/model_config.json: ì „ì²´ ì„¤ì •\")\n",
        "print(\"   - models/lora_config/: LoRA ì„¤ì •\")\n",
        "print(\"   - ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì´ ì„¤ì •ë“¤ì„ ì‚¬ìš©í•©ë‹ˆë‹¤\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´\n",
        "\n",
        "### ğŸ¯ ë‹¤ìŒ ë…¸íŠ¸ë¶ì—ì„œ í•  ì¼\n",
        "**00.05-fine-tuning.ipynb**ì—ì„œ:\n",
        "1. **RAFT ë°ì´í„°** ë¡œë“œí•˜ê¸°\n",
        "2. **í•™ìŠµ ì„¤ì •** êµ¬ì„±í•˜ê¸°\n",
        "3. **ì‹¤ì œ íŒŒì¸íŠœë‹** ì‹¤í–‰í•˜ê¸°\n",
        "4. **í•™ìŠµ ê³¼ì •** ëª¨ë‹ˆí„°ë§í•˜ê¸°\n",
        "\n",
        "### ğŸ’¡ ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ê²ƒ\n",
        "- âœ… EXAONE ëª¨ë¸ ë¡œë“œ ë° ì„¤ì •\n",
        "- âœ… 4ë¹„íŠ¸ ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½\n",
        "- âœ… LoRA ì„¤ì •ìœ¼ë¡œ íš¨ìœ¨ì  í•™ìŠµ\n",
        "- âœ… í† í¬ë‚˜ì´ì € ì„¤ì • ë° í…ŒìŠ¤íŠ¸\n",
        "- âœ… ëª¨ë¸ ì„¤ì • ì €ì¥\n",
        "\n",
        "### ğŸ”§ LoRAì˜ í•µì‹¬\n",
        "- **r=16**: LoRA rank (ì ë‹¹í•œ í¬ê¸°)\n",
        "- **target_modules**: ì–´í…ì…˜ ë ˆì´ì–´ë“¤ë§Œ í•™ìŠµ\n",
        "- **ë©”ëª¨ë¦¬ ì ˆì•½**: ì „ì²´ ëª¨ë¸ì˜ 1%ë§Œ í•™ìŠµ\n",
        "- **ë¹ ë¥¸ í•™ìŠµ**: ì ì€ íŒŒë¼ë¯¸í„°ë¡œ íš¨ìœ¨ì \n",
        "\n",
        "### ğŸš€ ì¤€ë¹„ ì™„ë£Œ!\n",
        "ì´ì œ ë‹¤ìŒ ë…¸íŠ¸ë¶ìœ¼ë¡œ ë„˜ì–´ê°€ì„œ ì‹¤ì œ íŒŒì¸íŠœë‹ì„ ì‹¤í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
