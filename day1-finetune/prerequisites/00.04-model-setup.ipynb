{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# π¤– Day 1-00.04: λ¨λΈ μ„¤μ •ν•κΈ° (μ΄λ³΄μμ©)\n",
    "\n",
    "## π― μ΄λ² λ…ΈνΈλ¶μ—μ„ ν•  μΌ\n",
    "- **EXAONE λ¨λΈ** λ΅λ“ν•κΈ° (Hugging Face ν™μ©)\n",
    "- **ν† ν¬λ‚μ΄μ €** μ„¤μ •ν•κΈ°\n",
    "- **LoRA μ„¤μ •**ν•κΈ° (λ§¤μ° κ°„λ‹¨ν•κ²!)\n",
    "- **λ¨λΈ μ¤€λΉ„** μ™„λ£ν•κΈ°\n",
    "\n",
    "## π’΅ μ‚¬μ©ν•  λ¨λΈ\n",
    "- **λ¨λΈλ…**: `LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct`\n",
    "- **μ–Έμ–΄**: ν•κµ­μ–΄μ— νΉν™”\n",
    "- **ν¬κΈ°**: 7.8B νλΌλ―Έν„° (μ λ‹Ήν• ν¬κΈ°)\n",
    "- **μ©λ„**: μ§λ¬Έλ‹µλ³€, λ€ν™”, ν…μ¤νΈ μƒμ„±\n",
    "\n",
    "## π”§ LoRAλ€?\n",
    "**Low-Rank Adaptation**μ μ¤„μ„λ§λ΅, μ μ€ λ©”λ¨λ¦¬λ΅ ν¨μ¨μ μΌλ΅ νμΈνλ‹ν•λ” λ°©λ²•μ…λ‹λ‹¤.\n",
    "\n",
    "### π― LoRAμ μ¥μ \n",
    "- **λ©”λ¨λ¦¬ μ μ•½**: μ „μ²΄ λ¨λΈμ„ ν•™μµν•μ§€ μ•κ³  μΌλ¶€λ§ ν•™μµ\n",
    "- **λΉ λ¥Έ ν•™μµ**: μ μ€ νλΌλ―Έν„°λ§ μ—…λ°μ΄νΈ\n",
    "- **μ•μ •μ **: κΈ°μ΅΄ λ¨λΈμ μ„±λ¥μ„ μ μ§€ν•λ©΄μ„ νΉν™”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ν•„μ”ν• λΌμ΄λΈλ¬λ¦¬ λ¶λ¬μ¤κΈ°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# λ¨λΈ μ„¤μ •μ— ν•„μ”ν• λΌμ΄λΈλ¬λ¦¬λ“¤μ„ λ¶λ¬μµλ‹λ‹¤\n",
    "import torch\n",
    "import json\n",
    "from transformers import (\n",
    "    AutoTokenizer,           # ν…μ¤νΈλ¥Ό ν† ν°μΌλ΅ λ³€ν™\n",
    "    AutoModelForCausalLM,    # μ–Έμ–΄ λ¨λΈ\n",
    "    BitsAndBytesConfig       # 4λΉ„νΈ μ–‘μν™” μ„¤μ •\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,              # LoRA μ„¤μ •\n",
    "    get_peft_model,          # LoRA λ¨λΈ μƒμ„±\n",
    "    TaskType,                # νƒμ¤ν¬ νƒ€μ…\n",
    "    prepare_model_for_kbit_training  # 4λΉ„νΈ ν•™μµ μ¤€λΉ„\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"β… λ¨λΈ μ„¤μ • λΌμ΄λΈλ¬λ¦¬κ°€ μ¤€λΉ„λμ—μµλ‹λ‹¤!\")\n",
    "print(f\"π”¥ PyTorch λ²„μ „: {torch.__version__}\")\n",
    "print(f\"π€ CUDA μ‚¬μ© κ°€λ¥: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. λ¨λΈ μ„¤μ • (λ§¤μ° κ°„λ‹¨ν•κ²!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAONE λ¨λΈμ„ μ„¤μ •ν•©λ‹λ‹¤ (ν•λ“μ½”λ”© μ—†μ΄!)\n",
    "print(\"π¤– EXAONE λ¨λΈ μ„¤μ • μ¤‘...\")\n",
    "\n",
    "# λ¨λΈ μ΄λ¦„ (Hugging Faceμ—μ„ κ°€μ Έμ¤κΈ°) - κ³µκ° λ¨λΈ μ‚¬μ©\n",
    "model_name = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n",
    "\n",
    "print(f\"π“¥ λ¨λΈ λ‹¤μ΄λ΅λ“ μ¤‘: {model_name}\")\n",
    "print(\"π’΅ μ΄ κ³Όμ •μ€ μ²μμ—λ§ μ‹κ°„μ΄ κ±Έλ¦½λ‹λ‹¤. λ‹¤μμ—λ” μΊμ‹μ—μ„ λΉ λ¥΄κ² λ΅λ“λ©λ‹λ‹¤.\")\n",
    "\n",
    "# 4λΉ„νΈ μ–‘μν™” μ„¤μ • (λ©”λ¨λ¦¬ μ μ•½μ„ μ„ν•΄)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # 4λΉ„νΈλ΅ λ΅λ“\n",
    "    bnb_4bit_compute_dtype=torch.float16, # κ³„μ‚°μ€ 16λΉ„νΈλ΅\n",
    "    bnb_4bit_use_double_quant=True,       # μ΄μ¤‘ μ–‘μν™” μ‚¬μ©\n",
    "    bnb_4bit_quant_type=\"nf4\"             # 4λΉ„νΈ μ–‘μν™” νƒ€μ…\n",
    ")\n",
    "\n",
    "print(\"β… λ¨λΈ μ„¤μ • μ™„λ£!\")\n",
    "print(\"   - 4λΉ„νΈ μ–‘μν™”: λ©”λ¨λ¦¬ μ μ•½\")\n",
    "print(\"   - 16λΉ„νΈ κ³„μ‚°: μ •ν™•λ„ μ μ§€\")\n",
    "print(\"   - μ΄μ¤‘ μ–‘μν™”: λ” λ‚μ€ μ••μ¶•\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ν† ν¬λ‚μ΄μ € λ΅λ“ν•κΈ°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAONE-3.5-2.4B-Instruct λ¨λΈ μ‚¬μ© (κ³µκ° λ¨λΈ)\n",
    "print(\"β… EXAONE-3.5-2.4B-Instruct λ¨λΈ μ‚¬μ©\")\n",
    "print(\"   - κ³µκ° λ¨λΈ: μΈμ¦ λ¶ν•„μ”\")\n",
    "print(\"   - ν¬κΈ°: 2.4B νλΌλ―Έν„° (7.8Bλ³΄λ‹¤ μ‘κ³  λΉ λ¦„)\")\n",
    "print(\"   - ν•κµ­μ–΄ νΉν™”: EXAONE μ‹λ¦¬μ¦\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ν† ν¬λ‚μ΄μ €λ¥Ό λ΅λ“ν•©λ‹λ‹¤ (ν…μ¤νΈλ¥Ό ν† ν°μΌλ΅ λ³€ν™ν•λ” λ„κµ¬)\n",
    "print(\"π”¤ ν† ν¬λ‚μ΄μ € λ΅λ“ μ¤‘...\")\n",
    "\n",
    "# ν† ν¬λ‚μ΄μ € λ΅λ“\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,  # EXAONE λ¨λΈμ νΉλ³„ν• μ½”λ“ μ‚¬μ©\n",
    "    padding_side=\"right\"     # ν¨λ”©μ„ μ¤λ¥Έμ½μ— μ¶”κ°€\n",
    ")\n",
    "\n",
    "# ν¨λ”© ν† ν° μ„¤μ • (λ°°μΉ μ²λ¦¬μ— ν•„μ”)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # EOS ν† ν°μ„ ν¨λ”© ν† ν°μΌλ΅ μ‚¬μ©\n",
    "\n",
    "print(\"β… ν† ν¬λ‚μ΄μ € λ΅λ“ μ™„λ£!\")\n",
    "print(f\"   - μ–΄ν ν¬κΈ°: {tokenizer.vocab_size:,}κ°\")\n",
    "print(f\"   - ν¨λ”© ν† ν°: {tokenizer.pad_token}\")\n",
    "print(f\"   - EOS ν† ν°: {tokenizer.eos_token}\")\n",
    "\n",
    "# ν† ν¬λ‚μ΄μ € ν…μ¤νΈ\n",
    "test_text = \"μ•λ…•ν•μ„Έμ”! μΈκ³µμ§€λ¥μ— λ€ν•΄ μ„¤λ…ν•΄μ£Όμ„Έμ”.\"\n",
    "tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "print(f\"\\nπ§ ν† ν¬λ‚μ΄μ € ν…μ¤νΈ:\")\n",
    "print(f\"   μ…λ ¥: {test_text}\")\n",
    "print(f\"   ν† ν° μ: {tokens['input_ids'].shape[1]}κ°\")\n",
    "print(f\"   ν† ν° ID: {tokens['input_ids'][0][:10].tolist()}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. λ¨λΈ λ΅λ“ν•κΈ°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAONE λ¨λΈμ„ λ΅λ“ν•©λ‹λ‹¤ (4λΉ„νΈ μ–‘μν™” μ μ©)\n",
    "print(\"π¤– EXAONE λ¨λΈ λ΅λ“ μ¤‘...\")\n",
    "print(\"π’΅ μ΄ κ³Όμ •μ€ μ‹κ°„μ΄ κ±Έλ¦΄ μ μμµλ‹λ‹¤. μ μ‹λ§ κΈ°λ‹¤λ ¤μ£Όμ„Έμ”!\")\n",
    "\n",
    "# λ¨λΈ λ΅λ“ (4λΉ„νΈ μ–‘μν™” μ μ©)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,  # 4λΉ„νΈ μ–‘μν™” μ μ©\n",
    "    device_map=\"auto\",                       # μλ™μΌλ΅ GPU/CPU λ°°μΉ\n",
    "    trust_remote_code=True,                  # EXAONE λ¨λΈμ νΉλ³„ν• μ½”λ“ μ‚¬μ©\n",
    "    torch_dtype=torch.float16                # 16λΉ„νΈ λ¶€λ™μ†μμ  μ‚¬μ©\n",
    ")\n",
    "\n",
    "print(\"β… λ¨λΈ λ΅λ“ μ™„λ£!\")\n",
    "print(f\"   - λ¨λΈ ν¬κΈ°: 7.8B νλΌλ―Έν„°\")\n",
    "print(f\"   - μ–‘μν™”: 4λΉ„νΈ (λ©”λ¨λ¦¬ μ μ•½)\")\n",
    "print(f\"   - λ””λ°”μ΄μ¤: {next(model.parameters()).device}\")\n",
    "\n",
    "# λ¨λΈ μ •λ³΄ μ¶λ ¥\n",
    "print(f\"\\nπ“ λ¨λΈ μ •λ³΄:\")\n",
    "print(f\"   - λ¨λΈ νƒ€μ…: {type(model).__name__}\")\n",
    "print(f\"   - ν•™μµ κ°€λ¥ν• νλΌλ―Έν„°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}κ°\")\n",
    "print(f\"   - μ „μ²΄ νλΌλ―Έν„°: {sum(p.numel() for p in model.parameters()):,}κ°\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LoRA μ„¤μ •ν•κΈ° (λ§¤μ° κ°„λ‹¨ν•κ²!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA μ„¤μ •μ„ λ§λ“­λ‹λ‹¤ (λ¨λ“  νλΌλ―Έν„°λ¥Ό μ„¤λ…!)\n",
    "print(\"π”§ LoRA μ„¤μ • μ¤‘...\")\n",
    "\n",
    "# LoRA μ„¤μ • (κ° νλΌλ―Έν„°μ μλ―Έλ¥Ό λ…ν™•ν μ„¤λ…)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                           # LoRA rank (λ‚®μ„μλ΅ λΉ λ¦„, λ†’μ„μλ΅ μ •ν™•)\n",
    "    lora_alpha=32,                  # LoRA scaling (ν•™μµλ¥  μ΅°μ )\n",
    "    target_modules=[                # μ–΄λ–¤ λ μ΄μ–΄λ¥Ό ν•™μµν• μ§€\n",
    "        \"q_proj\",                   # Query ν”„λ΅μ μ… (μ–΄ν…μ…μ μ§λ¬Έ λ¶€λ¶„)\n",
    "        \"v_proj\",                   # Value ν”„λ΅μ μ… (μ–΄ν…μ…μ κ°’ λ¶€λ¶„)\n",
    "        \"k_proj\",                   # Key ν”„λ΅μ μ… (μ–΄ν…μ…μ ν‚¤ λ¶€λ¶„)\n",
    "        \"o_proj\"                    # Output ν”„λ΅μ μ… (μ–΄ν…μ… μ¶λ ¥)\n",
    "    ],\n",
    "    lora_dropout=0.05,              # κ³Όμ ν•© λ°©μ§€λ¥Ό μ„ν• λ“λ΅­μ•„μ›ƒ\n",
    "    bias=\"none\",                    # λ°”μ΄μ–΄μ¤ ν•™μµ μ•ν•¨\n",
    "    task_type=TaskType.CAUSAL_LM,   # μ–Έμ–΄ λ¨λΈλ§ νƒμ¤ν¬\n",
    ")\n",
    "\n",
    "print(\"β… LoRA μ„¤μ • μ™„λ£!\")\n",
    "print(\"   - r=16: LoRA rank (μ λ‹Ήν• ν¬κΈ°)\")\n",
    "print(\"   - alpha=32: μ¤μΌ€μΌλ§ ν©ν„°\")\n",
    "print(\"   - target_modules: μ–΄ν…μ… λ μ΄μ–΄λ“¤\")\n",
    "print(\"   - dropout=0.05: κ³Όμ ν•© λ°©μ§€\")\n",
    "print(\"   - task_type: μ–Έμ–΄ λ¨λΈλ§\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LoRA λ¨λΈ μƒμ„±ν•κΈ°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4λΉ„νΈ ν•™μµμ„ μ„ν• λ¨λΈ μ¤€λΉ„\n",
    "print(\"π”„ 4λΉ„νΈ ν•™μµ μ¤€λΉ„ μ¤‘...\")\n",
    "\n",
    "# 4λΉ„νΈ μ–‘μν™”λ λ¨λΈμ„ ν•™μµ κ°€λ¥ν•κ² μ¤€λΉ„\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA μ–΄λ‘ν„°λ¥Ό λ¨λΈμ— μ¶”κ°€\n",
    "print(\"π”§ LoRA μ–΄λ‘ν„° μ¶”κ°€ μ¤‘...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"β… LoRA λ¨λΈ μƒμ„± μ™„λ£!\")\n",
    "\n",
    "# ν•™μµ κ°€λ¥ν• νλΌλ―Έν„° ν™•μΈ\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\nπ“ νλΌλ―Έν„° μ •λ³΄:\")\n",
    "print(f\"   - ν•™μµ κ°€λ¥ν• νλΌλ―Έν„°: {trainable_params:,}κ°\")\n",
    "print(f\"   - μ „μ²΄ νλΌλ―Έν„°: {all_params:,}κ°\")\n",
    "print(f\"   - ν•™μµ λΉ„μ¨: {trainable_params/all_params*100:.2f}%\")\n",
    "print(f\"   - λ©”λ¨λ¦¬ μ μ•½: {(1-trainable_params/all_params)*100:.1f}%\")\n",
    "\n",
    "# λ¨λΈ κµ¬μ΅° ν™•μΈ\n",
    "print(f\"\\nπ” LoRA μ–΄λ‘ν„° μ •λ³΄:\")\n",
    "for name, module in model.named_modules():\n",
    "    if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):\n",
    "        print(f\"   - {name}: LoRA μ–΄λ‘ν„° μ μ©λ¨\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. λ¨λΈ ν…μ¤νΈν•κΈ°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# μ„¤μ •λ λ¨λΈμ΄ μ λ€λ΅ μ‘λ™ν•λ”μ§€ ν…μ¤νΈν•΄λ΄…μ‹λ‹¤\n",
    "print(\"π§ λ¨λΈ ν…μ¤νΈ μ¤‘...\")\n",
    "\n",
    "# ν…μ¤νΈ ν…μ¤νΈ\n",
    "test_prompt = \"μ§λ¬Έ: μΈκ³µμ§€λ¥μ΄λ€ λ¬΄μ—‡μΈκ°€μ”?\\nλ‹µλ³€:\"\n",
    "\n",
    "# ν† ν¬λ‚μ΄μ§•\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# μƒμ„± μ„¤μ •\n",
    "generation_config = {\n",
    "    \"max_new_tokens\": 50,        # μµλ€ 50κ° ν† ν° μƒμ„±\n",
    "    \"temperature\": 0.7,          # μ°½μμ„± μ΅°μ  (0.7 = μ λ‹Ήν μ°½μμ )\n",
    "    \"do_sample\": True,           # μƒν”λ§ μ‚¬μ©\n",
    "    \"pad_token_id\": tokenizer.eos_token_id  # ν¨λ”© ν† ν° ID\n",
    "}\n",
    "\n",
    "# ν…μ¤νΈ μƒμ„±\n",
    "with torch.no_grad():  # κ·Έλλ””μ–ΈνΈ κ³„μ‚° μ•ν•¨ (λ©”λ¨λ¦¬ μ μ•½)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        **generation_config\n",
    "    )\n",
    "\n",
    "# μƒμ„±λ ν…μ¤νΈ λ””μ½”λ”©\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"β… λ¨λΈ ν…μ¤νΈ μ™„λ£!\")\n",
    "print(f\"\\nπ“ ν…μ¤νΈ κ²°κ³Ό:\")\n",
    "print(f\"μ…λ ¥: {test_prompt}\")\n",
    "print(f\"μ¶λ ¥: {generated_text}\")\n",
    "print(f\"\\nπ― μƒμ„±λ λ¶€λ¶„λ§:\")\n",
    "print(generated_text[len(test_prompt):])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. μ„¤μ • μ €μ¥ν•κΈ°\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRAκ°€ λ­”κ°€μ”?\n",
    "\n",
    "- **μ „μ²΄ λ―Έμ„Έμ΅°μ •(Full Fine-tuning)**μ€ λ¨λΈμ λ¨λ“  κ°€μ¤‘μΉλ¥Ό λ‹¤μ‹ ν•™μµν•©λ‹λ‹¤. μλ¥Ό λ“¤μ–΄ 70μ–µ(7B) νλΌλ―Έν„° λ¨λΈμ΄λΌλ©΄ μ GBμ λ©”λ¨λ¦¬κ°€ ν•„μ”ν•κ³ , μ €μ¥ νμΌλ„ λ™μΌν• ν¬κΈ°λ΅ μƒκΉλ‹λ‹¤.\n",
    "- **LoRA(Low-Rank Adaptation)**λ” μ¤‘μ”ν• μΈµμ—λ§ λ§¤μ° μ‘μ€ λ³΄μ΅° ν–‰λ ¬μ„ λ¶™μ—¬ ν•™μµν•©λ‹λ‹¤. μ›λ κ°€μ¤‘μΉλ” κ·Έλ€λ΅ κ³ μ •ν•κ³  μƒλ΅ μ¶”κ°€λ μλ°±λ§ κ° μ •λ„μ νλΌλ―Έν„°λ§ μ—…λ°μ΄νΈν•λ―€λ΅ GPU λ©”λ¨λ¦¬μ™€ μ €μ¥ κ³µκ°„μ΄ ν¬κ² μ¤„μ–΄λ“­λ‹λ‹¤.\n",
    "- μ΄λ ‡κ² ν•™μµν• λ³΄μ΅° ν–‰λ ¬μ„ μ›λ³Έ λ¨λΈμ— λ”ν•λ©΄ μ „μ²΄ λ¨λΈμ΄ λ―Έμ„Έμ΅°μ •λ κ²ƒμ²λΌ λ™μ‘ν•©λ‹λ‹¤. ν•„μ”ν•  λ•λ§ λ¶λ¬μ¤λ‹ ν•™μµ/λ°°ν¬ λ¨λ‘ κ°€λ³μµλ‹λ‹¤.\n",
    "- ν•μ΄νΌνλΌλ―Έν„° μ΄ν•΄ν•κΈ°:\n",
    "  - `r`: μ¶”κ°€ ν–‰λ ¬μ λ­ν¬(ν¬κΈ°). κ°’μ΄ ν΄μλ΅ ν‘ν„λ ¥μ΄ λ†’μ•„μ§€μ§€λ§ ν•™μµλ‰μ΄ λμ–΄λ‚©λ‹λ‹¤.\n",
    "  - `lora_alpha`: ν•™μµλ LoRA κ°€μ¤‘μΉλ¥Ό μ–Όλ§λ‚ ν¬κ² λ°μν• μ§€ μ •ν•λ” μ¤μΌ€μΌ κ°’μ…λ‹λ‹¤.\n",
    "  - `target_modules`: LoRAλ¥Ό μ μ©ν•  λ¨λΈ μΈµ λ©λ΅μ…λ‹λ‹¤. μ—¬κΈ°λ§ μ¶”κ°€ νλΌλ―Έν„°κ°€ μƒκΉλ‹λ‹¤.\n",
    "  - `lora_dropout`: ν•™μµ μ¤‘ μΌλ¶€ LoRA μ—°κ²°μ„ λμ–΄ κ³Όμ ν•©μ„ λ§‰λ” ν™•λ¥ μ…λ‹λ‹¤.\n",
    "- μ΄ λ…ΈνΈλ¶μ—μ„λ” μ΄λ ‡κ² ν•™μµν• LoRA κµ¬μ„±κ³Ό κ°€μ¤‘μΉλ¥Ό λ”°λ΅ μ €μ¥ν–λ‹¤κ°€, λ‹¤μ λ‹¨κ³„μ—μ„ κΈ°λ³Έ λ¨λΈκ³Ό λ‹¤μ‹ ν•©μ³ μ“°λ„λ΅ μ¤€λΉ„ν•©λ‹λ‹¤.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# λ¨λΈ μ„¤μ •μ„ μ €μ¥ν•©λ‹λ‹¤ (λ‹¤μ λ‹¨κ³„μ—μ„ μ‚¬μ©)\n",
    "print(\"π’Ύ λ¨λΈ μ„¤μ • μ €μ¥ μ¤‘...\")\n",
    "\n",
    "# JSON μ§λ ¬ν™”λ¥Ό μ„ν•΄ ν•„μ”ν• κ°’ μ •λ¦¬\n",
    "target_modules = lora_config.target_modules\n",
    "if isinstance(target_modules, (set, tuple)):\n",
    "    target_modules = list(target_modules)\n",
    "\n",
    "quant_type = quantization_config.bnb_4bit_quant_type\n",
    "if quant_type is not None:\n",
    "    quant_type = str(quant_type)\n",
    "\n",
    "# λ¨λΈ μ„¤μ • μ •λ³΄\n",
    "model_config = {\n",
    "    \"model_name\": model_name,\n",
    "    \"lora_config\": {\n",
    "        \"r\": lora_config.r,\n",
    "        \"lora_alpha\": lora_config.lora_alpha,\n",
    "        \"target_modules\": target_modules,\n",
    "        \"lora_dropout\": lora_config.lora_dropout,\n",
    "        \"bias\": lora_config.bias,\n",
    "        \"task_type\": str(lora_config.task_type)\n",
    "    },\n",
    "    \"quantization_config\": {\n",
    "        \"load_in_4bit\": quantization_config.load_in_4bit,\n",
    "        \"bnb_4bit_compute_dtype\": str(quantization_config.bnb_4bit_compute_dtype),\n",
    "        \"bnb_4bit_use_double_quant\": quantization_config.bnb_4bit_use_double_quant,\n",
    "        \"bnb_4bit_quant_type\": quant_type\n",
    "    },\n",
    "    \"model_info\": {\n",
    "        \"trainable_params\": int(trainable_params),\n",
    "        \"all_params\": int(all_params),\n",
    "        \"trainable_ratio\": float(trainable_params / all_params)\n",
    "    }\n",
    "}\n",
    "\n",
    "# μ„¤μ • μ €μ¥\n",
    "with open(\"models/model_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(model_config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# LoRA μ„¤μ •λ„ λ³„λ„λ΅ μ €μ¥\n",
    "model.save_pretrained(\"models/lora_config\")\n",
    "\n",
    "print(\"β… μ„¤μ • μ €μ¥ μ™„λ£!\")\n",
    "print(\"   - models/model_config.json: μ „μ²΄ μ„¤μ •\")\n",
    "print(\"   - models/lora_config/: LoRA μ„¤μ •\")\n",
    "print(\"   - λ‹¤μ λ‹¨κ³„μ—μ„ μ΄ μ„¤μ •λ“¤μ„ μ‚¬μ©ν•©λ‹λ‹¤\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### μ™ μ„¤μ •μ„ λ”°λ΅ μ €μ¥ν•λ‚μ”?\n",
    "\n",
    "- λ™μΌν• ν™κ²½μ—μ„ λ‹¤μ‹ ν•™μµν•κ±°λ‚ ν‰κ°€ν•  λ•, μ„¤μ • νμΌ ν•λ‚λ§ μμΌλ©΄ νλΌλ―Έν„° κ°’μ„ μΌμΌμ΄ κΈ°μ–µν•μ§€ μ•μ•„λ„ μ¬ν„ν•  μ μμµλ‹λ‹¤.\n",
    "- ν€μ›κ³Ό κ³µμ ν•κ±°λ‚ λ‹¤λ¥Έ μ‹¤ν—μ—μ„ μ¬μ‚¬μ©ν•  λ•, LoRA κ°€μ¤‘μΉ(`models/lora_config/`)μ™€ κµ¬μ„±(`models/model_config.json`)μ„ ν•¨κ» μ „λ‹¬ν•λ©΄ λ°”λ΅ λ¶λ¬μ™€ μ‚¬μ©ν•  μ μμµλ‹λ‹¤.\n",
    "- μ¶”ν›„μ— LoRAλ§ λ°”κΏ” λΌμ°λ©΄μ„ μ—¬λ¬ μ‹¤ν—μ„ λΉ„κµν•λ ¤λ©΄, κΈ°λ³Έ λ¨λΈμ€ κ·Έλ€λ΅ λ‘κ³  LoRA νμΌλ§ κµμ²΄ν•λ©΄ λλ―€λ΅ κ΄€λ¦¬κ°€ κ°„νΈν•΄μ§‘λ‹λ‹¤.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. λ‹¤μ λ‹¨κ³„ μ•λ‚΄\n",
    "\n",
    "### π― λ‹¤μ λ…ΈνΈλ¶μ—μ„ ν•  μΌ\n",
    "**00.05-fine-tuning.ipynb**μ—μ„:\n",
    "1. **RAFT λ°μ΄ν„°** λ΅λ“ν•κΈ°\n",
    "2. **ν•™μµ μ„¤μ •** κµ¬μ„±ν•κΈ°\n",
    "3. **μ‹¤μ  νμΈνλ‹** μ‹¤ν–‰ν•κΈ°\n",
    "4. **ν•™μµ κ³Όμ •** λ¨λ‹ν„°λ§ν•κΈ°\n",
    "\n",
    "### π’΅ μ§€κΈκΉμ§€ λ°°μ΄ κ²ƒ\n",
    "- β… EXAONE λ¨λΈ λ΅λ“ λ° μ„¤μ •\n",
    "- β… 4λΉ„νΈ μ–‘μν™”λ΅ λ©”λ¨λ¦¬ μ μ•½\n",
    "- β… LoRA μ„¤μ •μΌλ΅ ν¨μ¨μ  ν•™μµ\n",
    "- β… ν† ν¬λ‚μ΄μ € μ„¤μ • λ° ν…μ¤νΈ\n",
    "- β… λ¨λΈ μ„¤μ • μ €μ¥\n",
    "\n",
    "### π”§ LoRAμ ν•µμ‹¬\n",
    "- **r=16**: LoRA rank (μ λ‹Ήν• ν¬κΈ°)\n",
    "- **target_modules**: μ–΄ν…μ… λ μ΄μ–΄λ“¤λ§ ν•™μµ\n",
    "- **λ©”λ¨λ¦¬ μ μ•½**: μ „μ²΄ λ¨λΈμ 1%λ§ ν•™μµ\n",
    "- **λΉ λ¥Έ ν•™μµ**: μ μ€ νλΌλ―Έν„°λ΅ ν¨μ¨μ \n",
    "\n",
    "### π€ μ¤€λΉ„ μ™„λ£!\n",
    "μ΄μ  λ‹¤μ λ…ΈνΈλ¶μΌλ΅ λ„μ–΄κ°€μ„ μ‹¤μ  νμΈνλ‹μ„ μ‹¤ν–‰ν•΄λ³΄κ² μµλ‹λ‹¤!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}