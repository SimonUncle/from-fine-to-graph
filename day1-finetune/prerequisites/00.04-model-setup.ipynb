{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1-00.04: EXAONE ëª¨ë¸ ì¤€ë¹„í•˜ê¸°\n",
    "\n",
    "EXAONE-3.5-2.4B-Instruct ëª¨ë¸ì„ 4bit + LoRA êµ¬ì„±ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì¤€ë¹„í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ë° bitsandbytes í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **bitsandbytes 4bit ì¤€ë¹„**\n",
    "> - ê¸°ë³¸ ëŸ°íƒ€ì„ì— bitsandbytesê°€ ì„¤ì¹˜ë¼ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "> - ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì€ ê²½ìš° `pip install -q bitsandbytes accelerate` ì‹¤í–‰ í›„ ëŸ°íƒ€ì„ì„ ì¬ì‹œì‘í•˜ì„¸ìš”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ê·¸ëƒ¥ (FP16/FP32) ëª¨ë¸\n",
    "\n",
    "ì¼ë°˜ì ìœ¼ë¡œ Hugging Faceì—ì„œ ë¶ˆëŸ¬ì˜¤ëŠ” ëª¨ë¸ì€ FP16(half precision) ë˜ëŠ” **FP32(full precision)**ë¡œ ì €ì¥ë¼ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ëª¨ë“  ê°€ì¤‘ì¹˜(weight) ê°’ì´ 16bit ë˜ëŠ” 32bit ë¶€ë™ì†Œìˆ˜ì ìœ¼ë¡œ í‘œí˜„ë¼ì„œ ì •í™•ë„ê°€ ê°€ì¥ ë†’ê³  ì›ë˜ ì„±ëŠ¥ì„ ìœ ì§€í•©ë‹ˆë‹¤.\n",
    "\n",
    "í•˜ì§€ë§Œ GPU ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì°¨ì§€í•˜ê³ , ì—°ì‚°ëŸ‰ë„ í½ë‹ˆë‹¤.\n",
    "\n",
    "2. 4ë¹„íŠ¸ í€€íƒ€ì´ì¦ˆ ëª¨ë¸\n",
    "\n",
    "32bit â†’ 4bitë¡œ ì¤„ì—¬ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì•½ 8ë°° ì¤„ì„.\n",
    "\n",
    "ì˜ˆ: 13B íŒŒë¼ë¯¸í„° ëª¨ë¸\n",
    "\n",
    "FP16: ì•½ 26GB í•„ìš”\n",
    "\n",
    "4bit: ì•½ 6.5GB í•„ìš”\n",
    "\n",
    "ëŒ€ì‹  ê°€ì¤‘ì¹˜ í‘œí˜„ ì •ë°€ë„ê°€ ë‚®ì•„ì ¸ì„œ ì •ë³´ ì†ì‹¤ì´ ìƒê¹ë‹ˆë‹¤.\n",
    "\n",
    "ì¶”ë¡  ì†ë„ë„ ë¹¨ë¼ì§€ê³ , ì ì€ GPUì—ì„œë„ ëŒë¦´ ìˆ˜ ìˆì§€ë§Œ,\n",
    "\n",
    "**ì¶œë ¥ í’ˆì§ˆ(ì •í™•ë„)**ì´ ì¡°ê¸ˆ ë–¨ì–´ì§ˆ ìˆ˜ ìˆì–´ìš”. (ë³´í†µ 1~3% ì •ë„ ì„±ëŠ¥ ì €í•˜ ë³´ê³ ë¨)\n",
    "\n",
    "3. ì‹¤ì œ ì¶œë ¥ ì°¨ì´\n",
    "\n",
    "FP16/32 ëª¨ë¸: ì›ë˜ í•™ìŠµëœ ì§€ì‹ì„ ê°€ì¥ ì¶©ì‹¤íˆ ë°˜ì˜.\n",
    "\n",
    "4bit ëª¨ë¸: ì˜ë¯¸ëŠ” ë¹„ìŠ·í•˜ê²Œ ë‚´ì§€ë§Œ,\n",
    "\n",
    "ë¬¸ë§¥ ìœ ì§€ê°€ ì§§ì•„ì§„ë‹¤ê±°ë‚˜,\n",
    "\n",
    "ì„¸ë°€í•œ ìˆ˜ì¹˜/í¬ê·€ ë‹¨ì–´ì—ì„œ ì—ëŸ¬ê°€ ë‚  ìˆ˜ ìˆìŒ.\n",
    "\n",
    "í° ë§¥ë½ì—ì„œëŠ” ì°¨ì´ ì•ˆ ë³´ì¼ ìˆ˜ ìˆì§€ë§Œ, ì–´ë ¤ìš´ ì§ˆë¬¸ì¼ìˆ˜ë¡ ì°¨ì´ê°€ ë‘ë“œëŸ¬ì§‘ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def require_bitsandbytes(compute_dtype=torch.float16) -> BitsAndBytesConfig:\n",
    "    try:\n",
    "        import bitsandbytes as bnb  # noqa: F401\n",
    "        print(\"âœ… bitsandbytesê°€ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    except ImportError as exc:\n",
    "        raise RuntimeError(\n",
    "            \"bitsandbytesê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. `pip install -q bitsandbytes accelerate` ì‹¤í–‰ í›„ ëŸ°íƒ€ì„ì„ ì¬ì‹œì‘í•˜ì„¸ìš”.\"\n",
    "        ) from exc\n",
    "\n",
    "    try:\n",
    "        config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=compute_dtype,\n",
    "        )\n",
    "        print(\"âœ… 4bit ì–‘ìí™” êµ¬ì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        return config\n",
    "    except Exception as exc:\n",
    "        raise RuntimeError(\n",
    "            \"bitsandbytes ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. `pip install -U bitsandbytes` ì‹¤í–‰ í›„ ëŸ°íƒ€ì„ì„ ì¬ì‹œì‘í•œ ë’¤ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.\"\n",
    "            f\"ì˜¤ë¥˜ ë©”ì‹œì§€: {exc}\"\n",
    "        ) from exc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "- Hugging Faceì—ì„œ EXAONE ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "- 4bit quantizationì„ ì ìš©í•´ ë©”ëª¨ë¦¬ë¥¼ ì¤„ì…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n",
    "print(f\"ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: {MODEL_NAME}\")\n",
    "\n",
    "quant_config = require_bitsandbytes(compute_dtype=torch.float16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\",\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=quant_config,\n",
    ")\n",
    "\n",
    "print(\"âœ… ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"   - íŒ¨ë”© í† í°: {tokenizer.pad_token}\")\n",
    "print(f\"   - ë””ë°”ì´ìŠ¤: {next(model.parameters()).device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LoRA ì„¤ì • ë° ì ìš©\n",
    "- 4bit ì–‘ìí™”ë¡œ ê¸°ë³¸ ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì—¬ í•™ìŠµ ì¤€ë¹„ë¥¼ í•©ë‹ˆë‹¤.\n",
    "- LoRAëŠ” **ì¶”ê°€ í•™ìŠµì„ ìœ„í•œ ì–´ëŒ‘í„°**ë¡œ, ì „ì²´ ê°€ì¤‘ì¹˜ë¥¼ ê±´ë“œë¦¬ì§€ ì•Šê³  ì¼ë¶€ ëª¨ë“ˆ(q/k/v/out)ì—ë§Œ ì‘ì€ í–‰ë ¬ì„ ë¶™ì…ë‹ˆë‹¤.\n",
    "- ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” í›ˆë ¨ì„ ì‹¤í–‰í•˜ì§€ ì•Šê³  **ì–´ëŒ‘í„°ë¥¼ ë¶™ì—¬ ì‚¬ìš©í•  ì¤€ë¹„ ìƒíƒœ**ê¹Œì§€ ë§Œë“¤ë©°, ì´í›„ íŒŒì¸íŠœë‹ ë…¸íŠ¸ë¶ì—ì„œ ì‹¤ì œ í•™ìŠµì´ ì§„í–‰ë©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"âœ… LoRA ì ìš© ì™„ë£Œ\")\n",
    "print(f\"   - í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,}\")\n",
    "print(f\"   - ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,}\")\n",
    "print(f\"   - í•™ìŠµ ë¹„ìœ¨: {trainable_params / total_params:.3%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸\n",
    "LoRA ì–´ëŒ‘í„°ë¥¼ ë¶™ì¸ ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. (ì•„ì§ í•™ìŠµì€ í•˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ, ì¶œë ¥ì€ ê¸°ë³¸ ëª¨ë¸ê³¼ ë¹„ìŠ·í•©ë‹ˆë‹¤.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PROMPT = \"ì§ˆë¬¸: ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?ë‹µë³€:\"\n",
    "inputs = tokenizer(TEST_PROMPT, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=60,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"ğŸ§ª ì˜ˆì‹œ ì¶œë ¥\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. ë‹¤ìŒ ë‹¨ê³„ë¡œ\n",
    "- ì¤€ë¹„ëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì €ì¥í•˜ê±°ë‚˜ ë°”ë¡œ ì´ì–´ì§€ëŠ” íŒŒì¸íŠœë‹ ì‹¤ìŠµì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- ë‹¤ìŒ ë…¸íŠ¸ë¶: `main-practice/03_fine_tuning_with_lora.ipynb`\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
