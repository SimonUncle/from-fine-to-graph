{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🤖 Day 1-00.04: 모델 설정하기 (초보자용)\n",
    "\n",
    "## 🎯 이번 노트북에서 할 일\n",
    "- **EXAONE 모델** 로드하기 (Hugging Face 활용)\n",
    "- **토크나이저** 설정하기\n",
    "- **LoRA 설정**하기 (매우 간단하게!)\n",
    "- **모델 준비** 완료하기\n",
    "\n",
    "## 💡 사용할 모델\n",
    "- **모델명**: `LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct`\n",
    "- **언어**: 한국어에 특화\n",
    "- **크기**: 7.8B 파라미터 (적당한 크기)\n",
    "- **용도**: 질문답변, 대화, 텍스트 생성\n",
    "\n",
    "## 🔧 LoRA란?\n",
    "**Low-Rank Adaptation**의 줄임말로, 적은 메모리로 효율적으로 파인튜닝하는 방법입니다.\n",
    "\n",
    "### 🎯 LoRA의 장점\n",
    "- **메모리 절약**: 전체 모델을 학습하지 않고 일부만 학습\n",
    "- **빠른 학습**: 적은 파라미터만 업데이트\n",
    "- **안정적**: 기존 모델의 성능을 유지하면서 특화\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 필요한 라이브러리 불러오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설정에 필요한 라이브러리들을 불러옵니다\n",
    "import torch\n",
    "import json\n",
    "from transformers import (\n",
    "    AutoTokenizer,           # 텍스트를 토큰으로 변환\n",
    "    AutoModelForCausalLM,    # 언어 모델\n",
    "    BitsAndBytesConfig       # 4비트 양자화 설정\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,              # LoRA 설정\n",
    "    get_peft_model,          # LoRA 모델 생성\n",
    "    TaskType,                # 태스크 타입\n",
    "    prepare_model_for_kbit_training  # 4비트 학습 준비\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ 모델 설정 라이브러리가 준비되었습니다!\")\n",
    "print(f\"🔥 PyTorch 버전: {torch.__version__}\")\n",
    "print(f\"🚀 CUDA 사용 가능: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 설정 (매우 간단하게!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAONE 모델을 설정합니다 (하드코딩 없이!)\n",
    "print(\"🤖 EXAONE 모델 설정 중...\")\n",
    "\n",
    "# 모델 이름 (Hugging Face에서 가져오기) - 공개 모델 사용\n",
    "model_name = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n",
    "\n",
    "print(f\"📥 모델 다운로드 중: {model_name}\")\n",
    "print(\"💡 이 과정은 처음에만 시간이 걸립니다. 다음에는 캐시에서 빠르게 로드됩니다.\")\n",
    "\n",
    "# 4비트 양자화 설정 (메모리 절약을 위해)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # 4비트로 로드\n",
    "    bnb_4bit_compute_dtype=torch.float16, # 계산은 16비트로\n",
    "    bnb_4bit_use_double_quant=True,       # 이중 양자화 사용\n",
    "    bnb_4bit_quant_type=\"nf4\"             # 4비트 양자화 타입\n",
    ")\n",
    "\n",
    "print(\"✅ 모델 설정 완료!\")\n",
    "print(\"   - 4비트 양자화: 메모리 절약\")\n",
    "print(\"   - 16비트 계산: 정확도 유지\")\n",
    "print(\"   - 이중 양자화: 더 나은 압축\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 토크나이저 로드하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAONE-3.5-2.4B-Instruct 모델 사용 (공개 모델)\n",
    "print(\"✅ EXAONE-3.5-2.4B-Instruct 모델 사용\")\n",
    "print(\"   - 공개 모델: 인증 불필요\")\n",
    "print(\"   - 크기: 2.4B 파라미터 (7.8B보다 작고 빠름)\")\n",
    "print(\"   - 한국어 특화: EXAONE 시리즈\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저를 로드합니다 (텍스트를 토큰으로 변환하는 도구)\n",
    "print(\"🔤 토크나이저 로드 중...\")\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,  # EXAONE 모델의 특별한 코드 사용\n",
    "    padding_side=\"right\"     # 패딩을 오른쪽에 추가\n",
    ")\n",
    "\n",
    "# 패딩 토큰 설정 (배치 처리에 필요)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # EOS 토큰을 패딩 토큰으로 사용\n",
    "\n",
    "print(\"✅ 토크나이저 로드 완료!\")\n",
    "print(f\"   - 어휘 크기: {tokenizer.vocab_size:,}개\")\n",
    "print(f\"   - 패딩 토큰: {tokenizer.pad_token}\")\n",
    "print(f\"   - EOS 토큰: {tokenizer.eos_token}\")\n",
    "\n",
    "# 토크나이저 테스트\n",
    "test_text = \"안녕하세요! 인공지능에 대해 설명해주세요.\"\n",
    "tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "print(f\"\\n🧪 토크나이저 테스트:\")\n",
    "print(f\"   입력: {test_text}\")\n",
    "print(f\"   토큰 수: {tokens['input_ids'].shape[1]}개\")\n",
    "print(f\"   토큰 ID: {tokens['input_ids'][0][:10].tolist()}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 로드하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAONE 모델을 로드합니다 (4비트 양자화 적용)\n",
    "print(\"🤖 EXAONE 모델 로드 중...\")\n",
    "print(\"💡 이 과정은 시간이 걸릴 수 있습니다. 잠시만 기다려주세요!\")\n",
    "\n",
    "# 모델 로드 (4비트 양자화 적용)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,  # 4비트 양자화 적용\n",
    "    device_map=\"auto\",                       # 자동으로 GPU/CPU 배치\n",
    "    trust_remote_code=True,                  # EXAONE 모델의 특별한 코드 사용\n",
    "    torch_dtype=torch.float16                # 16비트 부동소수점 사용\n",
    ")\n",
    "\n",
    "print(\"✅ 모델 로드 완료!\")\n",
    "print(f\"   - 모델 크기: 7.8B 파라미터\")\n",
    "print(f\"   - 양자화: 4비트 (메모리 절약)\")\n",
    "print(f\"   - 디바이스: {next(model.parameters()).device}\")\n",
    "\n",
    "# 모델 정보 출력\n",
    "print(f\"\\n📊 모델 정보:\")\n",
    "print(f\"   - 모델 타입: {type(model).__name__}\")\n",
    "print(f\"   - 학습 가능한 파라미터: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}개\")\n",
    "print(f\"   - 전체 파라미터: {sum(p.numel() for p in model.parameters()):,}개\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LoRA 설정하기 (매우 간단하게!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 설정을 만듭니다 (모든 파라미터를 설명!)\n",
    "print(\"🔧 LoRA 설정 중...\")\n",
    "\n",
    "# LoRA 설정 (각 파라미터의 의미를 명확히 설명)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                           # LoRA rank (낮을수록 빠름, 높을수록 정확)\n",
    "    lora_alpha=32,                  # LoRA scaling (학습률 조절)\n",
    "    target_modules=[                # 어떤 레이어를 학습할지\n",
    "        \"q_proj\",                   # Query 프로젝션 (어텐션의 질문 부분)\n",
    "        \"v_proj\",                   # Value 프로젝션 (어텐션의 값 부분)\n",
    "        \"k_proj\",                   # Key 프로젝션 (어텐션의 키 부분)\n",
    "        \"o_proj\"                    # Output 프로젝션 (어텐션 출력)\n",
    "    ],\n",
    "    lora_dropout=0.05,              # 과적합 방지를 위한 드롭아웃\n",
    "    bias=\"none\",                    # 바이어스 학습 안함\n",
    "    task_type=TaskType.CAUSAL_LM,   # 언어 모델링 태스크\n",
    ")\n",
    "\n",
    "print(\"✅ LoRA 설정 완료!\")\n",
    "print(\"   - r=16: LoRA rank (적당한 크기)\")\n",
    "print(\"   - alpha=32: 스케일링 팩터\")\n",
    "print(\"   - target_modules: 어텐션 레이어들\")\n",
    "print(\"   - dropout=0.05: 과적합 방지\")\n",
    "print(\"   - task_type: 언어 모델링\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LoRA 모델 생성하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4비트 학습을 위한 모델 준비\n",
    "print(\"🔄 4비트 학습 준비 중...\")\n",
    "\n",
    "# 4비트 양자화된 모델을 학습 가능하게 준비\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA 어댑터를 모델에 추가\n",
    "print(\"🔧 LoRA 어댑터 추가 중...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"✅ LoRA 모델 생성 완료!\")\n",
    "\n",
    "# 학습 가능한 파라미터 확인\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\n📊 파라미터 정보:\")\n",
    "print(f\"   - 학습 가능한 파라미터: {trainable_params:,}개\")\n",
    "print(f\"   - 전체 파라미터: {all_params:,}개\")\n",
    "print(f\"   - 학습 비율: {trainable_params/all_params*100:.2f}%\")\n",
    "print(f\"   - 메모리 절약: {(1-trainable_params/all_params)*100:.1f}%\")\n",
    "\n",
    "# 모델 구조 확인\n",
    "print(f\"\\n🔍 LoRA 어댑터 정보:\")\n",
    "for name, module in model.named_modules():\n",
    "    if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):\n",
    "        print(f\"   - {name}: LoRA 어댑터 적용됨\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 모델 테스트하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정된 모델이 제대로 작동하는지 테스트해봅시다\n",
    "print(\"🧪 모델 테스트 중...\")\n",
    "\n",
    "# 테스트 텍스트\n",
    "test_prompt = \"질문: 인공지능이란 무엇인가요?\\n답변:\"\n",
    "\n",
    "# 토크나이징\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 생성 설정\n",
    "generation_config = {\n",
    "    \"max_new_tokens\": 50,        # 최대 50개 토큰 생성\n",
    "    \"temperature\": 0.7,          # 창의성 조절 (0.7 = 적당히 창의적)\n",
    "    \"do_sample\": True,           # 샘플링 사용\n",
    "    \"pad_token_id\": tokenizer.eos_token_id  # 패딩 토큰 ID\n",
    "}\n",
    "\n",
    "# 텍스트 생성\n",
    "with torch.no_grad():  # 그래디언트 계산 안함 (메모리 절약)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        **generation_config\n",
    "    )\n",
    "\n",
    "# 생성된 텍스트 디코딩\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"✅ 모델 테스트 완료!\")\n",
    "print(f\"\\n📝 테스트 결과:\")\n",
    "print(f\"입력: {test_prompt}\")\n",
    "print(f\"출력: {generated_text}\")\n",
    "print(f\"\\n🎯 생성된 부분만:\")\n",
    "print(generated_text[len(test_prompt):])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 설정 저장하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA가 뭔가요?\n",
    "\n",
    "- **전체 미세조정(Full Fine-tuning)**은 모델의 모든 가중치를 다시 학습합니다. 예를 들어 70억(7B) 파라미터 모델이라면 수 GB의 메모리가 필요하고, 저장 파일도 동일한 크기로 생깁니다.\n",
    "- **LoRA(Low-Rank Adaptation)**는 중요한 층에만 매우 작은 보조 행렬을 붙여 학습합니다. 원래 가중치는 그대로 고정하고 새로 추가된 수백만 개 정도의 파라미터만 업데이트하므로 GPU 메모리와 저장 공간이 크게 줄어듭니다.\n",
    "- 이렇게 학습한 보조 행렬을 원본 모델에 더하면 전체 모델이 미세조정된 것처럼 동작합니다. 필요할 때만 불러오니 학습/배포 모두 가볍습니다.\n",
    "- 하이퍼파라미터 이해하기:\n",
    "  - `r`: 추가 행렬의 랭크(크기). 값이 클수록 표현력이 높아지지만 학습량이 늘어납니다.\n",
    "  - `lora_alpha`: 학습된 LoRA 가중치를 얼마나 크게 반영할지 정하는 스케일 값입니다.\n",
    "  - `target_modules`: LoRA를 적용할 모델 층 목록입니다. 여기만 추가 파라미터가 생깁니다.\n",
    "  - `lora_dropout`: 학습 중 일부 LoRA 연결을 끊어 과적합을 막는 확률입니다.\n",
    "- 이 노트북에서는 이렇게 학습한 LoRA 구성과 가중치를 따로 저장했다가, 다음 단계에서 기본 모델과 다시 합쳐 쓰도록 준비합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설정을 저장합니다 (다음 단계에서 사용)\n",
    "print(\"💾 모델 설정 저장 중...\")\n",
    "\n",
    "# JSON 직렬화를 위해 필요한 값 정리\n",
    "target_modules = lora_config.target_modules\n",
    "if isinstance(target_modules, (set, tuple)):\n",
    "    target_modules = list(target_modules)\n",
    "\n",
    "quant_type = quantization_config.bnb_4bit_quant_type\n",
    "if quant_type is not None:\n",
    "    quant_type = str(quant_type)\n",
    "\n",
    "# 모델 설정 정보\n",
    "model_config = {\n",
    "    \"model_name\": model_name,\n",
    "    \"lora_config\": {\n",
    "        \"r\": lora_config.r,\n",
    "        \"lora_alpha\": lora_config.lora_alpha,\n",
    "        \"target_modules\": target_modules,\n",
    "        \"lora_dropout\": lora_config.lora_dropout,\n",
    "        \"bias\": lora_config.bias,\n",
    "        \"task_type\": str(lora_config.task_type)\n",
    "    },\n",
    "    \"quantization_config\": {\n",
    "        \"load_in_4bit\": quantization_config.load_in_4bit,\n",
    "        \"bnb_4bit_compute_dtype\": str(quantization_config.bnb_4bit_compute_dtype),\n",
    "        \"bnb_4bit_use_double_quant\": quantization_config.bnb_4bit_use_double_quant,\n",
    "        \"bnb_4bit_quant_type\": quant_type\n",
    "    },\n",
    "    \"model_info\": {\n",
    "        \"trainable_params\": int(trainable_params),\n",
    "        \"all_params\": int(all_params),\n",
    "        \"trainable_ratio\": float(trainable_params / all_params)\n",
    "    }\n",
    "}\n",
    "\n",
    "# 설정 저장\n",
    "with open(\"models/model_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(model_config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# LoRA 설정도 별도로 저장\n",
    "model.save_pretrained(\"models/lora_config\")\n",
    "\n",
    "print(\"✅ 설정 저장 완료!\")\n",
    "print(\"   - models/model_config.json: 전체 설정\")\n",
    "print(\"   - models/lora_config/: LoRA 설정\")\n",
    "print(\"   - 다음 단계에서 이 설정들을 사용합니다\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 왜 설정을 따로 저장하나요?\n",
    "\n",
    "- 동일한 환경에서 다시 학습하거나 평가할 때, 설정 파일 하나만 있으면 파라미터 값을 일일이 기억하지 않아도 재현할 수 있습니다.\n",
    "- 팀원과 공유하거나 다른 실험에서 재사용할 때, LoRA 가중치(`models/lora_config/`)와 구성(`models/model_config.json`)을 함께 전달하면 바로 불러와 사용할 수 있습니다.\n",
    "- 추후에 LoRA만 바꿔 끼우면서 여러 실험을 비교하려면, 기본 모델은 그대로 두고 LoRA 파일만 교체하면 되므로 관리가 간편해집니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 다음 단계 안내\n",
    "\n",
    "### 🎯 다음 노트북에서 할 일\n",
    "**00.05-fine-tuning.ipynb**에서:\n",
    "1. **RAFT 데이터** 로드하기\n",
    "2. **학습 설정** 구성하기\n",
    "3. **실제 파인튜닝** 실행하기\n",
    "4. **학습 과정** 모니터링하기\n",
    "\n",
    "### 💡 지금까지 배운 것\n",
    "- ✅ EXAONE 모델 로드 및 설정\n",
    "- ✅ 4비트 양자화로 메모리 절약\n",
    "- ✅ LoRA 설정으로 효율적 학습\n",
    "- ✅ 토크나이저 설정 및 테스트\n",
    "- ✅ 모델 설정 저장\n",
    "\n",
    "### 🔧 LoRA의 핵심\n",
    "- **r=16**: LoRA rank (적당한 크기)\n",
    "- **target_modules**: 어텐션 레이어들만 학습\n",
    "- **메모리 절약**: 전체 모델의 1%만 학습\n",
    "- **빠른 학습**: 적은 파라미터로 효율적\n",
    "\n",
    "### 🚀 준비 완료!\n",
    "이제 다음 노트북으로 넘어가서 실제 파인튜닝을 실행해보겠습니다!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}