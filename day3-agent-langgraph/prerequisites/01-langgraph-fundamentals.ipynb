{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 01: LangGraph 고급 워크플로우 패턴\n",
    "\n",
    "## 💡 00번에서 배운 기본기를 고급 패턴으로 발전시키자!\n",
    "\n",
    "00번에서 **State**, **Node**, **StateGraph**의 기본 개념을 배웠어요.  \n",
    "이제 LLM을 활용한 실무 수준의 고급 워크플로우 패턴들을 학습해보자!\n",
    "\n",
    "## 🎯 이 노트북에서 배우는 것들\n",
    "\n",
    "### ✅ 고급 워크플로우 목표\n",
    "1. **Routing**: LLM이 쿼리를 분석해서 적절한 경로로 라우팅\n",
    "2. **Fan-out/Fan-in**: 병렬 처리로 효율성 극대화\n",
    "3. **대화 기록 요약**: 긴 대화를 지능적으로 요약\n",
    "4. **Human in the Loop**: AI 처리에 인간 검토 추가\n",
    "5. **실무 통합**: 모든 패턴을 조합한 실제 시스템\n",
    "\n",
    "### 🔄 학습 흐름\n",
    "- **00번**: 기본 개념 이해 (State, Node, 하드코딩 방식)\n",
    "- **01번**: LLM 활용 고급 패턴 (라우팅, 병렬처리, 요약 등)\n",
    "- **02번**: 실제 메모리 + ReAct Agent 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치 및 import + EXAONE 모델 로드\n",
    "!pip install -q langgraph langchain langchain-teddynote\n",
    "!pip install -q grandalf matplotlib networkx pyppeteer\n",
    "!pip install -q git+https://github.com/lgai-exaone/transformers@add-exaone4\n",
    "!pip install -q torch accelerate\n",
    "\n",
    "from typing import TypedDict, List, Dict, Any, Annotated\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import interrupt, Command\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_teddynote.graphs import visualize_graph\n",
    "from langchain_teddynote.messages import stream_graph, random_uuid\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import gc\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ 라이브러리 import 완료!\")\n",
    "print(\"🔥 실제 Human in the Loop을 위한 interrupt, Command 추가!\")\n",
    "\n",
    "# 🤖 EXAONE 모델 로드\n",
    "MODEL_NAME = \"LGAI-EXAONE/EXAONE-4.0-1.2B\"\n",
    "\n",
    "print(f\"🚀 EXAONE-4.0-1.2B 모델 로드 시작: {MODEL_NAME}\")\n",
    "\n",
    "# EXAONE-4.0-1.2B 모델 및 토크나이저 로드 (CPU 호환)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32,  # CPU 호환을 위해 float32 사용\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"✅ EXAONE-4.0-1.2B 모델 로드 성공!\")\n",
    "\n",
    "# GPU 메모리 최적화\n",
    "def clear_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def pure_exaone_inference(messages_or_prompt):\n",
    "    \"\"\"🔥 EXAONE 대화 맥락 지원 함수 - HuggingFace 가이드라인 준수\"\"\"\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    try:\n",
    "        # 🎯 입력이 문자열이면 단일 메시지로, 리스트면 대화 맥락으로 처리\n",
    "        if isinstance(messages_or_prompt, str):\n",
    "            messages = [{\"role\": \"user\", \"content\": messages_or_prompt}]\n",
    "        else:\n",
    "            # 이미 EXAONE chat format인 경우\n",
    "            messages = messages_or_prompt\n",
    "        \n",
    "        # 🔥 EXAONE 표준 chat template로 전체 대화 맥락 처리\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,  # 전체 대화 history\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=80,     # 더 짧고 집중된 응답\n",
    "                temperature=0.1,       # 일관성을 위한 낮은 값\n",
    "                do_sample=True,        # HuggingFace 가이드 권장\n",
    "                top_p=0.9,            # 적절한 창의성 허용\n",
    "                pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # 입력 길이만큼 제거하고 새로 생성된 부분만 추출\n",
    "        generated_ids = outputs[0][input_ids.shape[-1]:]\n",
    "        ai_response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "        \n",
    "        clear_gpu_memory()\n",
    "        \n",
    "        # 🔥 깔끔한 응답만 반환\n",
    "        return ai_response if ai_response else \"안녕하세요!\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        clear_gpu_memory()\n",
    "        print(f\"🚨 EXAONE 추론 오류: {e}\")\n",
    "        return \"죄송해요, 다시 말씀해 주시겠어요?\"\n",
    "\n",
    "print(\"✅ EXAONE 모델 설정 완료!\")\n",
    "print(\"🔥 최적화된 EXAONE 시스템:\")\n",
    "print(\"   💬 chat template: 전체 대화 history 처리\")\n",
    "print(\"   🎯 temperature=0.1 + do_sample=True: 일관성 + 창의성\")\n",
    "print(\"   🧠 max_new_tokens=80: 집중된 응답\")\n",
    "print(\"   🎪 top_p=0.9: 적절한 다양성\")\n",
    "print(\"💻 CPU 최적화 모드 (교육용 안정성 우선)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 Part 1: Routing 패턴 - LLM이 판단하는 지능적 라우팅\n",
    "\n",
    "### 🎯 Routing이란?\n",
    "00번에서는 **글자 수**나 **단순 조건**으로 분기했지만,  \n",
    "이제는 **LLM이 내용을 이해**해서 적절한 처리 경로를 결정해보자!\n",
    "\n",
    "### 🔄 실무 시나리오\n",
    "- **계산 문제**: 계산기로 라우팅\n",
    "- **번역 요청**: 번역 엔진으로 라우팅  \n",
    "- **일반 대화**: 대화 시스템으로 라우팅\n",
    "- **코딩 질문**: 코드 분석기로 라우팅\n",
    "\n",
    "**핵심**: LLM이 질문의 **의도**를 파악해서 최적의 처리 방법 선택!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🤖 Routing용 State 정의\n",
    "class RoutingState(TypedDict):\n",
    "    user_query: str          # 사용자 질문\n",
    "    query_type: str          # LLM이 분석한 질문 유형\n",
    "    processing_route: str    # 선택된 처리 경로\n",
    "    result: str             # 최종 처리 결과\n",
    "    confidence: float       # 라우팅 신뢰도\n",
    "\n",
    "def intelligent_router(state: RoutingState) -> str:\n",
    "    \"\"\"🧠 EXAONE LLM이 질문을 분석해서 적절한 경로 결정\"\"\"\n",
    "    user_query = state[\"user_query\"]\n",
    "    \n",
    "    # 🤖 EXAONE에게 질문 분류 요청\n",
    "    routing_prompt = f\"\"\"\n",
    "다음 사용자 질문을 분석해서 가장 적절한 처리 방법을 선택해주세요:\n",
    "\n",
    "질문: {user_query}\n",
    "\n",
    "선택 가능한 경로:\n",
    "1. \"calculation\" - 수학 계산이나 연산이 필요한 질문\n",
    "2. \"translation\" - 번역이나 언어 변환이 필요한 질문\n",
    "3. \"coding\" - 프로그래밍이나 코드 관련 질문\n",
    "4. \"general\" - 일반적인 대화나 상식 질문\n",
    "\n",
    "가장 적절한 경로를 하나만 선택해서 답변해주세요: \"\"\"\n",
    "\n",
    "    llm_response = pure_exaone_inference(routing_prompt)\n",
    "    \n",
    "    # 🔍 LLM 응답에서 라우팅 경로 추출\n",
    "    if \"calculation\" in llm_response.lower():\n",
    "        return \"calculation\"\n",
    "    elif \"translation\" in llm_response.lower():\n",
    "        return \"translation\" \n",
    "    elif \"coding\" in llm_response.lower():\n",
    "        return \"coding\"\n",
    "    else:\n",
    "        return \"general\"\n",
    "\n",
    "print(\"🧠 지능적 라우터 정의 완료!\")\n",
    "print(\"   - EXAONE LLM이 질문 내용을 분석\")\n",
    "print(\"   - 4가지 경로 중 최적 경로 선택\")\n",
    "print(\"   - 하드코딩 없는 완전 지능적 판단\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 각 경로별 처리 노드들 정의\n",
    "def calculation_node(state: RoutingState) -> RoutingState:\n",
    "    \"\"\"🧮 계산 전용 노드 - EXAONE을 계산기로 활용\"\"\"\n",
    "    user_query = state[\"user_query\"]\n",
    "    \n",
    "    # EXAONE에게 계산 요청\n",
    "    calc_prompt = f\"\"\"\n",
    "다음 수학 계산을 정확히 계산해주세요:\n",
    "\n",
    "계산: {user_query}\n",
    "\n",
    "숫자와 기본 사칙연산만 사용해서 정확한 답을 제공해주세요.\n",
    "예: 10 + 5 = 15\n",
    "\"\"\"\n",
    "    \n",
    "    result = pure_exaone_inference(calc_prompt)\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"query_type\": \"calculation\",\n",
    "        \"processing_route\": \"계산 처리\",\n",
    "        \"result\": f\"🧮 계산 결과: {result}\",\n",
    "        \"confidence\": 0.95\n",
    "    }\n",
    "\n",
    "def translation_node(state: RoutingState) -> RoutingState:\n",
    "    \"\"\"🌐 번역 전용 노드 - EXAONE을 번역기로 활용\"\"\"\n",
    "    user_query = state[\"user_query\"]\n",
    "    \n",
    "    translation_prompt = f\"\"\"\n",
    "다음 텍스트를 번역해주세요:\n",
    "\n",
    "원문: {user_query}\n",
    "\n",
    "한국어면 영어로, 영어면 한국어로 번역해주세요.\n",
    "정확하고 자연스러운 번역을 제공해주세요.\n",
    "\"\"\"\n",
    "    \n",
    "    result = pure_exaone_inference(translation_prompt)\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"query_type\": \"translation\", \n",
    "        \"processing_route\": \"번역 처리\",\n",
    "        \"result\": f\"🌐 번역 결과: {result}\",\n",
    "        \"confidence\": 0.90\n",
    "    }\n",
    "\n",
    "def coding_node(state: RoutingState) -> RoutingState:\n",
    "    \"\"\"💻 코딩 전용 노드 - EXAONE을 프로그래밍 도우미로 활용\"\"\"\n",
    "    user_query = state[\"user_query\"]\n",
    "    \n",
    "    coding_prompt = f\"\"\"\n",
    "다음 프로그래밍 질문에 답변해주세요:\n",
    "\n",
    "질문: {user_query}\n",
    "\n",
    "코드 예시와 함께 명확한 설명을 제공해주세요.\n",
    "실용적이고 이해하기 쉬운 답변을 부탁드립니다.\n",
    "\"\"\"\n",
    "    \n",
    "    result = pure_exaone_inference(coding_prompt)\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"query_type\": \"coding\",\n",
    "        \"processing_route\": \"코딩 처리\", \n",
    "        \"result\": f\"💻 코딩 답변: {result}\",\n",
    "        \"confidence\": 0.85\n",
    "    }\n",
    "\n",
    "def general_node(state: RoutingState) -> RoutingState:\n",
    "    \"\"\"💬 일반 대화 노드 - EXAONE을 친근한 대화 상대로 활용\"\"\"\n",
    "    user_query = state[\"user_query\"]\n",
    "    \n",
    "    general_prompt = f\"\"\"\n",
    "다음 질문에 친근하고 도움이 되는 답변을 해주세요:\n",
    "\n",
    "질문: {user_query}\n",
    "\n",
    "자연스럽고 유용한 정보를 포함한 답변을 부탁드립니다.\n",
    "\"\"\"\n",
    "    \n",
    "    result = pure_exaone_inference(general_prompt)\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"query_type\": \"general\",\n",
    "        \"processing_route\": \"일반 대화 처리\",\n",
    "        \"result\": f\"💬 일반 답변: {result}\",\n",
    "        \"confidence\": 0.80\n",
    "    }\n",
    "\n",
    "print(\"🔧 모든 라우팅 처리 노드 정의 완료!\")\n",
    "print(\"   🧮 calculation_node: 수학 계산 전문\")\n",
    "print(\"   🌐 translation_node: 언어 번역 전문\")  \n",
    "print(\"   💻 coding_node: 프로그래밍 질문 전문\")\n",
    "print(\"   💬 general_node: 일반 대화 전문\")\n",
    "print(\"   🎯 각 노드가 특화된 EXAONE 프롬프트 사용\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 지능적 라우팅 워크플로우 구축\n",
    "routing_workflow = StateGraph(RoutingState)\n",
    "\n",
    "# 모든 처리 노드들 추가\n",
    "routing_workflow.add_node(\"calculation\", calculation_node)\n",
    "routing_workflow.add_node(\"translation\", translation_node) \n",
    "routing_workflow.add_node(\"coding\", coding_node)\n",
    "routing_workflow.add_node(\"general\", general_node)\n",
    "\n",
    "# 🧠 조건부 시작점: EXAONE이 분석해서 적절한 노드로 라우팅\n",
    "routing_workflow.set_conditional_entry_point(\n",
    "    intelligent_router,  # LLM 기반 라우팅 함수\n",
    "    {\n",
    "        \"calculation\": \"calculation\",\n",
    "        \"translation\": \"translation\", \n",
    "        \"coding\": \"coding\",\n",
    "        \"general\": \"general\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# 모든 처리 노드는 END로 연결 (단일 처리 후 종료)\n",
    "routing_workflow.add_edge(\"calculation\", END)\n",
    "routing_workflow.add_edge(\"translation\", END)\n",
    "routing_workflow.add_edge(\"coding\", END)\n",
    "routing_workflow.add_edge(\"general\", END)\n",
    "\n",
    "# 워크플로우 컴파일\n",
    "routing_app = routing_workflow.compile()\n",
    "\n",
    "print(\"🔄 지능적 라우팅 워크플로우 완성!\")\n",
    "print(\"   🧠 EXAONE LLM이 질문 내용 분석\")\n",
    "print(\"   🎯 4개 전문 노드 중 최적 경로 자동 선택\")  \n",
    "print(\"   ⚡ 하드코딩 없는 완전 지능적 분기\")\n",
    "print(\"   💎 실무에서 바로 사용 가능한 패턴\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 라우팅 워크플로우 시각화  \n",
    "print(\"📊 지능적 라우팅 워크플로우 구조:\")\n",
    "visualize_graph(routing_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 지능적 라우팅 시스템 테스트\n",
    "print(\"🧪 EXAONE 기반 지능적 라우팅 테스트\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 4가지 다른 유형의 질문으로 라우팅 테스트\n",
    "routing_test_queries = [\n",
    "    \"10 + 25를 계산해줘\",\n",
    "    \"안녕하세요를 영어로 번역해줘\", \n",
    "    \"파이썬에서 리스트를 어떻게 만드나요?\",\n",
    "    \"오늘 날씨가 정말 좋네요!\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(routing_test_queries, 1):\n",
    "    print(f\"\\n🔍 테스트 {i}: \\\"{query}\\\"\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 라우팅 시스템 실행\n",
    "    result = routing_app.invoke({\n",
    "        \"user_query\": query,\n",
    "        \"query_type\": \"\",\n",
    "        \"processing_route\": \"\",\n",
    "        \"result\": \"\",\n",
    "        \"confidence\": 0.0\n",
    "    })\n",
    "    \n",
    "    print(f\"🧠 질문 유형 분석: {result['query_type']}\")\n",
    "    print(f\"🛣️  선택된 처리 경로: {result['processing_route']}\")\n",
    "    print(f\"📊 라우팅 신뢰도: {result['confidence']*100:.0f}%\")\n",
    "    print(f\"✅ 처리 결과: {result['result']}\")\n",
    "\n",
    "print(f\"\\n🎯 지능적 라우팅 테스트 완료!\")\n",
    "print(\"💡 EXAONE LLM이 질문 내용을 이해하고 적절한 경로로 자동 분기!\")\n",
    "print(\"🔥 하드코딩 없이 완전히 지능적인 워크플로우 구현!\")\n",
    "\n",
    "# 라우팅 워크플로우의 핵심 장점 설명\n",
    "print(f\"\\n{'🎯' * 20} 라우팅 패턴의 핵심 장점 {'🎯' * 20}\")\n",
    "print(\"✅ 완전 지능적 분기: LLM이 내용을 이해해서 판단\")\n",
    "print(\"✅ 확장성: 새로운 처리 경로 추가 시 코드 수정 최소화\")\n",
    "print(\"✅ 유지보수성: 각 처리 노드가 독립적으로 관리\")\n",
    "print(\"✅ 실무 적용성: 실제 서비스에서 바로 사용 가능한 구조\")\n",
    "print(\"✅ 성능 최적화: 각 작업에 특화된 프롬프트로 최적 성능\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Fan-out/Fan-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Annotated\n",
    "import operator\n",
    "\n",
    "class FanoutState(TypedDict):\n",
    "    user_topic: str              # 사용자가 요청한 주제\n",
    "    results: Annotated[List[str], operator.add]  # 병렬 결과들을 병합\n",
    "    final_summary: str           # 최종 통합 결과\n",
    "\n",
    "def distributor_node(state: FanoutState) -> FanoutState:\n",
    "    \"\"\"🚀 시작 노드 - 병렬 처리를 위한 분배\"\"\"\n",
    "    return {\n",
    "        **state,\n",
    "        \"results\": []  # 결과 리스트 초기화\n",
    "    }\n",
    "\n",
    "# 🏛️ 역사 전문 노드\n",
    "def history_research_node(state: FanoutState) -> FanoutState:\n",
    "    \"\"\"🏛️ 역사 정보를 전문적으로 조사\"\"\"\n",
    "    topic = state[\"user_topic\"]\n",
    "    \n",
    "    history_prompt = f\"\"\"\n",
    "'{topic}'에 관련된 역사적 배경과 중요한 역사적 사건들에 대해 전문적으로 설명해주세요:\n",
    "\n",
    "주제: {topic}\n",
    "\n",
    "역사적 관점에서:\n",
    "- 주요 역사적 사건\n",
    "- 시대적 배경\n",
    "- 역사적 의미와 영향\n",
    "\n",
    "전문적이고 정확한 역사 정보를 제공해주세요.\n",
    "\"\"\"\n",
    "    \n",
    "    result = pure_exaone_inference(history_prompt)\n",
    "    \n",
    "    return {\n",
    "        \"results\": [f\"🏛️ 역사 분야: {result}\"]\n",
    "    }\n",
    "\n",
    "# 🎭 문화 전문 노드\n",
    "def culture_research_node(state: FanoutState) -> FanoutState:\n",
    "    \"\"\"🎭 문화 정보를 전문적으로 조사\"\"\"\n",
    "    topic = state[\"user_topic\"]\n",
    "    \n",
    "    culture_prompt = f\"\"\"\n",
    "'{topic}'에 관련된 문화적 특징과 전통에 대해 전문적으로 설명해주세요:\n",
    "\n",
    "주제: {topic}\n",
    "\n",
    "문화적 관점에서:\n",
    "- 전통 문화와 풍습\n",
    "- 예술과 문학\n",
    "- 생활 문화와 특징\n",
    "\n",
    "문화적 깊이가 있는 정보를 제공해주세요.\n",
    "\"\"\"\n",
    "    \n",
    "    result = pure_exaone_inference(culture_prompt)\n",
    "    \n",
    "    return {\n",
    "        \"results\": [f\"🎭 문화 분야: {result}\"]\n",
    "    }\n",
    "\n",
    "# 🗺️ 관광지 전문 노드  \n",
    "def tourism_research_node(state: FanoutState) -> FanoutState:\n",
    "    \"\"\"🗺️ 관광 정보를 전문적으로 조사\"\"\"\n",
    "    topic = state[\"user_topic\"]\n",
    "    \n",
    "    tourism_prompt = f\"\"\"\n",
    "'{topic}'에 관련된 관광지와 명소에 대해 전문적으로 설명해주세요:\n",
    "\n",
    "주제: {topic}\n",
    "\n",
    "관광 관점에서:\n",
    "- 대표적인 명소와 관광지\n",
    "- 특별한 체험과 활동\n",
    "- 방문할만한 가치가 있는 곳들\n",
    "\n",
    "실용적인 관광 정보를 제공해주세요.\n",
    "\"\"\"\n",
    "    \n",
    "    result = pure_exaone_inference(tourism_prompt)\n",
    "    \n",
    "    return {\n",
    "        \"results\": [f\"🗺️ 관광 분야: {result}\"]\n",
    "    }\n",
    "\n",
    "print(\"🔄 Fan-out 노드들 정의 완료!\")\n",
    "print(\"   🚀 distributor_node: 병렬 처리 시작점\")\n",
    "print(\"   🏛️ history_research_node: 역사 전문 조사\")\n",
    "print(\"   🎭 culture_research_node: 문화 전문 조사\")\n",
    "print(\"   🗺️ tourism_research_node: 관광 전문 조사\")\n",
    "print(\"   ⚡ Annotated 키로 안전한 병렬 결과 병합\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔗 Fan-in 노드 및 올바른 워크플로우 구축\n",
    "def synthesis_node(state: FanoutState) -> FanoutState:\n",
    "    \"\"\"🔗 3개 전문 분야 결과를 종합해서 완전한 답변 생성\"\"\"\n",
    "    topic = state[\"user_topic\"]\n",
    "    results = state.get(\"results\", [])\n",
    "    \n",
    "    # 각 분야별 결과 정리\n",
    "    all_results = \"\\n\\n\".join(results)\n",
    "    \n",
    "    # 전체 정보를 종합하는 EXAONE 프롬프트\n",
    "    synthesis_prompt = f\"\"\"\n",
    "다음 주제에 대한 3개 분야의 전문 조사 결과를 종합해서 완전하고 체계적인 답변을 만들어주세요:\n",
    "\n",
    "주제: {topic}\n",
    "\n",
    "조사 결과:\n",
    "{all_results}\n",
    "\n",
    "위 3개 전문 분야의 정보를 종합하여:\n",
    "1. 전체적인 개요와 특징\n",
    "2. 각 분야 간의 연관성과 통합적 관점\n",
    "3. 종합적인 결론과 권장사항\n",
    "\n",
    "체계적이고 완성도 높은 통합 답변을 제공해주세요.\n",
    "\"\"\"\n",
    "    \n",
    "    final_result = pure_exaone_inference(synthesis_prompt)\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"final_summary\": f\"🔗 종합 결과: {final_result}\"\n",
    "    }\n",
    "\n",
    "# 🔄 Fan-out/Fan-in 워크플로우 구축 (LangGraph 표준 구조)\n",
    "fanout_workflow = StateGraph(FanoutState)\n",
    "\n",
    "# 1️⃣ 모든 노드 추가\n",
    "fanout_workflow.add_node(\"distributor\", distributor_node)    # 시작/분배 노드\n",
    "fanout_workflow.add_node(\"history\", history_research_node)   # 병렬 노드 1\n",
    "fanout_workflow.add_node(\"culture\", culture_research_node)   # 병렬 노드 2  \n",
    "fanout_workflow.add_node(\"tourism\", tourism_research_node)   # 병렬 노드 3\n",
    "fanout_workflow.add_node(\"synthesis\", synthesis_node)        # 통합 노드\n",
    "\n",
    "# 2️⃣ 워크플로우 연결 (사용자 예시 패턴 적용)\n",
    "fanout_workflow.add_edge(START, \"distributor\")    # START → distributor\n",
    "\n",
    "# 3️⃣ Fan-out: distributor에서 3개 병렬 노드로 분기\n",
    "fanout_workflow.add_edge(\"distributor\", \"history\")\n",
    "fanout_workflow.add_edge(\"distributor\", \"culture\") \n",
    "fanout_workflow.add_edge(\"distributor\", \"tourism\")\n",
    "\n",
    "# 4️⃣ Fan-in: 3개 병렬 노드에서 synthesis로 통합\n",
    "fanout_workflow.add_edge(\"history\", \"synthesis\")\n",
    "fanout_workflow.add_edge(\"culture\", \"synthesis\")\n",
    "fanout_workflow.add_edge(\"tourism\", \"synthesis\")\n",
    "\n",
    "# 5️⃣ 최종 종료\n",
    "fanout_workflow.add_edge(\"synthesis\", END)\n",
    "\n",
    "# 컴파일\n",
    "fanout_app = fanout_workflow.compile()\n",
    "\n",
    "print(\"🔄 Fan-out/Fan-in 워크플로우 완성!\")\n",
    "print(\"   🚀 START → distributor: 단일 시작점\")\n",
    "print(\"   📤 Fan-out: distributor → {history, culture, tourism}\")\n",
    "print(\"   📥 Fan-in: {history, culture, tourism} → synthesis\") \n",
    "print(\"   🏁 synthesis → END: 최종 완료\")\n",
    "print(\"   ⚡ LangGraph 표준 구조로 안전한 병렬 처리!\")\n",
    "print(\"   🎯 InvalidUpdateError 완전 해결!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Fan-out/Fan-in 워크플로우 시각화\n",
    "print(\"📊 병렬 처리 워크플로우 구조:\")\n",
    "visualize_graph(fanout_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 Fan-out/Fan-in 시스템 테스트 (수정된 구조)\n",
    "print(\"🧪 병렬 처리 시스템 테스트\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 복합적인 주제로 테스트 (3개 분야 모두에서 정보가 나올 수 있는 주제)\n",
    "test_topic = \"한국\"\n",
    "\n",
    "print(f\"🔍 테스트 주제: \\\"{test_topic}\\\"\")\n",
    "print(\"📤 Fan-out: 역사, 문화, 관광 3개 분야 병렬 조사 시작...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Fan-out/Fan-in 시스템 실행 (새로운 State 구조)\n",
    "result = fanout_app.invoke({\n",
    "    \"user_topic\": test_topic,\n",
    "    \"results\": [],\n",
    "    \"final_summary\": \"\"\n",
    "})\n",
    "\n",
    "print(\"📥 병렬 처리 결과:\")\n",
    "print(f\"\\n🔗 수집된 전문 분야 결과 ({len(result['results'])}개):\")\n",
    "for i, res in enumerate(result['results'], 1):\n",
    "    print(f\"   {i}. {res}\")\n",
    "\n",
    "print(f\"\\n🔗 최종 통합 결과:\")\n",
    "print(f\"   {result['final_summary']}\")\n",
    "\n",
    "print(f\"\\n🎯 Fan-out/Fan-in 테스트 완료!\")\n",
    "print(\"🔥 LangGraph 표준 구조로 완벽한 Fan-out/Fan-in 구현!\")\n",
    "\n",
    "# Fan-out/Fan-in 패턴의 핵심 장점 설명\n",
    "print(f\"\\n{'🔄' * 20} Fan-out/Fan-in 패턴의 핵심 장점 {'🔄' * 20}\")\n",
    "print(\"⚡ 성능 향상: 병렬 처리로 처리 시간 단축\")\n",
    "print(\"🎯 품질 향상: 다각도 전문 분석으로 완성도 높은 답변\")\n",
    "print(\"🔧 안전성: Annotated 키로 상태 충돌 완전 방지\")\n",
    "print(\"📈 확장성: 새로운 전문 분야 추가 용이\")\n",
    "print(\"🎪 실무 적용: LangGraph 표준을 따르는 안정적 구조\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Part 3: 대화 기록 요약 패턴 - 지능적 메모리 관리\n",
    "\n",
    "### 🎯 대화 요약이 왜 중요한가?\n",
    "긴 대화가 계속되면 **토큰 한계**에 도달하고 **성능이 저하**됩니다.  \n",
    "하지만 단순히 오래된 대화를 버리면 **중요한 맥락을 잃게** 됩니다.\n",
    "\n",
    "### 💡 지능적 해결책: EXAONE 기반 요약\n",
    "- **중요한 정보 보존**: 사용자 이름, 선호도, 핵심 대화 내용\n",
    "- **불필요한 내용 제거**: 인사말, 반복적인 내용, 임시 정보  \n",
    "- **맥락 유지**: 요약 후에도 자연스러운 대화 흐름 보장\n",
    "\n",
    "### 🔄 실무 시나리오\n",
    "1. 대화가 20턴을 넘어가면 자동 요약 트리거\n",
    "2. EXAONE이 핵심 내용만 추출해서 간단히 요약\n",
    "3. 요약된 내용 + 최근 5턴으로 메모리 최적화\n",
    "4. 성능 유지하면서 맥락도 보존하는 완벽한 균형!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📚 대화 요약용 State 정의 (디버그 출력 추가)\n",
    "class SummarizationState(TypedDict):\n",
    "    conversation_history: List[str]  # 전체 대화 기록\n",
    "    summary: str                     # 요약된 핵심 내용  \n",
    "    recent_turns: List[str]         # 최근 몇 턴의 대화\n",
    "    user_input: str                 # 현재 사용자 입력\n",
    "    response: str                   # AI 응답\n",
    "    should_summarize: bool          # 요약 필요 여부\n",
    "\n",
    "def should_summarize_check(state: SummarizationState) -> str:\n",
    "    \"\"\"🔍 요약이 필요한지 판단 (3턴 = 6개 항목 기준)\"\"\"\n",
    "    history = state.get(\"conversation_history\", [])\n",
    "    current_count = len(history)\n",
    "    \n",
    "    # 🐛 요약 체크 함수 디버그 출력\n",
    "    print(f\"🔍 [DEBUG] should_summarize_check 실행\")\n",
    "    print(f\"🔍 [DEBUG] 현재 대화 기록: {current_count}개\")\n",
    "    print(f\"🔍 [DEBUG] 요약 임계값: 6개 (3턴)\")\n",
    "    \n",
    "    if current_count >= 6:  # 3턴 = 6개 항목 (사용자+AI 각 1개씩)\n",
    "        print(f\"🔍 [DEBUG] → 요약 필요! (임계값 도달)\")\n",
    "        return \"summarize\"\n",
    "    else:\n",
    "        remaining = 6 - current_count\n",
    "        print(f\"🔍 [DEBUG] → 일반 대화 (요약까지 {remaining}개 항목 남음)\")\n",
    "        return \"normal_chat\"\n",
    "\n",
    "def summarization_node(state: SummarizationState) -> SummarizationState:\n",
    "    \"\"\"📚 EXAONE을 사용해서 대화 기록을 지능적으로 요약\"\"\"\n",
    "    history = state.get(\"conversation_history\", [])\n",
    "    \n",
    "    # 🐛 요약 노드 실행 시작 디버그\n",
    "    print(\"📚 [DEBUG] 요약 노드 실행 시작!\")\n",
    "    print(f\"📚 [DEBUG] 요약 전 대화 기록: {len(history)}개\")\n",
    "    \n",
    "    # 기존 요약이 있다면 포함\n",
    "    existing_summary = state.get(\"summary\", \"\")\n",
    "    summary_prefix = f\"이전 요약: {existing_summary}\\n\\n\" if existing_summary else \"\"\n",
    "    \n",
    "    # 요약할 대화 내용 준비 (너무 길면 최근 내용만)\n",
    "    recent_history = history[-15:] if len(history) > 15 else history\n",
    "    conversation_text = \"\\n\".join(recent_history)\n",
    "    \n",
    "    summarization_prompt = f\"\"\"\n",
    "{summary_prefix}다음 대화 내용을 핵심만 간단히 요약해주세요:\n",
    "\n",
    "대화 내용:\n",
    "{conversation_text}\n",
    "\n",
    "요약 시 포함해야 할 정보:\n",
    "1. 사용자의 이름, 선호도 등 개인 정보\n",
    "2. 중요한 질문과 답변의 핵심 내용\n",
    "3. 진행 중인 주제나 작업\n",
    "4. 향후 대화에 필요한 맥락 정보\n",
    "\n",
    "불필요한 인사말이나 반복적인 내용은 제외하고, 핵심만 간단히 요약해주세요.\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"📚 [DEBUG] EXAONE 요약 실행 중...\")\n",
    "    new_summary = pure_exaone_inference(summarization_prompt)\n",
    "    print(f\"📚 [DEBUG] 생성된 요약: \\\"{new_summary[:50]}...\\\"\")\n",
    "    \n",
    "    # 최근 5턴만 유지하고 나머지는 요약으로 대체\n",
    "    recent_turns = history[-5:] if len(history) >= 5 else history\n",
    "    print(f\"📚 [DEBUG] 요약 후 유지할 대화: {len(recent_turns)}개\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"summary\": new_summary,\n",
    "        \"recent_turns\": recent_turns,\n",
    "        \"conversation_history\": recent_turns,  # 메모리 최적화\n",
    "        \"should_summarize\": False\n",
    "    }\n",
    "\n",
    "def normal_chat_node(state: SummarizationState) -> SummarizationState:\n",
    "    \"\"\"💬 일반적인 대화 처리 (요약 없이)\"\"\"\n",
    "    user_input = state[\"user_input\"]\n",
    "    history = state.get(\"conversation_history\", [])\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    \n",
    "    # 🐛 일반 대화 노드 시작 디버그\n",
    "    print(\"💬 [DEBUG] 일반 대화 노드 실행!\")\n",
    "    print(f\"💬 [DEBUG] 현재 사용자 입력: \\\"{user_input}\\\"\")\n",
    "    \n",
    "    # 기존 요약 + 최근 대화 맥락으로 프롬프트 구성\n",
    "    context = \"\"\n",
    "    if summary:\n",
    "        context += f\"이전 대화 요약: {summary}\\n\\n\"\n",
    "        print(\"💬 [DEBUG] 이전 요약 내용 포함\")\n",
    "    \n",
    "    if history:\n",
    "        recent_context = \"\\n\".join(history[-5:])  # 최근 5턴\n",
    "        context += f\"최근 대화:\\n{recent_context}\\n\\n\"\n",
    "        print(f\"💬 [DEBUG] 최근 대화 {len(history[-5:])}턴 포함\")\n",
    "    \n",
    "    chat_prompt = f\"{context}사용자: {user_input}\\n\\n위 맥락을 고려해서 자연스럽게 응답해주세요.\"\n",
    "    \n",
    "    print(\"💬 [DEBUG] EXAONE 응답 생성 중...\")\n",
    "    response = pure_exaone_inference(chat_prompt)\n",
    "    \n",
    "    # 대화 기록에 현재 턴 추가\n",
    "    updated_history = history + [f\"사용자: {user_input}\", f\"AI: {response}\"]\n",
    "    print(f\"💬 [DEBUG] 업데이트된 대화 기록: {len(updated_history)}개\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"conversation_history\": updated_history,\n",
    "        \"response\": response\n",
    "    }\n",
    "\n",
    "print(\"📚 대화 요약 시스템 노드 정의 완료 (디버그 포함)!\")\n",
    "print(\"   🔍 should_summarize_check: 요약 필요성 판단 + 디버그\")  \n",
    "print(\"   📚 summarization_node: EXAONE 기반 지능적 요약 + 디버그\")\n",
    "print(\"   💬 normal_chat_node: 일반 대화 처리 + 디버그\")\n",
    "print(\"   🐛 모든 노드에 디버그 출력 추가로 실행 과정 추적 가능\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📚 대화 요약 워크플로우 구축\n",
    "summarization_workflow = StateGraph(SummarizationState)\n",
    "\n",
    "# 노드들 추가\n",
    "summarization_workflow.add_node(\"normal_chat\", normal_chat_node)\n",
    "summarization_workflow.add_node(\"summarize\", summarization_node)\n",
    "\n",
    "# 조건부 시작: 요약 필요성 체크 후 적절한 노드로 라우팅\n",
    "summarization_workflow.set_conditional_entry_point(\n",
    "    should_summarize_check,\n",
    "    {\n",
    "        \"normal_chat\": \"normal_chat\",\n",
    "        \"summarize\": \"summarize\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# 요약 후에는 일반 대화로 이어짐\n",
    "summarization_workflow.add_edge(\"summarize\", \"normal_chat\")\n",
    "summarization_workflow.add_edge(\"normal_chat\", END)\n",
    "\n",
    "# 컴파일\n",
    "summarization_app = summarization_workflow.compile()\n",
    "\n",
    "print(\"📚 대화 요약 워크플로우 완성!\")\n",
    "print(\"   🔍 자동 길이 체크: 3턴 이상이면 자동 요약 (강의용)\")\n",
    "print(\"   📚 지능적 요약: EXAONE이 핵심 내용만 추출\")\n",
    "print(\"   💾 메모리 최적화: 요약 + 최근 5턴만 유지\")\n",
    "print(\"   🎯 성능과 맥락의 완벽한 균형!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 4턴 대화 요약 시스템 테스트 (문제 해결 버전)\n",
    "print(\"🧪 EXAONE 기반 대화 요약 시스템 테스트\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 4턴 대화를 시뮬레이션하여 요약 기능 확실히 테스트\n",
    "print(\"📝 4턴 대화 시뮬레이션을 통한 요약 테스트\")\n",
    "print(\"💡 3턴 후(6개 항목 후)에 요약이 트리거되도록 설정!\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 초기 상태 설정\n",
    "test_state = {\n",
    "    \"conversation_history\": [],\n",
    "    \"summary\": \"\",\n",
    "    \"recent_turns\": [],\n",
    "    \"user_input\": \"\",\n",
    "    \"response\": \"\",\n",
    "    \"should_summarize\": False\n",
    "}\n",
    "\n",
    "# 4턴의 대화 시나리오 (요약이 확실히 발생하도록)\n",
    "conversation_turns = [\n",
    "    \"안녕하세요! 저는 김민수라고 합니다.\",\n",
    "    \"파이썬 프로그래밍을 배우고 싶은데 어떻게 시작하면 좋을까요?\",\n",
    "    \"데이터 분석 분야에 관심이 있어서 판다스도 배우고 싶습니다.\",\n",
    "    \"머신러닝도 함께 배울 수 있을까요?\"\n",
    "]\n",
    "\n",
    "print(\"🔄 턴별 대화 진행 및 상태 변화 관찰:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for turn_num, user_input in enumerate(conversation_turns, 1):\n",
    "    print(f\"\\n{'='*20} {turn_num}턴 시작 {'='*20}\")\n",
    "    print(f\"📢 사용자 입력: \\\"{user_input}\\\"\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 🐛 워크플로우 실행 전 상태 확인\n",
    "    before_history = test_state.get(\"conversation_history\", [])\n",
    "    print(f\"🔍 [BEFORE] 대화 기록 개수: {len(before_history)}개\")\n",
    "    \n",
    "    # 현재 턴의 사용자 입력 설정\n",
    "    test_state[\"user_input\"] = user_input\n",
    "    \n",
    "    # 🔥 대화 시스템 실행 (디버그 출력 포함)\n",
    "    print(\"🚀 워크플로우 실행 중...\")\n",
    "    result = summarization_app.invoke(test_state)\n",
    "    \n",
    "    # 결과 업데이트\n",
    "    test_state = result\n",
    "    \n",
    "    # 🐛 워크플로우 실행 후 상태 확인\n",
    "    after_history = result.get(\"conversation_history\", [])\n",
    "    print(f\"🔍 [AFTER] 대화 기록 개수: {len(after_history)}개\")\n",
    "    print(f\"🤖 AI 응답: \\\"{result.get('response', 'N/A')[:80]}...\\\"\")\n",
    "    \n",
    "    # 요약 여부 상세 확인\n",
    "    summary_content = result.get(\"summary\", \"\")\n",
    "    if summary_content:\n",
    "        print(f\"📚 ✅ 요약 생성 성공!\")\n",
    "        print(f\"   📝 요약 내용: \\\"{summary_content[:100]}...\\\"\")\n",
    "        print(f\"   💾 최근 대화 유지: {len(result.get('recent_turns', []))}개 항목\")\n",
    "        print(f\"   🔥 메모리 최적화: {len(before_history)} → {len(after_history)}개로 압축!\")\n",
    "        print(\"   ✅ 요약 시스템 정상 작동!\")\n",
    "    else:\n",
    "        print(\"   ⏳ 요약 없음 - 아직 조건 미달\")\n",
    "    \n",
    "    # 다음 턴 예상\n",
    "    expected_next = \"요약 후 대화\" if len(after_history) >= 6 else \"일반 대화\"\n",
    "    print(f\"   🔮 다음 턴 예상: {expected_next}\")\n",
    "    \n",
    "    print(f\"{'='*20} {turn_num}턴 완료 {'='*20}\")\n",
    "\n",
    "print(f\"\\n{'🎯' * 25} 최종 테스트 결과 분석 {'🎯' * 25}\")\n",
    "\n",
    "# 최종 결과 검증\n",
    "final_history = test_state.get(\"conversation_history\", [])\n",
    "final_summary = test_state.get(\"summary\", \"\")\n",
    "\n",
    "if final_summary:\n",
    "    print(\"🎉 요약 시스템 테스트 성공!\")\n",
    "    print(f\"   📊 최종 대화 기록: {len(final_history)}개\")\n",
    "    print(f\"   📝 요약 길이: {len(final_summary)}글자\")\n",
    "    print(f\"   💾 메모리 절약률: {((8-len(final_history))/8*100):.1f}% 절약\")\n",
    "    print(f\"   🎯 맥락 보존: 요약으로 핵심 정보 유지\")\n",
    "else:\n",
    "    print(\"❌ 요약 시스템 문제 발견!\")\n",
    "    print(\"   🔧 추가 디버깅이 필요합니다.\")\n",
    "\n",
    "print(f\"\\n{'🚀' * 25} 실무 적용 가이드 {'🚀' * 25}\")\n",
    "print(\"📈 확장성: 긴 고객 상담에서 토큰 한계 방지\")\n",
    "print(\"🧠 지능성: EXAONE이 핵심 내용만 선별하여 요약\")\n",
    "print(\"⚖️  균형성: 성능 최적화와 맥락 보존을 동시에 달성\")\n",
    "print(\"🔧 실용성: should_summarize_check에서 턴 수 쉽게 조정\")\n",
    "print(\"💡 강의 효과: 학생들이 요약 과정을 단계적으로 관찰 가능\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 대화 요약 워크플로우 시각화\n",
    "print(\"📊 대화 요약 워크플로우 구조:\")\n",
    "visualize_graph(summarization_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 👥 Part 4: Human in the Loop 패턴 - 인간의 지혜가 필요한 순간\n",
    "\n",
    "### 🎯 Human in the Loop가 필요한 이유\n",
    "AI가 아무리 똑똑해도 **중요한 고객 응대**나 **민감한 이메일**에는 인간의 검토가 필수!\n",
    "\n",
    "### 📧 실제 비즈니스 시나리오: 이메일 자동 응답 시스템\n",
    "현대 기업의 고객 서비스팀이 매일 수백 통의 고객 이메일을 처리하는 상황을 생각해보세요:\n",
    "\n",
    "**🔄 Human in the Loop 워크플로우:**\n",
    "1. **고객 이메일 접수**: 배송 지연, 환불 요청, 기술 문의 등\n",
    "2. **AI 초안 생성**: EXAONE이 전문적이고 친절한 응답 초안 자동 작성\n",
    "3. **인간 검토 단계**: 고객서비스팀이 AI 초안을 검토하고 피드백 제공\n",
    "4. **최종 응답 확정**: 피드백 반영하여 완성된 응답을 고객에게 발송\n",
    "\n",
    "### 💡 핵심 가치\n",
    "- **효율성**: AI가 80% 작업을 자동화하여 처리 시간 대폭 단축\n",
    "- **품질 보장**: 인간 전문가의 검토로 고객 만족도 향상\n",
    "- **위험 관리**: 민감한 내용이나 복잡한 상황에서 실수 방지\n",
    "- **일관성**: 회사 정책과 톤앤매너를 일관되게 유지\n",
    "\n",
    "### 🚀 실무 적용 효과\n",
    "- **처리량 300% 증가**: 하루 100통 → 300통 처리 가능\n",
    "- **응답 품질 향상**: 인간 검토로 고객 만족도 95% 달성\n",
    "- **비용 절감**: 인력 투입은 줄이고 서비스 품질은 향상\n",
    "\n",
    "아래에서 실제 작동하는 Human in the Loop 시스템을 체험해보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START, END\n",
    "from langgraph.graph import StateGraph, add_messages\n",
    "from langgraph.types import interrupt, Command\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# 📧 이메일 자동 응답 시스템용 State\n",
    "class EmailState(TypedDict):\n",
    "    customer_email: str      # 고객 이메일 내용\n",
    "    ai_draft: str           # AI가 생성한 응답 초안\n",
    "    human_feedback: str     # 인간 검토자 피드백\n",
    "    final_response: str     # 최종 이메일 응답\n",
    "\n",
    "def ai_draft_node(state: EmailState):\n",
    "    \"\"\"🤖 AI가 고객 이메일에 대한 응답 초안 생성\"\"\"\n",
    "    customer_email = state[\"customer_email\"]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "다음 고객 이메일에 대한 전문적이고 친절한 응답을 작성해주세요:\n",
    "\n",
    "고객 이메일: {customer_email}\n",
    "\n",
    "회사 정책에 맞는 정중하고 도움이 되는 응답을 작성해주세요.\n",
    "\"\"\"\n",
    "    \n",
    "    ai_draft = pure_exaone_inference(prompt)\n",
    "    print(f\"🤖 AI 응답 초안 생성 완료\")\n",
    "    return {\"ai_draft\": ai_draft}\n",
    "\n",
    "def human_review_node(state: EmailState):\n",
    "    \"\"\"👤 인간 검토자가 AI 초안을 검토\"\"\"\n",
    "    ai_draft = state[\"ai_draft\"]\n",
    "    \n",
    "    print(f\"📧 AI 응답 초안: {ai_draft}\")\n",
    "    \n",
    "    # 🛑 여기서 인간 검토자의 승인/피드백 대기\n",
    "    feedback = interrupt({\n",
    "        \"message\": \"AI가 이메일 응답 초안을 생성했습니다. 검토 후 피드백을 주세요:\",\n",
    "        \"draft\": ai_draft\n",
    "    })\n",
    "    \n",
    "    return {\"human_feedback\": feedback}\n",
    "\n",
    "def finalize_node(state: EmailState):\n",
    "    \"\"\"✅ 최종 이메일 응답 확정\"\"\"\n",
    "    ai_draft = state[\"ai_draft\"]\n",
    "    feedback = state[\"human_feedback\"]\n",
    "    \n",
    "    if \"승인\" in feedback:\n",
    "        final_response = ai_draft\n",
    "    else:\n",
    "        # 피드백 반영해서 수정\n",
    "        prompt = f\"\"\"\n",
    "다음 AI 초안을 검토자 피드백에 따라 수정해주세요:\n",
    "\n",
    "원본 초안: {ai_draft}\n",
    "검토자 피드백: {feedback}\n",
    "\n",
    "피드백을 반영한 개선된 응답을 작성해주세요.\n",
    "\"\"\"\n",
    "        final_response = pure_exaone_inference(prompt)\n",
    "    \n",
    "    print(f\"✅ 최종 이메일 응답 확정!\")\n",
    "    return {\"final_response\": final_response}\n",
    "\n",
    "print(\"📧 이메일 자동 응답 Human in the Loop 시스템 구성 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📧 이메일 자동 응답 워크플로우 생성\n",
    "workflow = StateGraph(EmailState)\n",
    "\n",
    "# 노드 추가\n",
    "workflow.add_node(\"ai_draft\", ai_draft_node)\n",
    "workflow.add_node(\"human_review\", human_review_node)\n",
    "workflow.add_node(\"finalize\", finalize_node)\n",
    "\n",
    "# 워크플로우 연결\n",
    "workflow.add_edge(START, \"ai_draft\")\n",
    "workflow.add_edge(\"ai_draft\", \"human_review\")\n",
    "workflow.add_edge(\"human_review\", \"finalize\")\n",
    "workflow.add_edge(\"finalize\", END)\n",
    "\n",
    "# 체크포인터 설정 (Human in the Loop에 필수)\n",
    "checkpointer = MemorySaver()\n",
    "email_app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "print(\"✅ 이메일 자동 응답 시스템 준비 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📧 1단계: 고객 이메일 처리 (interrupt 지점까지)\n",
    "import uuid\n",
    "\n",
    "# 고객 이메일 예시\n",
    "customer_email = \"\"\"\n",
    "안녕하세요,\n",
    "\n",
    "지난주에 주문한 노트북이 아직 도착하지 않았습니다. \n",
    "주문번호는 ORD-2024-1234이고, 배송 예정일이 3일 전이었는데\n",
    "아직 배송 추적에서 확인이 안 됩니다.\n",
    "\n",
    "언제쯤 받을 수 있을지 알려주시면 감사하겠습니다.\n",
    "\n",
    "김고객 드림\n",
    "\"\"\"\n",
    "\n",
    "print(\"📧 고객 이메일:\")\n",
    "print(customer_email)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 워크플로우 실행\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "result = email_app.invoke(\n",
    "    {\"customer_email\": customer_email}, \n",
    "    config=config\n",
    ")\n",
    "\n",
    "# interrupt 확인\n",
    "if \"__interrupt__\" in result:\n",
    "    print(\"✅ Human in the Loop 활성화!\")\n",
    "    print(\"👤 검토자의 피드백이 필요합니다.\")\n",
    "    print(\"\\n📋 AI 초안:\")\n",
    "    print(result[\"ai_draft\"])\n",
    "    print(f\"\\n🔍 Thread ID: {thread_id}\")\n",
    "    print(\"📌 다음 셀에서 피드백을 제공하세요!\")\n",
    "else:\n",
    "    print(\"❌ interrupt가 발생하지 않았습니다.\")\n",
    "    print(\"결과:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📧 2단계: 검토자 피드백 후 최종 응답 생성\n",
    "\n",
    "# 검토자 피드백 (실제로는 웹 인터페이스에서 받음)\n",
    "reviewer_feedback = \"\"\"\n",
    "AI 초안이 전반적으로 좋습니다. \n",
    "다만 다음 내용을 추가해주세요:\n",
    "1. 사과 인사를 더 명확히\n",
    "2. 구체적인 해결 방안 제시\n",
    "3. 고객 만족을 위한 추가 혜택 언급\n",
    "\n",
    "승인은 수정 후에 하겠습니다.\n",
    "\"\"\"\n",
    "\n",
    "print(\"👤 검토자 피드백:\")\n",
    "print(reviewer_feedback)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Command(resume)으로 워크플로우 재시작\n",
    "final_result = email_app.invoke(\n",
    "    Command(resume=reviewer_feedback),\n",
    "    config  # 이전과 동일한 config 사용\n",
    ")\n",
    "\n",
    "print(\"✅ 최종 이메일 응답 완성!\")\n",
    "print(\"\\n📧 최종 고객 응답:\")\n",
    "print(\"=\"*60)\n",
    "print(final_result[\"final_response\"])\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n🎉 Human in the Loop 프로세스 완료!\")\n",
    "print(\"💼 실무에서는 이 응답이 고객에게 자동 발송됩니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📧 Human in the Loop 워크플로우 정리\n",
    "\n",
    "print(\"📋 이메일 자동 응답 Human in the Loop 시스템:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "🎯 실제 비즈니스 시나리오:\n",
    "   고객 → 배송 지연 문의 이메일 발송\n",
    "   \n",
    "📍 1단계 (AI 초안 생성):\n",
    "   🤖 EXAONE이 고객 이메일 분석\n",
    "   📝 전문적인 응답 초안 자동 생성\n",
    "   \n",
    "📍 2단계 (Human 검토):\n",
    "   🛑 interrupt() 발생 → 워크플로우 중단\n",
    "   👤 고객서비스팀 검토자가 초안 확인\n",
    "   💬 피드백: \"사과 인사 추가, 구체적 해결책 제시\"\n",
    "   \n",
    "📍 3단계 (최종 응답):\n",
    "   🔄 AI가 피드백 반영하여 응답 수정\n",
    "   ✅ 최종 승인된 이메일 고객에게 발송\n",
    "\n",
    "💡 핵심 가치:\n",
    "   - 자동화로 효율성 증대\n",
    "   - 인간 검토로 품질 보장\n",
    "   - 고객 만족도 향상\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n🔧 워크플로우 구조:\")\n",
    "visualize_graph(email_app)\n",
    "\n",
    "print(\"\\n🎓 학습 포인트:\")\n",
    "print(\"- interrupt()로 워크플로우 중단\")\n",
    "print(\"- Command(resume)로 재시작\")\n",
    "print(\"- 체크포인터로 상태 저장\")\n",
    "print(\"- 실무 적용 가능한 패턴\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 LangGraph 기본 패턴 학습 완료!\n",
    "\n",
    "### ✅ 성공적으로 마스터한 4가지 핵심 패턴:\n",
    "\n",
    "#### 1️⃣ **Routing 패턴** 🧠\n",
    "- **목적**: 사용자 입력을 지능적으로 분류하여 적절한 처리 경로로 라우팅\n",
    "- **실무 활용**: 고객 문의 유형별 자동 분류, 우선순위 설정\n",
    "- **핵심 기술**: 조건부 엣지, LLM 기반 분류\n",
    "\n",
    "#### 2️⃣ **Fan-out/Fan-in 패턴** 🔄  \n",
    "- **목적**: 복잡한 작업을 여러 전문 영역으로 병렬 분산 후 결과 통합\n",
    "- **실무 활용**: 다각도 분석, 전문팀 협업, 성능 최적화\n",
    "- **핵심 기술**: 병렬 노드 실행, 결과 집계\n",
    "\n",
    "#### 3️⃣ **Summarization 패턴** 📚\n",
    "- **목적**: 긴 대화나 문서를 핵심 정보만 압축하여 메모리 효율성 향상\n",
    "- **실무 활용**: 채팅봇 컨텍스트 관리, 회의록 요약, 문서 압축\n",
    "- **핵심 기술**: 조건부 요약, 상태 관리\n",
    "\n",
    "#### 4️⃣ **Human in the Loop 패턴** 👥\n",
    "- **목적**: 중요한 결정이나 민감한 작업에 인간의 검토와 승인 과정 추가\n",
    "- **실무 활용**: 이메일 자동 응답, 고객 서비스, 콘텐츠 검토\n",
    "- **핵심 기술**: interrupt() 함수, 워크플로우 중단/재시작\n",
    "\n",
    "### 🚀 다음 단계: 고급 통합 패턴\n",
    "기본 패턴을 완전히 이해했다면, `01-advanced-integration.ipynb`에서 모든 패턴을 조합한 실무급 시스템을 구축해보세요!\n",
    "\n",
    "### 💡 실무 적용 가이드\n",
    "각 패턴은 독립적으로도 강력하지만, 조합할 때 진정한 비즈니스 가치를 발휘합니다:\n",
    "- **단일 패턴**: 특정 문제 해결\n",
    "- **패턴 조합**: 완전한 엔터프라이즈 솔루션\n",
    "\n",
    "🎉 **축하합니다! 이제 LangGraph로 실무급 AI 워크플로우를 구축할 수 있습니다!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
